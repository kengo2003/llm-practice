{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 200.0,
  "eval_steps": 500,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 20.459415435791016,
      "learning_rate": 0.0,
      "loss": 43.6969,
      "step": 1
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 22.83440589904785,
      "learning_rate": 5e-06,
      "loss": 43.1427,
      "step": 2
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8697806000709534,
      "learning_rate": 1e-05,
      "loss": 3.0577,
      "step": 3
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 12.860330581665039,
      "learning_rate": 1.5e-05,
      "loss": 41.4693,
      "step": 4
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 11.870275497436523,
      "learning_rate": 2e-05,
      "loss": 36.8376,
      "step": 5
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 13.611270904541016,
      "learning_rate": 2.5e-05,
      "loss": 43.7218,
      "step": 6
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 9.046823501586914,
      "learning_rate": 3e-05,
      "loss": 43.6994,
      "step": 7
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 11.922431945800781,
      "learning_rate": 3.5e-05,
      "loss": 44.2767,
      "step": 8
    },
    {
      "epoch": 1.0,
      "grad_norm": 12.845932960510254,
      "learning_rate": 4e-05,
      "loss": 41.6816,
      "step": 9
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 11.858071327209473,
      "learning_rate": 4.5e-05,
      "loss": 43.0051,
      "step": 10
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 11.970510482788086,
      "learning_rate": 5e-05,
      "loss": 40.1683,
      "step": 11
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 29.858386993408203,
      "learning_rate": 5.500000000000001e-05,
      "loss": 43.4317,
      "step": 12
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.740627110004425,
      "learning_rate": 6e-05,
      "loss": 2.8931,
      "step": 13
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 9.868083953857422,
      "learning_rate": 6.500000000000001e-05,
      "loss": 40.0795,
      "step": 14
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 18.174835205078125,
      "learning_rate": 7e-05,
      "loss": 45.0132,
      "step": 15
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 17.810258865356445,
      "learning_rate": 7.500000000000001e-05,
      "loss": 37.3241,
      "step": 16
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 24.393091201782227,
      "learning_rate": 8e-05,
      "loss": 41.3322,
      "step": 17
    },
    {
      "epoch": 2.0,
      "grad_norm": 22.69576644897461,
      "learning_rate": 8.5e-05,
      "loss": 42.3444,
      "step": 18
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 11.846285820007324,
      "learning_rate": 9e-05,
      "loss": 40.5644,
      "step": 19
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.9544410705566406,
      "learning_rate": 9.5e-05,
      "loss": 3.222,
      "step": 20
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 13.164341926574707,
      "learning_rate": 0.0001,
      "loss": 39.4006,
      "step": 21
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 14.310562133789062,
      "learning_rate": 9.99438202247191e-05,
      "loss": 42.3583,
      "step": 22
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 17.844221115112305,
      "learning_rate": 9.988764044943821e-05,
      "loss": 47.405,
      "step": 23
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 17.61896514892578,
      "learning_rate": 9.983146067415731e-05,
      "loss": 43.8213,
      "step": 24
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 43.59560775756836,
      "learning_rate": 9.977528089887641e-05,
      "loss": 42.4414,
      "step": 25
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 15.12163257598877,
      "learning_rate": 9.971910112359551e-05,
      "loss": 42.457,
      "step": 26
    },
    {
      "epoch": 3.0,
      "grad_norm": 20.252214431762695,
      "learning_rate": 9.966292134831461e-05,
      "loss": 39.5336,
      "step": 27
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 15.511311531066895,
      "learning_rate": 9.960674157303371e-05,
      "loss": 38.6735,
      "step": 28
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 11.673100471496582,
      "learning_rate": 9.955056179775282e-05,
      "loss": 38.2893,
      "step": 29
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 52.63070297241211,
      "learning_rate": 9.94943820224719e-05,
      "loss": 45.2509,
      "step": 30
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.8403227925300598,
      "learning_rate": 9.943820224719102e-05,
      "loss": 2.9187,
      "step": 31
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 19.638381958007812,
      "learning_rate": 9.938202247191012e-05,
      "loss": 45.5796,
      "step": 32
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 27.21330451965332,
      "learning_rate": 9.932584269662922e-05,
      "loss": 38.1514,
      "step": 33
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 12.241079330444336,
      "learning_rate": 9.926966292134831e-05,
      "loss": 38.1607,
      "step": 34
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 13.322210311889648,
      "learning_rate": 9.921348314606742e-05,
      "loss": 41.0343,
      "step": 35
    },
    {
      "epoch": 4.0,
      "grad_norm": 15.509678840637207,
      "learning_rate": 9.915730337078653e-05,
      "loss": 39.6725,
      "step": 36
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 16.294004440307617,
      "learning_rate": 9.910112359550561e-05,
      "loss": 42.0662,
      "step": 37
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 73.28028869628906,
      "learning_rate": 9.904494382022473e-05,
      "loss": 39.3589,
      "step": 38
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 14.263712882995605,
      "learning_rate": 9.898876404494383e-05,
      "loss": 40.9744,
      "step": 39
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.8807140588760376,
      "learning_rate": 9.893258426966293e-05,
      "loss": 2.8834,
      "step": 40
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 11.613258361816406,
      "learning_rate": 9.887640449438202e-05,
      "loss": 39.8572,
      "step": 41
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 17.822879791259766,
      "learning_rate": 9.882022471910113e-05,
      "loss": 39.7462,
      "step": 42
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 12.513205528259277,
      "learning_rate": 9.876404494382024e-05,
      "loss": 38.7314,
      "step": 43
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 16.58576011657715,
      "learning_rate": 9.870786516853932e-05,
      "loss": 36.6854,
      "step": 44
    },
    {
      "epoch": 5.0,
      "grad_norm": 21.564712524414062,
      "learning_rate": 9.865168539325843e-05,
      "loss": 37.9188,
      "step": 45
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 28.305809020996094,
      "learning_rate": 9.859550561797754e-05,
      "loss": 42.2786,
      "step": 46
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 20.017854690551758,
      "learning_rate": 9.853932584269664e-05,
      "loss": 41.1175,
      "step": 47
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 42.53876876831055,
      "learning_rate": 9.848314606741573e-05,
      "loss": 41.4117,
      "step": 48
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 29.272077560424805,
      "learning_rate": 9.842696629213483e-05,
      "loss": 39.724,
      "step": 49
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 43.56783676147461,
      "learning_rate": 9.837078651685395e-05,
      "loss": 44.117,
      "step": 50
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 25.10702133178711,
      "learning_rate": 9.831460674157303e-05,
      "loss": 37.1756,
      "step": 51
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 14.48568058013916,
      "learning_rate": 9.825842696629214e-05,
      "loss": 36.3526,
      "step": 52
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 27.627117156982422,
      "learning_rate": 9.820224719101124e-05,
      "loss": 37.8275,
      "step": 53
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.9008326530456543,
      "learning_rate": 9.814606741573035e-05,
      "loss": 2.9077,
      "step": 54
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 24.24131965637207,
      "learning_rate": 9.808988764044944e-05,
      "loss": 40.6084,
      "step": 55
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 25.534116744995117,
      "learning_rate": 9.803370786516854e-05,
      "loss": 40.5926,
      "step": 56
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 13.289558410644531,
      "learning_rate": 9.797752808988764e-05,
      "loss": 37.1779,
      "step": 57
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 21.53590202331543,
      "learning_rate": 9.792134831460674e-05,
      "loss": 34.0784,
      "step": 58
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 14.350179672241211,
      "learning_rate": 9.786516853932585e-05,
      "loss": 35.1501,
      "step": 59
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 15.414670944213867,
      "learning_rate": 9.780898876404495e-05,
      "loss": 35.627,
      "step": 60
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 17.517654418945312,
      "learning_rate": 9.775280898876405e-05,
      "loss": 32.8187,
      "step": 61
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 22.656082153320312,
      "learning_rate": 9.769662921348315e-05,
      "loss": 35.8626,
      "step": 62
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.8902820348739624,
      "learning_rate": 9.764044943820225e-05,
      "loss": 2.9706,
      "step": 63
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 17.361547470092773,
      "learning_rate": 9.758426966292135e-05,
      "loss": 34.4738,
      "step": 64
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 16.169498443603516,
      "learning_rate": 9.752808988764045e-05,
      "loss": 31.3557,
      "step": 65
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 17.738435745239258,
      "learning_rate": 9.747191011235956e-05,
      "loss": 35.6556,
      "step": 66
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 21.479354858398438,
      "learning_rate": 9.741573033707866e-05,
      "loss": 35.554,
      "step": 67
    },
    {
      "epoch": 7.555555555555555,
      "grad_norm": 18.615840911865234,
      "learning_rate": 9.735955056179776e-05,
      "loss": 30.1261,
      "step": 68
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 20.19278907775879,
      "learning_rate": 9.730337078651686e-05,
      "loss": 33.6406,
      "step": 69
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 21.432395935058594,
      "learning_rate": 9.724719101123596e-05,
      "loss": 32.5529,
      "step": 70
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 23.017196655273438,
      "learning_rate": 9.719101123595506e-05,
      "loss": 35.3603,
      "step": 71
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.0508753061294556,
      "learning_rate": 9.713483146067416e-05,
      "loss": 2.9413,
      "step": 72
    },
    {
      "epoch": 8.11111111111111,
      "grad_norm": 56.08367156982422,
      "learning_rate": 9.707865168539327e-05,
      "loss": 36.8522,
      "step": 73
    },
    {
      "epoch": 8.222222222222221,
      "grad_norm": 14.602008819580078,
      "learning_rate": 9.702247191011237e-05,
      "loss": 29.978,
      "step": 74
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 12.086461067199707,
      "learning_rate": 9.696629213483146e-05,
      "loss": 30.7016,
      "step": 75
    },
    {
      "epoch": 8.444444444444445,
      "grad_norm": 3.4942004680633545,
      "learning_rate": 9.691011235955057e-05,
      "loss": 2.8949,
      "step": 76
    },
    {
      "epoch": 8.555555555555555,
      "grad_norm": 15.783173561096191,
      "learning_rate": 9.685393258426967e-05,
      "loss": 33.9601,
      "step": 77
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 36.12409973144531,
      "learning_rate": 9.679775280898877e-05,
      "loss": 36.972,
      "step": 78
    },
    {
      "epoch": 8.777777777777779,
      "grad_norm": 18.763017654418945,
      "learning_rate": 9.674157303370786e-05,
      "loss": 27.5248,
      "step": 79
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 18.37207794189453,
      "learning_rate": 9.668539325842698e-05,
      "loss": 29.4367,
      "step": 80
    },
    {
      "epoch": 9.0,
      "grad_norm": 45.29115676879883,
      "learning_rate": 9.662921348314608e-05,
      "loss": 28.2071,
      "step": 81
    },
    {
      "epoch": 9.11111111111111,
      "grad_norm": 18.958860397338867,
      "learning_rate": 9.657303370786517e-05,
      "loss": 29.1066,
      "step": 82
    },
    {
      "epoch": 9.222222222222221,
      "grad_norm": 16.391429901123047,
      "learning_rate": 9.651685393258427e-05,
      "loss": 28.0473,
      "step": 83
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 12.932689666748047,
      "learning_rate": 9.646067415730338e-05,
      "loss": 30.8088,
      "step": 84
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 18.743558883666992,
      "learning_rate": 9.640449438202248e-05,
      "loss": 31.517,
      "step": 85
    },
    {
      "epoch": 9.555555555555555,
      "grad_norm": 0.9180639982223511,
      "learning_rate": 9.634831460674157e-05,
      "loss": 2.8105,
      "step": 86
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 21.398141860961914,
      "learning_rate": 9.629213483146067e-05,
      "loss": 29.9117,
      "step": 87
    },
    {
      "epoch": 9.777777777777779,
      "grad_norm": 16.313228607177734,
      "learning_rate": 9.623595505617979e-05,
      "loss": 27.4774,
      "step": 88
    },
    {
      "epoch": 9.88888888888889,
      "grad_norm": 24.742380142211914,
      "learning_rate": 9.617977528089888e-05,
      "loss": 26.6112,
      "step": 89
    },
    {
      "epoch": 10.0,
      "grad_norm": 15.570066452026367,
      "learning_rate": 9.612359550561798e-05,
      "loss": 26.0946,
      "step": 90
    },
    {
      "epoch": 10.11111111111111,
      "grad_norm": 16.41312599182129,
      "learning_rate": 9.606741573033708e-05,
      "loss": 27.3696,
      "step": 91
    },
    {
      "epoch": 10.222222222222221,
      "grad_norm": 16.726734161376953,
      "learning_rate": 9.601123595505619e-05,
      "loss": 29.141,
      "step": 92
    },
    {
      "epoch": 10.333333333333334,
      "grad_norm": 1.6741313934326172,
      "learning_rate": 9.595505617977528e-05,
      "loss": 3.0956,
      "step": 93
    },
    {
      "epoch": 10.444444444444445,
      "grad_norm": 43.555335998535156,
      "learning_rate": 9.589887640449438e-05,
      "loss": 27.4442,
      "step": 94
    },
    {
      "epoch": 10.555555555555555,
      "grad_norm": 13.685409545898438,
      "learning_rate": 9.58426966292135e-05,
      "loss": 30.054,
      "step": 95
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 17.424036026000977,
      "learning_rate": 9.578651685393259e-05,
      "loss": 30.1649,
      "step": 96
    },
    {
      "epoch": 10.777777777777779,
      "grad_norm": 28.76755142211914,
      "learning_rate": 9.573033707865169e-05,
      "loss": 29.798,
      "step": 97
    },
    {
      "epoch": 10.88888888888889,
      "grad_norm": 17.62810516357422,
      "learning_rate": 9.567415730337079e-05,
      "loss": 28.5359,
      "step": 98
    },
    {
      "epoch": 11.0,
      "grad_norm": 20.17151641845703,
      "learning_rate": 9.56179775280899e-05,
      "loss": 30.7525,
      "step": 99
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 9.378682136535645,
      "learning_rate": 9.556179775280899e-05,
      "loss": 24.869,
      "step": 100
    },
    {
      "epoch": 11.222222222222221,
      "grad_norm": 28.448070526123047,
      "learning_rate": 9.550561797752809e-05,
      "loss": 26.8305,
      "step": 101
    },
    {
      "epoch": 11.333333333333334,
      "grad_norm": 15.466256141662598,
      "learning_rate": 9.54494382022472e-05,
      "loss": 25.147,
      "step": 102
    },
    {
      "epoch": 11.444444444444445,
      "grad_norm": 0.8441023826599121,
      "learning_rate": 9.53932584269663e-05,
      "loss": 2.7008,
      "step": 103
    },
    {
      "epoch": 11.555555555555555,
      "grad_norm": 22.061941146850586,
      "learning_rate": 9.53370786516854e-05,
      "loss": 27.1517,
      "step": 104
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 26.999923706054688,
      "learning_rate": 9.52808988764045e-05,
      "loss": 30.0073,
      "step": 105
    },
    {
      "epoch": 11.777777777777779,
      "grad_norm": 20.61201286315918,
      "learning_rate": 9.52247191011236e-05,
      "loss": 29.2029,
      "step": 106
    },
    {
      "epoch": 11.88888888888889,
      "grad_norm": 73.91630554199219,
      "learning_rate": 9.51685393258427e-05,
      "loss": 29.177,
      "step": 107
    },
    {
      "epoch": 12.0,
      "grad_norm": 12.568124771118164,
      "learning_rate": 9.51123595505618e-05,
      "loss": 24.4592,
      "step": 108
    },
    {
      "epoch": 12.11111111111111,
      "grad_norm": 14.508379936218262,
      "learning_rate": 9.50561797752809e-05,
      "loss": 26.4119,
      "step": 109
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 57.12199401855469,
      "learning_rate": 9.5e-05,
      "loss": 27.3183,
      "step": 110
    },
    {
      "epoch": 12.333333333333334,
      "grad_norm": 1.1316359043121338,
      "learning_rate": 9.49438202247191e-05,
      "loss": 2.7739,
      "step": 111
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 10.68338680267334,
      "learning_rate": 9.488764044943821e-05,
      "loss": 24.1688,
      "step": 112
    },
    {
      "epoch": 12.555555555555555,
      "grad_norm": 12.107837677001953,
      "learning_rate": 9.483146067415731e-05,
      "loss": 25.8283,
      "step": 113
    },
    {
      "epoch": 12.666666666666666,
      "grad_norm": 14.66016960144043,
      "learning_rate": 9.477528089887641e-05,
      "loss": 23.5559,
      "step": 114
    },
    {
      "epoch": 12.777777777777779,
      "grad_norm": 13.585541725158691,
      "learning_rate": 9.471910112359551e-05,
      "loss": 23.2202,
      "step": 115
    },
    {
      "epoch": 12.88888888888889,
      "grad_norm": 13.150199890136719,
      "learning_rate": 9.466292134831461e-05,
      "loss": 23.4797,
      "step": 116
    },
    {
      "epoch": 13.0,
      "grad_norm": 17.56572151184082,
      "learning_rate": 9.46067415730337e-05,
      "loss": 24.4668,
      "step": 117
    },
    {
      "epoch": 13.11111111111111,
      "grad_norm": 13.615521430969238,
      "learning_rate": 9.455056179775282e-05,
      "loss": 22.8839,
      "step": 118
    },
    {
      "epoch": 13.222222222222221,
      "grad_norm": 17.949918746948242,
      "learning_rate": 9.449438202247192e-05,
      "loss": 26.1359,
      "step": 119
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 12.105640411376953,
      "learning_rate": 9.443820224719102e-05,
      "loss": 23.8338,
      "step": 120
    },
    {
      "epoch": 13.444444444444445,
      "grad_norm": 13.7371187210083,
      "learning_rate": 9.438202247191012e-05,
      "loss": 21.279,
      "step": 121
    },
    {
      "epoch": 13.555555555555555,
      "grad_norm": 14.511295318603516,
      "learning_rate": 9.432584269662922e-05,
      "loss": 22.9303,
      "step": 122
    },
    {
      "epoch": 13.666666666666666,
      "grad_norm": 49.34272766113281,
      "learning_rate": 9.426966292134832e-05,
      "loss": 24.3992,
      "step": 123
    },
    {
      "epoch": 13.777777777777779,
      "grad_norm": 0.9640946984291077,
      "learning_rate": 9.421348314606741e-05,
      "loss": 2.8054,
      "step": 124
    },
    {
      "epoch": 13.88888888888889,
      "grad_norm": 16.414989471435547,
      "learning_rate": 9.415730337078653e-05,
      "loss": 23.3216,
      "step": 125
    },
    {
      "epoch": 14.0,
      "grad_norm": 13.705578804016113,
      "learning_rate": 9.410112359550563e-05,
      "loss": 23.6756,
      "step": 126
    },
    {
      "epoch": 14.11111111111111,
      "grad_norm": 10.212410926818848,
      "learning_rate": 9.404494382022473e-05,
      "loss": 21.9367,
      "step": 127
    },
    {
      "epoch": 14.222222222222221,
      "grad_norm": 21.195934295654297,
      "learning_rate": 9.398876404494382e-05,
      "loss": 21.9088,
      "step": 128
    },
    {
      "epoch": 14.333333333333334,
      "grad_norm": 15.553223609924316,
      "learning_rate": 9.393258426966293e-05,
      "loss": 21.8271,
      "step": 129
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 12.08147144317627,
      "learning_rate": 9.387640449438203e-05,
      "loss": 21.1425,
      "step": 130
    },
    {
      "epoch": 14.555555555555555,
      "grad_norm": 9.854875564575195,
      "learning_rate": 9.382022471910112e-05,
      "loss": 20.2639,
      "step": 131
    },
    {
      "epoch": 14.666666666666666,
      "grad_norm": 10.435234069824219,
      "learning_rate": 9.376404494382022e-05,
      "loss": 20.0124,
      "step": 132
    },
    {
      "epoch": 14.777777777777779,
      "grad_norm": 24.122650146484375,
      "learning_rate": 9.370786516853934e-05,
      "loss": 23.6753,
      "step": 133
    },
    {
      "epoch": 14.88888888888889,
      "grad_norm": 1.0324147939682007,
      "learning_rate": 9.365168539325843e-05,
      "loss": 2.5248,
      "step": 134
    },
    {
      "epoch": 15.0,
      "grad_norm": 15.977360725402832,
      "learning_rate": 9.359550561797753e-05,
      "loss": 21.2883,
      "step": 135
    },
    {
      "epoch": 15.11111111111111,
      "grad_norm": 13.733307838439941,
      "learning_rate": 9.353932584269663e-05,
      "loss": 21.9656,
      "step": 136
    },
    {
      "epoch": 15.222222222222221,
      "grad_norm": 1.2050260305404663,
      "learning_rate": 9.348314606741574e-05,
      "loss": 2.7008,
      "step": 137
    },
    {
      "epoch": 15.333333333333334,
      "grad_norm": 14.549531936645508,
      "learning_rate": 9.342696629213483e-05,
      "loss": 21.854,
      "step": 138
    },
    {
      "epoch": 15.444444444444445,
      "grad_norm": 14.10015869140625,
      "learning_rate": 9.337078651685393e-05,
      "loss": 20.0461,
      "step": 139
    },
    {
      "epoch": 15.555555555555555,
      "grad_norm": 12.240668296813965,
      "learning_rate": 9.331460674157303e-05,
      "loss": 20.3394,
      "step": 140
    },
    {
      "epoch": 15.666666666666666,
      "grad_norm": 9.225956916809082,
      "learning_rate": 9.325842696629214e-05,
      "loss": 19.5242,
      "step": 141
    },
    {
      "epoch": 15.777777777777779,
      "grad_norm": 10.41899585723877,
      "learning_rate": 9.320224719101124e-05,
      "loss": 18.3139,
      "step": 142
    },
    {
      "epoch": 15.88888888888889,
      "grad_norm": 14.00097370147705,
      "learning_rate": 9.314606741573034e-05,
      "loss": 20.3913,
      "step": 143
    },
    {
      "epoch": 16.0,
      "grad_norm": 13.158034324645996,
      "learning_rate": 9.308988764044944e-05,
      "loss": 19.2235,
      "step": 144
    },
    {
      "epoch": 16.11111111111111,
      "grad_norm": 12.714741706848145,
      "learning_rate": 9.303370786516854e-05,
      "loss": 18.2979,
      "step": 145
    },
    {
      "epoch": 16.22222222222222,
      "grad_norm": 13.023884773254395,
      "learning_rate": 9.297752808988764e-05,
      "loss": 17.1257,
      "step": 146
    },
    {
      "epoch": 16.333333333333332,
      "grad_norm": 10.669894218444824,
      "learning_rate": 9.292134831460674e-05,
      "loss": 18.2302,
      "step": 147
    },
    {
      "epoch": 16.444444444444443,
      "grad_norm": 11.99280834197998,
      "learning_rate": 9.286516853932585e-05,
      "loss": 16.828,
      "step": 148
    },
    {
      "epoch": 16.555555555555557,
      "grad_norm": 14.610145568847656,
      "learning_rate": 9.280898876404495e-05,
      "loss": 19.4046,
      "step": 149
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 1.2681111097335815,
      "learning_rate": 9.275280898876405e-05,
      "loss": 2.8828,
      "step": 150
    },
    {
      "epoch": 16.77777777777778,
      "grad_norm": 26.738771438598633,
      "learning_rate": 9.269662921348315e-05,
      "loss": 20.1185,
      "step": 151
    },
    {
      "epoch": 16.88888888888889,
      "grad_norm": 38.087608337402344,
      "learning_rate": 9.264044943820225e-05,
      "loss": 19.2535,
      "step": 152
    },
    {
      "epoch": 17.0,
      "grad_norm": 19.731735229492188,
      "learning_rate": 9.258426966292135e-05,
      "loss": 17.2691,
      "step": 153
    },
    {
      "epoch": 17.11111111111111,
      "grad_norm": 14.024699211120605,
      "learning_rate": 9.252808988764045e-05,
      "loss": 16.2655,
      "step": 154
    },
    {
      "epoch": 17.22222222222222,
      "grad_norm": 1.05197012424469,
      "learning_rate": 9.247191011235956e-05,
      "loss": 2.5644,
      "step": 155
    },
    {
      "epoch": 17.333333333333332,
      "grad_norm": 11.859967231750488,
      "learning_rate": 9.241573033707866e-05,
      "loss": 18.9435,
      "step": 156
    },
    {
      "epoch": 17.444444444444443,
      "grad_norm": 18.937618255615234,
      "learning_rate": 9.235955056179776e-05,
      "loss": 17.74,
      "step": 157
    },
    {
      "epoch": 17.555555555555557,
      "grad_norm": 13.574472427368164,
      "learning_rate": 9.230337078651686e-05,
      "loss": 17.5078,
      "step": 158
    },
    {
      "epoch": 17.666666666666668,
      "grad_norm": 15.912991523742676,
      "learning_rate": 9.224719101123596e-05,
      "loss": 16.136,
      "step": 159
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 13.31104850769043,
      "learning_rate": 9.219101123595506e-05,
      "loss": 13.734,
      "step": 160
    },
    {
      "epoch": 17.88888888888889,
      "grad_norm": 12.2042875289917,
      "learning_rate": 9.213483146067416e-05,
      "loss": 13.7447,
      "step": 161
    },
    {
      "epoch": 18.0,
      "grad_norm": 15.60808277130127,
      "learning_rate": 9.207865168539325e-05,
      "loss": 15.0694,
      "step": 162
    },
    {
      "epoch": 18.11111111111111,
      "grad_norm": 18.152341842651367,
      "learning_rate": 9.202247191011237e-05,
      "loss": 17.9802,
      "step": 163
    },
    {
      "epoch": 18.22222222222222,
      "grad_norm": 9.686123847961426,
      "learning_rate": 9.196629213483147e-05,
      "loss": 15.2289,
      "step": 164
    },
    {
      "epoch": 18.333333333333332,
      "grad_norm": 11.63270092010498,
      "learning_rate": 9.191011235955057e-05,
      "loss": 16.0097,
      "step": 165
    },
    {
      "epoch": 18.444444444444443,
      "grad_norm": 15.07314682006836,
      "learning_rate": 9.185393258426966e-05,
      "loss": 14.5371,
      "step": 166
    },
    {
      "epoch": 18.555555555555557,
      "grad_norm": 9.99541187286377,
      "learning_rate": 9.179775280898877e-05,
      "loss": 14.1458,
      "step": 167
    },
    {
      "epoch": 18.666666666666668,
      "grad_norm": 11.150952339172363,
      "learning_rate": 9.174157303370787e-05,
      "loss": 16.4478,
      "step": 168
    },
    {
      "epoch": 18.77777777777778,
      "grad_norm": 17.071674346923828,
      "learning_rate": 9.168539325842696e-05,
      "loss": 14.9491,
      "step": 169
    },
    {
      "epoch": 18.88888888888889,
      "grad_norm": 1.207979440689087,
      "learning_rate": 9.162921348314606e-05,
      "loss": 2.6115,
      "step": 170
    },
    {
      "epoch": 19.0,
      "grad_norm": 18.746849060058594,
      "learning_rate": 9.157303370786518e-05,
      "loss": 15.9589,
      "step": 171
    },
    {
      "epoch": 19.11111111111111,
      "grad_norm": 11.646191596984863,
      "learning_rate": 9.151685393258428e-05,
      "loss": 13.791,
      "step": 172
    },
    {
      "epoch": 19.22222222222222,
      "grad_norm": 12.801342964172363,
      "learning_rate": 9.146067415730337e-05,
      "loss": 14.3418,
      "step": 173
    },
    {
      "epoch": 19.333333333333332,
      "grad_norm": 11.431418418884277,
      "learning_rate": 9.140449438202247e-05,
      "loss": 12.5054,
      "step": 174
    },
    {
      "epoch": 19.444444444444443,
      "grad_norm": 10.350163459777832,
      "learning_rate": 9.134831460674158e-05,
      "loss": 13.6562,
      "step": 175
    },
    {
      "epoch": 19.555555555555557,
      "grad_norm": 9.09384536743164,
      "learning_rate": 9.129213483146067e-05,
      "loss": 11.9178,
      "step": 176
    },
    {
      "epoch": 19.666666666666668,
      "grad_norm": 1.3426125049591064,
      "learning_rate": 9.123595505617977e-05,
      "loss": 2.7295,
      "step": 177
    },
    {
      "epoch": 19.77777777777778,
      "grad_norm": 13.254528045654297,
      "learning_rate": 9.117977528089889e-05,
      "loss": 11.6343,
      "step": 178
    },
    {
      "epoch": 19.88888888888889,
      "grad_norm": 29.743492126464844,
      "learning_rate": 9.112359550561799e-05,
      "loss": 13.9145,
      "step": 179
    },
    {
      "epoch": 20.0,
      "grad_norm": 11.113221168518066,
      "learning_rate": 9.106741573033708e-05,
      "loss": 11.6102,
      "step": 180
    },
    {
      "epoch": 20.11111111111111,
      "grad_norm": 12.57615852355957,
      "learning_rate": 9.101123595505618e-05,
      "loss": 11.1805,
      "step": 181
    },
    {
      "epoch": 20.22222222222222,
      "grad_norm": 13.934369087219238,
      "learning_rate": 9.09550561797753e-05,
      "loss": 10.3117,
      "step": 182
    },
    {
      "epoch": 20.333333333333332,
      "grad_norm": 13.919414520263672,
      "learning_rate": 9.089887640449438e-05,
      "loss": 11.7642,
      "step": 183
    },
    {
      "epoch": 20.444444444444443,
      "grad_norm": 16.063581466674805,
      "learning_rate": 9.084269662921348e-05,
      "loss": 13.2531,
      "step": 184
    },
    {
      "epoch": 20.555555555555557,
      "grad_norm": 11.508731842041016,
      "learning_rate": 9.078651685393259e-05,
      "loss": 9.6711,
      "step": 185
    },
    {
      "epoch": 20.666666666666668,
      "grad_norm": 1.1160142421722412,
      "learning_rate": 9.07303370786517e-05,
      "loss": 2.7083,
      "step": 186
    },
    {
      "epoch": 20.77777777777778,
      "grad_norm": 10.503911018371582,
      "learning_rate": 9.067415730337079e-05,
      "loss": 9.5059,
      "step": 187
    },
    {
      "epoch": 20.88888888888889,
      "grad_norm": 9.673994064331055,
      "learning_rate": 9.061797752808989e-05,
      "loss": 12.2628,
      "step": 188
    },
    {
      "epoch": 21.0,
      "grad_norm": 14.740291595458984,
      "learning_rate": 9.056179775280899e-05,
      "loss": 11.1063,
      "step": 189
    },
    {
      "epoch": 21.11111111111111,
      "grad_norm": 11.396495819091797,
      "learning_rate": 9.050561797752809e-05,
      "loss": 10.5873,
      "step": 190
    },
    {
      "epoch": 21.22222222222222,
      "grad_norm": 9.12985897064209,
      "learning_rate": 9.04494382022472e-05,
      "loss": 9.3032,
      "step": 191
    },
    {
      "epoch": 21.333333333333332,
      "grad_norm": 7.743212699890137,
      "learning_rate": 9.03932584269663e-05,
      "loss": 10.5941,
      "step": 192
    },
    {
      "epoch": 21.444444444444443,
      "grad_norm": 1.2708176374435425,
      "learning_rate": 9.03370786516854e-05,
      "loss": 2.2952,
      "step": 193
    },
    {
      "epoch": 21.555555555555557,
      "grad_norm": 7.256185531616211,
      "learning_rate": 9.02808988764045e-05,
      "loss": 9.8662,
      "step": 194
    },
    {
      "epoch": 21.666666666666668,
      "grad_norm": 9.73060417175293,
      "learning_rate": 9.02247191011236e-05,
      "loss": 9.9381,
      "step": 195
    },
    {
      "epoch": 21.77777777777778,
      "grad_norm": 8.31811237335205,
      "learning_rate": 9.01685393258427e-05,
      "loss": 11.458,
      "step": 196
    },
    {
      "epoch": 21.88888888888889,
      "grad_norm": 7.537487030029297,
      "learning_rate": 9.01123595505618e-05,
      "loss": 9.7019,
      "step": 197
    },
    {
      "epoch": 22.0,
      "grad_norm": 7.606233596801758,
      "learning_rate": 9.00561797752809e-05,
      "loss": 9.3966,
      "step": 198
    },
    {
      "epoch": 22.11111111111111,
      "grad_norm": 9.730469703674316,
      "learning_rate": 9e-05,
      "loss": 9.7222,
      "step": 199
    },
    {
      "epoch": 22.22222222222222,
      "grad_norm": 14.90502643585205,
      "learning_rate": 8.994382022471911e-05,
      "loss": 12.7607,
      "step": 200
    },
    {
      "epoch": 22.333333333333332,
      "grad_norm": 1.1035441160202026,
      "learning_rate": 8.988764044943821e-05,
      "loss": 2.4157,
      "step": 201
    },
    {
      "epoch": 22.444444444444443,
      "grad_norm": 5.924580097198486,
      "learning_rate": 8.983146067415731e-05,
      "loss": 10.2122,
      "step": 202
    },
    {
      "epoch": 22.555555555555557,
      "grad_norm": 6.638684272766113,
      "learning_rate": 8.977528089887641e-05,
      "loss": 10.4441,
      "step": 203
    },
    {
      "epoch": 22.666666666666668,
      "grad_norm": 6.83350133895874,
      "learning_rate": 8.97191011235955e-05,
      "loss": 8.7187,
      "step": 204
    },
    {
      "epoch": 22.77777777777778,
      "grad_norm": 7.287807464599609,
      "learning_rate": 8.966292134831461e-05,
      "loss": 10.0591,
      "step": 205
    },
    {
      "epoch": 22.88888888888889,
      "grad_norm": 5.722039222717285,
      "learning_rate": 8.960674157303372e-05,
      "loss": 9.3612,
      "step": 206
    },
    {
      "epoch": 23.0,
      "grad_norm": 5.08925724029541,
      "learning_rate": 8.95505617977528e-05,
      "loss": 9.4175,
      "step": 207
    },
    {
      "epoch": 23.11111111111111,
      "grad_norm": 6.097468376159668,
      "learning_rate": 8.949438202247192e-05,
      "loss": 8.9299,
      "step": 208
    },
    {
      "epoch": 23.22222222222222,
      "grad_norm": 1.3009953498840332,
      "learning_rate": 8.943820224719102e-05,
      "loss": 2.3768,
      "step": 209
    },
    {
      "epoch": 23.333333333333332,
      "grad_norm": 3.476879358291626,
      "learning_rate": 8.938202247191012e-05,
      "loss": 7.9524,
      "step": 210
    },
    {
      "epoch": 23.444444444444443,
      "grad_norm": 6.211472034454346,
      "learning_rate": 8.932584269662921e-05,
      "loss": 8.9726,
      "step": 211
    },
    {
      "epoch": 23.555555555555557,
      "grad_norm": 5.410786151885986,
      "learning_rate": 8.926966292134832e-05,
      "loss": 8.9856,
      "step": 212
    },
    {
      "epoch": 23.666666666666668,
      "grad_norm": 3.6745870113372803,
      "learning_rate": 8.921348314606743e-05,
      "loss": 8.7578,
      "step": 213
    },
    {
      "epoch": 23.77777777777778,
      "grad_norm": 6.785123825073242,
      "learning_rate": 8.915730337078651e-05,
      "loss": 8.9752,
      "step": 214
    },
    {
      "epoch": 23.88888888888889,
      "grad_norm": 5.6005635261535645,
      "learning_rate": 8.910112359550562e-05,
      "loss": 8.6849,
      "step": 215
    },
    {
      "epoch": 24.0,
      "grad_norm": 3.1263580322265625,
      "learning_rate": 8.904494382022473e-05,
      "loss": 8.1564,
      "step": 216
    },
    {
      "epoch": 24.11111111111111,
      "grad_norm": 7.830447673797607,
      "learning_rate": 8.898876404494383e-05,
      "loss": 8.3152,
      "step": 217
    },
    {
      "epoch": 24.22222222222222,
      "grad_norm": 3.857804298400879,
      "learning_rate": 8.893258426966292e-05,
      "loss": 9.0838,
      "step": 218
    },
    {
      "epoch": 24.333333333333332,
      "grad_norm": 4.104624271392822,
      "learning_rate": 8.887640449438202e-05,
      "loss": 8.3237,
      "step": 219
    },
    {
      "epoch": 24.444444444444443,
      "grad_norm": 6.390974044799805,
      "learning_rate": 8.882022471910114e-05,
      "loss": 8.664,
      "step": 220
    },
    {
      "epoch": 24.555555555555557,
      "grad_norm": 3.3836326599121094,
      "learning_rate": 8.876404494382022e-05,
      "loss": 8.1894,
      "step": 221
    },
    {
      "epoch": 24.666666666666668,
      "grad_norm": 4.804555892944336,
      "learning_rate": 8.870786516853933e-05,
      "loss": 9.301,
      "step": 222
    },
    {
      "epoch": 24.77777777777778,
      "grad_norm": 1.3454900979995728,
      "learning_rate": 8.865168539325843e-05,
      "loss": 2.571,
      "step": 223
    },
    {
      "epoch": 24.88888888888889,
      "grad_norm": 4.566197395324707,
      "learning_rate": 8.859550561797754e-05,
      "loss": 7.597,
      "step": 224
    },
    {
      "epoch": 25.0,
      "grad_norm": 4.248409271240234,
      "learning_rate": 8.853932584269663e-05,
      "loss": 8.1051,
      "step": 225
    },
    {
      "epoch": 25.11111111111111,
      "grad_norm": 5.318970203399658,
      "learning_rate": 8.848314606741573e-05,
      "loss": 8.1103,
      "step": 226
    },
    {
      "epoch": 25.22222222222222,
      "grad_norm": 4.783527374267578,
      "learning_rate": 8.842696629213483e-05,
      "loss": 8.7307,
      "step": 227
    },
    {
      "epoch": 25.333333333333332,
      "grad_norm": 2.5383522510528564,
      "learning_rate": 8.837078651685393e-05,
      "loss": 7.3172,
      "step": 228
    },
    {
      "epoch": 25.444444444444443,
      "grad_norm": 5.5967254638671875,
      "learning_rate": 8.831460674157304e-05,
      "loss": 7.4362,
      "step": 229
    },
    {
      "epoch": 25.555555555555557,
      "grad_norm": 3.9722466468811035,
      "learning_rate": 8.825842696629214e-05,
      "loss": 8.3209,
      "step": 230
    },
    {
      "epoch": 25.666666666666668,
      "grad_norm": 9.216514587402344,
      "learning_rate": 8.820224719101124e-05,
      "loss": 8.7294,
      "step": 231
    },
    {
      "epoch": 25.77777777777778,
      "grad_norm": 5.616478443145752,
      "learning_rate": 8.814606741573034e-05,
      "loss": 8.298,
      "step": 232
    },
    {
      "epoch": 25.88888888888889,
      "grad_norm": 6.107038497924805,
      "learning_rate": 8.808988764044944e-05,
      "loss": 7.6049,
      "step": 233
    },
    {
      "epoch": 26.0,
      "grad_norm": 1.3633052110671997,
      "learning_rate": 8.803370786516854e-05,
      "loss": 2.5869,
      "step": 234
    },
    {
      "epoch": 26.11111111111111,
      "grad_norm": 4.135030269622803,
      "learning_rate": 8.797752808988764e-05,
      "loss": 7.812,
      "step": 235
    },
    {
      "epoch": 26.22222222222222,
      "grad_norm": 3.9537134170532227,
      "learning_rate": 8.792134831460675e-05,
      "loss": 7.9113,
      "step": 236
    },
    {
      "epoch": 26.333333333333332,
      "grad_norm": 3.539741277694702,
      "learning_rate": 8.786516853932585e-05,
      "loss": 7.3715,
      "step": 237
    },
    {
      "epoch": 26.444444444444443,
      "grad_norm": 3.740492343902588,
      "learning_rate": 8.780898876404495e-05,
      "loss": 7.5202,
      "step": 238
    },
    {
      "epoch": 26.555555555555557,
      "grad_norm": 5.767666339874268,
      "learning_rate": 8.775280898876405e-05,
      "loss": 7.8392,
      "step": 239
    },
    {
      "epoch": 26.666666666666668,
      "grad_norm": 4.574343204498291,
      "learning_rate": 8.769662921348315e-05,
      "loss": 7.2051,
      "step": 240
    },
    {
      "epoch": 26.77777777777778,
      "grad_norm": 1.348127007484436,
      "learning_rate": 8.764044943820225e-05,
      "loss": 2.7558,
      "step": 241
    },
    {
      "epoch": 26.88888888888889,
      "grad_norm": 6.069395065307617,
      "learning_rate": 8.758426966292135e-05,
      "loss": 7.7505,
      "step": 242
    },
    {
      "epoch": 27.0,
      "grad_norm": 2.038861036300659,
      "learning_rate": 8.752808988764046e-05,
      "loss": 6.7921,
      "step": 243
    },
    {
      "epoch": 27.11111111111111,
      "grad_norm": 2.9461114406585693,
      "learning_rate": 8.747191011235956e-05,
      "loss": 7.3967,
      "step": 244
    },
    {
      "epoch": 27.22222222222222,
      "grad_norm": 4.552305698394775,
      "learning_rate": 8.741573033707866e-05,
      "loss": 7.5523,
      "step": 245
    },
    {
      "epoch": 27.333333333333332,
      "grad_norm": 1.2273064851760864,
      "learning_rate": 8.735955056179776e-05,
      "loss": 2.3551,
      "step": 246
    },
    {
      "epoch": 27.444444444444443,
      "grad_norm": 2.846050262451172,
      "learning_rate": 8.730337078651686e-05,
      "loss": 6.9969,
      "step": 247
    },
    {
      "epoch": 27.555555555555557,
      "grad_norm": 2.9579782485961914,
      "learning_rate": 8.724719101123596e-05,
      "loss": 6.9482,
      "step": 248
    },
    {
      "epoch": 27.666666666666668,
      "grad_norm": 4.620248794555664,
      "learning_rate": 8.719101123595505e-05,
      "loss": 7.1256,
      "step": 249
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 3.14310359954834,
      "learning_rate": 8.713483146067417e-05,
      "loss": 7.5119,
      "step": 250
    },
    {
      "epoch": 27.88888888888889,
      "grad_norm": 3.0373117923736572,
      "learning_rate": 8.707865168539327e-05,
      "loss": 7.2165,
      "step": 251
    },
    {
      "epoch": 28.0,
      "grad_norm": 2.2513113021850586,
      "learning_rate": 8.702247191011237e-05,
      "loss": 6.9398,
      "step": 252
    },
    {
      "epoch": 28.11111111111111,
      "grad_norm": 3.0733251571655273,
      "learning_rate": 8.696629213483146e-05,
      "loss": 6.6442,
      "step": 253
    },
    {
      "epoch": 28.22222222222222,
      "grad_norm": 5.2668046951293945,
      "learning_rate": 8.691011235955057e-05,
      "loss": 7.063,
      "step": 254
    },
    {
      "epoch": 28.333333333333332,
      "grad_norm": 4.14793062210083,
      "learning_rate": 8.685393258426967e-05,
      "loss": 6.7663,
      "step": 255
    },
    {
      "epoch": 28.444444444444443,
      "grad_norm": 3.11582612991333,
      "learning_rate": 8.679775280898876e-05,
      "loss": 6.9954,
      "step": 256
    },
    {
      "epoch": 28.555555555555557,
      "grad_norm": 3.377105236053467,
      "learning_rate": 8.674157303370786e-05,
      "loss": 7.4446,
      "step": 257
    },
    {
      "epoch": 28.666666666666668,
      "grad_norm": 3.1191861629486084,
      "learning_rate": 8.668539325842698e-05,
      "loss": 6.6485,
      "step": 258
    },
    {
      "epoch": 28.77777777777778,
      "grad_norm": 1.4291452169418335,
      "learning_rate": 8.662921348314608e-05,
      "loss": 2.5214,
      "step": 259
    },
    {
      "epoch": 28.88888888888889,
      "grad_norm": 4.4624552726745605,
      "learning_rate": 8.657303370786517e-05,
      "loss": 6.948,
      "step": 260
    },
    {
      "epoch": 29.0,
      "grad_norm": 3.3965280055999756,
      "learning_rate": 8.651685393258427e-05,
      "loss": 6.0718,
      "step": 261
    },
    {
      "epoch": 29.11111111111111,
      "grad_norm": 3.9042789936065674,
      "learning_rate": 8.646067415730338e-05,
      "loss": 6.7388,
      "step": 262
    },
    {
      "epoch": 29.22222222222222,
      "grad_norm": 3.5615100860595703,
      "learning_rate": 8.640449438202247e-05,
      "loss": 6.3377,
      "step": 263
    },
    {
      "epoch": 29.333333333333332,
      "grad_norm": 3.005864381790161,
      "learning_rate": 8.634831460674157e-05,
      "loss": 6.4977,
      "step": 264
    },
    {
      "epoch": 29.444444444444443,
      "grad_norm": 4.265782833099365,
      "learning_rate": 8.629213483146069e-05,
      "loss": 6.7038,
      "step": 265
    },
    {
      "epoch": 29.555555555555557,
      "grad_norm": 1.3731358051300049,
      "learning_rate": 8.623595505617979e-05,
      "loss": 2.33,
      "step": 266
    },
    {
      "epoch": 29.666666666666668,
      "grad_norm": 3.4663925170898438,
      "learning_rate": 8.617977528089888e-05,
      "loss": 6.4694,
      "step": 267
    },
    {
      "epoch": 29.77777777777778,
      "grad_norm": 2.1627633571624756,
      "learning_rate": 8.612359550561798e-05,
      "loss": 6.2271,
      "step": 268
    },
    {
      "epoch": 29.88888888888889,
      "grad_norm": 3.7015540599823,
      "learning_rate": 8.606741573033709e-05,
      "loss": 6.5626,
      "step": 269
    },
    {
      "epoch": 30.0,
      "grad_norm": 2.317176103591919,
      "learning_rate": 8.601123595505618e-05,
      "loss": 6.2005,
      "step": 270
    },
    {
      "epoch": 30.11111111111111,
      "grad_norm": 3.1613094806671143,
      "learning_rate": 8.595505617977528e-05,
      "loss": 6.2087,
      "step": 271
    },
    {
      "epoch": 30.22222222222222,
      "grad_norm": 2.9412970542907715,
      "learning_rate": 8.589887640449438e-05,
      "loss": 6.4239,
      "step": 272
    },
    {
      "epoch": 30.333333333333332,
      "grad_norm": 3.3974297046661377,
      "learning_rate": 8.58426966292135e-05,
      "loss": 6.142,
      "step": 273
    },
    {
      "epoch": 30.444444444444443,
      "grad_norm": 1.5227817296981812,
      "learning_rate": 8.578651685393259e-05,
      "loss": 6.0555,
      "step": 274
    },
    {
      "epoch": 30.555555555555557,
      "grad_norm": 3.410670518875122,
      "learning_rate": 8.573033707865169e-05,
      "loss": 6.4473,
      "step": 275
    },
    {
      "epoch": 30.666666666666668,
      "grad_norm": 2.7631657123565674,
      "learning_rate": 8.567415730337079e-05,
      "loss": 5.986,
      "step": 276
    },
    {
      "epoch": 30.77777777777778,
      "grad_norm": 2.6646406650543213,
      "learning_rate": 8.561797752808989e-05,
      "loss": 6.3493,
      "step": 277
    },
    {
      "epoch": 30.88888888888889,
      "grad_norm": 2.368398904800415,
      "learning_rate": 8.556179775280899e-05,
      "loss": 5.9832,
      "step": 278
    },
    {
      "epoch": 31.0,
      "grad_norm": 1.4958972930908203,
      "learning_rate": 8.550561797752809e-05,
      "loss": 2.2539,
      "step": 279
    },
    {
      "epoch": 31.11111111111111,
      "grad_norm": 2.5810797214508057,
      "learning_rate": 8.54494382022472e-05,
      "loss": 5.9695,
      "step": 280
    },
    {
      "epoch": 31.22222222222222,
      "grad_norm": 2.460857629776001,
      "learning_rate": 8.53932584269663e-05,
      "loss": 5.6621,
      "step": 281
    },
    {
      "epoch": 31.333333333333332,
      "grad_norm": 4.662493705749512,
      "learning_rate": 8.53370786516854e-05,
      "loss": 5.7704,
      "step": 282
    },
    {
      "epoch": 31.444444444444443,
      "grad_norm": 11.866541862487793,
      "learning_rate": 8.52808988764045e-05,
      "loss": 6.6918,
      "step": 283
    },
    {
      "epoch": 31.555555555555557,
      "grad_norm": 3.1857521533966064,
      "learning_rate": 8.52247191011236e-05,
      "loss": 5.729,
      "step": 284
    },
    {
      "epoch": 31.666666666666668,
      "grad_norm": 4.203891277313232,
      "learning_rate": 8.51685393258427e-05,
      "loss": 5.74,
      "step": 285
    },
    {
      "epoch": 31.77777777777778,
      "grad_norm": 2.834185838699341,
      "learning_rate": 8.51123595505618e-05,
      "loss": 5.5611,
      "step": 286
    },
    {
      "epoch": 31.88888888888889,
      "grad_norm": 4.018552780151367,
      "learning_rate": 8.505617977528089e-05,
      "loss": 5.662,
      "step": 287
    },
    {
      "epoch": 32.0,
      "grad_norm": 1.3435055017471313,
      "learning_rate": 8.5e-05,
      "loss": 2.4475,
      "step": 288
    },
    {
      "epoch": 32.111111111111114,
      "grad_norm": 3.898031234741211,
      "learning_rate": 8.494382022471911e-05,
      "loss": 5.2405,
      "step": 289
    },
    {
      "epoch": 32.22222222222222,
      "grad_norm": 1.4898983240127563,
      "learning_rate": 8.488764044943821e-05,
      "loss": 5.2652,
      "step": 290
    },
    {
      "epoch": 32.333333333333336,
      "grad_norm": 1.3230998516082764,
      "learning_rate": 8.483146067415731e-05,
      "loss": 2.1175,
      "step": 291
    },
    {
      "epoch": 32.44444444444444,
      "grad_norm": 1.3552411794662476,
      "learning_rate": 8.477528089887641e-05,
      "loss": 4.9102,
      "step": 292
    },
    {
      "epoch": 32.55555555555556,
      "grad_norm": 3.5235109329223633,
      "learning_rate": 8.471910112359551e-05,
      "loss": 5.5008,
      "step": 293
    },
    {
      "epoch": 32.666666666666664,
      "grad_norm": 2.0943002700805664,
      "learning_rate": 8.46629213483146e-05,
      "loss": 5.6722,
      "step": 294
    },
    {
      "epoch": 32.77777777777778,
      "grad_norm": 2.1282479763031006,
      "learning_rate": 8.460674157303372e-05,
      "loss": 5.2405,
      "step": 295
    },
    {
      "epoch": 32.888888888888886,
      "grad_norm": 8.12448787689209,
      "learning_rate": 8.455056179775282e-05,
      "loss": 5.3909,
      "step": 296
    },
    {
      "epoch": 33.0,
      "grad_norm": 3.1253697872161865,
      "learning_rate": 8.449438202247192e-05,
      "loss": 5.2321,
      "step": 297
    },
    {
      "epoch": 33.111111111111114,
      "grad_norm": 5.989770412445068,
      "learning_rate": 8.443820224719101e-05,
      "loss": 5.2492,
      "step": 298
    },
    {
      "epoch": 33.22222222222222,
      "grad_norm": 3.908404588699341,
      "learning_rate": 8.438202247191012e-05,
      "loss": 5.5218,
      "step": 299
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 2.5055201053619385,
      "learning_rate": 8.432584269662922e-05,
      "loss": 5.3623,
      "step": 300
    },
    {
      "epoch": 33.44444444444444,
      "grad_norm": 1.2858577966690063,
      "learning_rate": 8.426966292134831e-05,
      "loss": 2.4141,
      "step": 301
    },
    {
      "epoch": 33.55555555555556,
      "grad_norm": 2.909522771835327,
      "learning_rate": 8.421348314606741e-05,
      "loss": 5.1918,
      "step": 302
    },
    {
      "epoch": 33.666666666666664,
      "grad_norm": 1.6729270219802856,
      "learning_rate": 8.415730337078653e-05,
      "loss": 5.2138,
      "step": 303
    },
    {
      "epoch": 33.77777777777778,
      "grad_norm": 2.3882150650024414,
      "learning_rate": 8.410112359550563e-05,
      "loss": 5.086,
      "step": 304
    },
    {
      "epoch": 33.888888888888886,
      "grad_norm": 3.11972975730896,
      "learning_rate": 8.404494382022472e-05,
      "loss": 5.2964,
      "step": 305
    },
    {
      "epoch": 34.0,
      "grad_norm": 2.004559278488159,
      "learning_rate": 8.398876404494382e-05,
      "loss": 5.5524,
      "step": 306
    },
    {
      "epoch": 34.111111111111114,
      "grad_norm": 1.9932743310928345,
      "learning_rate": 8.393258426966293e-05,
      "loss": 5.0394,
      "step": 307
    },
    {
      "epoch": 34.22222222222222,
      "grad_norm": 1.5428160429000854,
      "learning_rate": 8.387640449438202e-05,
      "loss": 5.1479,
      "step": 308
    },
    {
      "epoch": 34.333333333333336,
      "grad_norm": 1.6902105808258057,
      "learning_rate": 8.382022471910112e-05,
      "loss": 5.1415,
      "step": 309
    },
    {
      "epoch": 34.44444444444444,
      "grad_norm": 1.285733938217163,
      "learning_rate": 8.376404494382022e-05,
      "loss": 2.2652,
      "step": 310
    },
    {
      "epoch": 34.55555555555556,
      "grad_norm": 2.012539863586426,
      "learning_rate": 8.370786516853934e-05,
      "loss": 5.0625,
      "step": 311
    },
    {
      "epoch": 34.666666666666664,
      "grad_norm": 2.3265891075134277,
      "learning_rate": 8.365168539325843e-05,
      "loss": 5.2409,
      "step": 312
    },
    {
      "epoch": 34.77777777777778,
      "grad_norm": 2.727707624435425,
      "learning_rate": 8.359550561797753e-05,
      "loss": 5.2021,
      "step": 313
    },
    {
      "epoch": 34.888888888888886,
      "grad_norm": 3.0693647861480713,
      "learning_rate": 8.353932584269663e-05,
      "loss": 5.3055,
      "step": 314
    },
    {
      "epoch": 35.0,
      "grad_norm": 23.97614860534668,
      "learning_rate": 8.348314606741573e-05,
      "loss": 6.1542,
      "step": 315
    },
    {
      "epoch": 35.111111111111114,
      "grad_norm": 1.3750091791152954,
      "learning_rate": 8.342696629213483e-05,
      "loss": 4.7884,
      "step": 316
    },
    {
      "epoch": 35.22222222222222,
      "grad_norm": 2.4181342124938965,
      "learning_rate": 8.337078651685393e-05,
      "loss": 5.2891,
      "step": 317
    },
    {
      "epoch": 35.333333333333336,
      "grad_norm": 1.0607669353485107,
      "learning_rate": 8.331460674157305e-05,
      "loss": 4.8692,
      "step": 318
    },
    {
      "epoch": 35.44444444444444,
      "grad_norm": 1.735119342803955,
      "learning_rate": 8.325842696629214e-05,
      "loss": 5.2272,
      "step": 319
    },
    {
      "epoch": 35.55555555555556,
      "grad_norm": 1.4121812582015991,
      "learning_rate": 8.320224719101124e-05,
      "loss": 2.1671,
      "step": 320
    },
    {
      "epoch": 35.666666666666664,
      "grad_norm": 2.617030143737793,
      "learning_rate": 8.314606741573034e-05,
      "loss": 5.0363,
      "step": 321
    },
    {
      "epoch": 35.77777777777778,
      "grad_norm": 4.239237308502197,
      "learning_rate": 8.308988764044944e-05,
      "loss": 5.5931,
      "step": 322
    },
    {
      "epoch": 35.888888888888886,
      "grad_norm": 5.066397190093994,
      "learning_rate": 8.303370786516854e-05,
      "loss": 4.981,
      "step": 323
    },
    {
      "epoch": 36.0,
      "grad_norm": 3.5869994163513184,
      "learning_rate": 8.297752808988764e-05,
      "loss": 5.495,
      "step": 324
    },
    {
      "epoch": 36.111111111111114,
      "grad_norm": 16.138212203979492,
      "learning_rate": 8.292134831460675e-05,
      "loss": 5.3921,
      "step": 325
    },
    {
      "epoch": 36.22222222222222,
      "grad_norm": 1.3816962242126465,
      "learning_rate": 8.286516853932585e-05,
      "loss": 2.3326,
      "step": 326
    },
    {
      "epoch": 36.333333333333336,
      "grad_norm": 1.365923523902893,
      "learning_rate": 8.280898876404495e-05,
      "loss": 4.9118,
      "step": 327
    },
    {
      "epoch": 36.44444444444444,
      "grad_norm": 4.724794864654541,
      "learning_rate": 8.275280898876405e-05,
      "loss": 5.3762,
      "step": 328
    },
    {
      "epoch": 36.55555555555556,
      "grad_norm": 1.9953824281692505,
      "learning_rate": 8.269662921348315e-05,
      "loss": 4.6658,
      "step": 329
    },
    {
      "epoch": 36.666666666666664,
      "grad_norm": 1.2744804620742798,
      "learning_rate": 8.264044943820225e-05,
      "loss": 4.8622,
      "step": 330
    },
    {
      "epoch": 36.77777777777778,
      "grad_norm": 7.266758441925049,
      "learning_rate": 8.258426966292135e-05,
      "loss": 5.1314,
      "step": 331
    },
    {
      "epoch": 36.888888888888886,
      "grad_norm": 3.1960270404815674,
      "learning_rate": 8.252808988764046e-05,
      "loss": 5.0038,
      "step": 332
    },
    {
      "epoch": 37.0,
      "grad_norm": 1.0800294876098633,
      "learning_rate": 8.247191011235956e-05,
      "loss": 4.8293,
      "step": 333
    },
    {
      "epoch": 37.111111111111114,
      "grad_norm": 2.7983016967773438,
      "learning_rate": 8.241573033707866e-05,
      "loss": 5.413,
      "step": 334
    },
    {
      "epoch": 37.22222222222222,
      "grad_norm": 4.04197883605957,
      "learning_rate": 8.235955056179776e-05,
      "loss": 2.1913,
      "step": 335
    },
    {
      "epoch": 37.333333333333336,
      "grad_norm": 1.8463692665100098,
      "learning_rate": 8.230337078651685e-05,
      "loss": 4.9112,
      "step": 336
    },
    {
      "epoch": 37.44444444444444,
      "grad_norm": 2.115741491317749,
      "learning_rate": 8.224719101123596e-05,
      "loss": 5.0901,
      "step": 337
    },
    {
      "epoch": 37.55555555555556,
      "grad_norm": 3.874972343444824,
      "learning_rate": 8.219101123595506e-05,
      "loss": 4.7796,
      "step": 338
    },
    {
      "epoch": 37.666666666666664,
      "grad_norm": 6.9378767013549805,
      "learning_rate": 8.213483146067417e-05,
      "loss": 5.0996,
      "step": 339
    },
    {
      "epoch": 37.77777777777778,
      "grad_norm": 2.8197362422943115,
      "learning_rate": 8.207865168539325e-05,
      "loss": 4.8348,
      "step": 340
    },
    {
      "epoch": 37.888888888888886,
      "grad_norm": 1.8880693912506104,
      "learning_rate": 8.202247191011237e-05,
      "loss": 4.7636,
      "step": 341
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.8996684551239014,
      "learning_rate": 8.196629213483147e-05,
      "loss": 4.7014,
      "step": 342
    },
    {
      "epoch": 38.111111111111114,
      "grad_norm": 3.356069564819336,
      "learning_rate": 8.191011235955056e-05,
      "loss": 5.1101,
      "step": 343
    },
    {
      "epoch": 38.22222222222222,
      "grad_norm": 3.875434637069702,
      "learning_rate": 8.185393258426966e-05,
      "loss": 4.9544,
      "step": 344
    },
    {
      "epoch": 38.333333333333336,
      "grad_norm": 3.525409460067749,
      "learning_rate": 8.179775280898877e-05,
      "loss": 4.9302,
      "step": 345
    },
    {
      "epoch": 38.44444444444444,
      "grad_norm": 2.0655934810638428,
      "learning_rate": 8.174157303370788e-05,
      "loss": 4.7299,
      "step": 346
    },
    {
      "epoch": 38.55555555555556,
      "grad_norm": 0.7833979725837708,
      "learning_rate": 8.168539325842696e-05,
      "loss": 4.6664,
      "step": 347
    },
    {
      "epoch": 38.666666666666664,
      "grad_norm": 1.6664048433303833,
      "learning_rate": 8.162921348314608e-05,
      "loss": 2.2499,
      "step": 348
    },
    {
      "epoch": 38.77777777777778,
      "grad_norm": 3.053168296813965,
      "learning_rate": 8.157303370786518e-05,
      "loss": 4.7977,
      "step": 349
    },
    {
      "epoch": 38.888888888888886,
      "grad_norm": 2.0877573490142822,
      "learning_rate": 8.151685393258427e-05,
      "loss": 4.6269,
      "step": 350
    },
    {
      "epoch": 39.0,
      "grad_norm": 1.0337730646133423,
      "learning_rate": 8.146067415730337e-05,
      "loss": 4.6176,
      "step": 351
    },
    {
      "epoch": 39.111111111111114,
      "grad_norm": 0.6426941156387329,
      "learning_rate": 8.140449438202248e-05,
      "loss": 4.5048,
      "step": 352
    },
    {
      "epoch": 39.22222222222222,
      "grad_norm": 0.7156606912612915,
      "learning_rate": 8.134831460674159e-05,
      "loss": 4.5327,
      "step": 353
    },
    {
      "epoch": 39.333333333333336,
      "grad_norm": 5.550042629241943,
      "learning_rate": 8.129213483146067e-05,
      "loss": 4.6614,
      "step": 354
    },
    {
      "epoch": 39.44444444444444,
      "grad_norm": 1.3863157033920288,
      "learning_rate": 8.123595505617978e-05,
      "loss": 2.1073,
      "step": 355
    },
    {
      "epoch": 39.55555555555556,
      "grad_norm": 0.9574163556098938,
      "learning_rate": 8.117977528089889e-05,
      "loss": 4.7108,
      "step": 356
    },
    {
      "epoch": 39.666666666666664,
      "grad_norm": 0.991446316242218,
      "learning_rate": 8.112359550561798e-05,
      "loss": 4.606,
      "step": 357
    },
    {
      "epoch": 39.77777777777778,
      "grad_norm": 0.8715350031852722,
      "learning_rate": 8.106741573033708e-05,
      "loss": 4.6105,
      "step": 358
    },
    {
      "epoch": 39.888888888888886,
      "grad_norm": 1.7547969818115234,
      "learning_rate": 8.101123595505618e-05,
      "loss": 4.5637,
      "step": 359
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.8200392127037048,
      "learning_rate": 8.095505617977528e-05,
      "loss": 4.5181,
      "step": 360
    },
    {
      "epoch": 40.111111111111114,
      "grad_norm": 3.1406748294830322,
      "learning_rate": 8.089887640449438e-05,
      "loss": 4.7423,
      "step": 361
    },
    {
      "epoch": 40.22222222222222,
      "grad_norm": 1.341982364654541,
      "learning_rate": 8.084269662921349e-05,
      "loss": 4.5494,
      "step": 362
    },
    {
      "epoch": 40.333333333333336,
      "grad_norm": 5.21439790725708,
      "learning_rate": 8.078651685393259e-05,
      "loss": 4.8965,
      "step": 363
    },
    {
      "epoch": 40.44444444444444,
      "grad_norm": 0.7177472710609436,
      "learning_rate": 8.073033707865169e-05,
      "loss": 4.441,
      "step": 364
    },
    {
      "epoch": 40.55555555555556,
      "grad_norm": 2.643939733505249,
      "learning_rate": 8.067415730337079e-05,
      "loss": 4.7271,
      "step": 365
    },
    {
      "epoch": 40.666666666666664,
      "grad_norm": 1.777026653289795,
      "learning_rate": 8.061797752808989e-05,
      "loss": 2.2247,
      "step": 366
    },
    {
      "epoch": 40.77777777777778,
      "grad_norm": 0.5136377811431885,
      "learning_rate": 8.056179775280899e-05,
      "loss": 4.4046,
      "step": 367
    },
    {
      "epoch": 40.888888888888886,
      "grad_norm": 0.898208737373352,
      "learning_rate": 8.05056179775281e-05,
      "loss": 4.4306,
      "step": 368
    },
    {
      "epoch": 41.0,
      "grad_norm": 2.483487606048584,
      "learning_rate": 8.04494382022472e-05,
      "loss": 4.6091,
      "step": 369
    },
    {
      "epoch": 41.111111111111114,
      "grad_norm": 1.8927232027053833,
      "learning_rate": 8.03932584269663e-05,
      "loss": 4.5036,
      "step": 370
    },
    {
      "epoch": 41.22222222222222,
      "grad_norm": 4.074737548828125,
      "learning_rate": 8.03370786516854e-05,
      "loss": 4.9224,
      "step": 371
    },
    {
      "epoch": 41.333333333333336,
      "grad_norm": 3.4887712001800537,
      "learning_rate": 8.02808988764045e-05,
      "loss": 4.4358,
      "step": 372
    },
    {
      "epoch": 41.44444444444444,
      "grad_norm": 1.3672540187835693,
      "learning_rate": 8.02247191011236e-05,
      "loss": 4.6031,
      "step": 373
    },
    {
      "epoch": 41.55555555555556,
      "grad_norm": 0.9302532076835632,
      "learning_rate": 8.016853932584269e-05,
      "loss": 4.5856,
      "step": 374
    },
    {
      "epoch": 41.666666666666664,
      "grad_norm": 3.6052937507629395,
      "learning_rate": 8.01123595505618e-05,
      "loss": 5.4888,
      "step": 375
    },
    {
      "epoch": 41.77777777777778,
      "grad_norm": 1.1806856393814087,
      "learning_rate": 8.00561797752809e-05,
      "loss": 2.0953,
      "step": 376
    },
    {
      "epoch": 41.888888888888886,
      "grad_norm": 6.062559127807617,
      "learning_rate": 8e-05,
      "loss": 4.9864,
      "step": 377
    },
    {
      "epoch": 42.0,
      "grad_norm": 0.9252751469612122,
      "learning_rate": 7.994382022471911e-05,
      "loss": 4.5246,
      "step": 378
    },
    {
      "epoch": 42.111111111111114,
      "grad_norm": 1.0950506925582886,
      "learning_rate": 7.988764044943821e-05,
      "loss": 4.4292,
      "step": 379
    },
    {
      "epoch": 42.22222222222222,
      "grad_norm": 0.6841261386871338,
      "learning_rate": 7.983146067415731e-05,
      "loss": 4.3937,
      "step": 380
    },
    {
      "epoch": 42.333333333333336,
      "grad_norm": 0.8444461226463318,
      "learning_rate": 7.97752808988764e-05,
      "loss": 4.5274,
      "step": 381
    },
    {
      "epoch": 42.44444444444444,
      "grad_norm": 4.412860870361328,
      "learning_rate": 7.971910112359551e-05,
      "loss": 4.9845,
      "step": 382
    },
    {
      "epoch": 42.55555555555556,
      "grad_norm": 2.8409998416900635,
      "learning_rate": 7.966292134831462e-05,
      "loss": 5.0861,
      "step": 383
    },
    {
      "epoch": 42.666666666666664,
      "grad_norm": 1.7295665740966797,
      "learning_rate": 7.960674157303372e-05,
      "loss": 1.9512,
      "step": 384
    },
    {
      "epoch": 42.77777777777778,
      "grad_norm": 0.6857701539993286,
      "learning_rate": 7.95505617977528e-05,
      "loss": 4.2941,
      "step": 385
    },
    {
      "epoch": 42.888888888888886,
      "grad_norm": 4.712216377258301,
      "learning_rate": 7.949438202247192e-05,
      "loss": 4.8823,
      "step": 386
    },
    {
      "epoch": 43.0,
      "grad_norm": 6.212260723114014,
      "learning_rate": 7.943820224719102e-05,
      "loss": 4.9426,
      "step": 387
    },
    {
      "epoch": 43.111111111111114,
      "grad_norm": 1.6041609048843384,
      "learning_rate": 7.938202247191011e-05,
      "loss": 2.2044,
      "step": 388
    },
    {
      "epoch": 43.22222222222222,
      "grad_norm": 3.8103244304656982,
      "learning_rate": 7.932584269662921e-05,
      "loss": 4.9588,
      "step": 389
    },
    {
      "epoch": 43.333333333333336,
      "grad_norm": 7.115278244018555,
      "learning_rate": 7.926966292134833e-05,
      "loss": 4.5004,
      "step": 390
    },
    {
      "epoch": 43.44444444444444,
      "grad_norm": 3.362086772918701,
      "learning_rate": 7.921348314606743e-05,
      "loss": 4.6937,
      "step": 391
    },
    {
      "epoch": 43.55555555555556,
      "grad_norm": 0.9698696136474609,
      "learning_rate": 7.915730337078651e-05,
      "loss": 4.301,
      "step": 392
    },
    {
      "epoch": 43.666666666666664,
      "grad_norm": 4.475214004516602,
      "learning_rate": 7.910112359550562e-05,
      "loss": 4.5549,
      "step": 393
    },
    {
      "epoch": 43.77777777777778,
      "grad_norm": 4.664597034454346,
      "learning_rate": 7.904494382022473e-05,
      "loss": 4.6303,
      "step": 394
    },
    {
      "epoch": 43.888888888888886,
      "grad_norm": 0.6380846500396729,
      "learning_rate": 7.898876404494382e-05,
      "loss": 4.4146,
      "step": 395
    },
    {
      "epoch": 44.0,
      "grad_norm": 5.440959453582764,
      "learning_rate": 7.893258426966292e-05,
      "loss": 5.5399,
      "step": 396
    },
    {
      "epoch": 44.111111111111114,
      "grad_norm": 0.8845913410186768,
      "learning_rate": 7.887640449438202e-05,
      "loss": 4.3385,
      "step": 397
    },
    {
      "epoch": 44.22222222222222,
      "grad_norm": 1.7018001079559326,
      "learning_rate": 7.882022471910114e-05,
      "loss": 2.0492,
      "step": 398
    },
    {
      "epoch": 44.333333333333336,
      "grad_norm": 1.2892179489135742,
      "learning_rate": 7.876404494382022e-05,
      "loss": 4.4752,
      "step": 399
    },
    {
      "epoch": 44.44444444444444,
      "grad_norm": 0.671755850315094,
      "learning_rate": 7.870786516853933e-05,
      "loss": 4.356,
      "step": 400
    },
    {
      "epoch": 44.55555555555556,
      "grad_norm": 1.149139642715454,
      "learning_rate": 7.865168539325843e-05,
      "loss": 4.458,
      "step": 401
    },
    {
      "epoch": 44.666666666666664,
      "grad_norm": 0.7319596409797668,
      "learning_rate": 7.859550561797753e-05,
      "loss": 4.5619,
      "step": 402
    },
    {
      "epoch": 44.77777777777778,
      "grad_norm": 1.7291542291641235,
      "learning_rate": 7.853932584269663e-05,
      "loss": 4.4128,
      "step": 403
    },
    {
      "epoch": 44.888888888888886,
      "grad_norm": 1.1125882863998413,
      "learning_rate": 7.848314606741573e-05,
      "loss": 4.2721,
      "step": 404
    },
    {
      "epoch": 45.0,
      "grad_norm": 6.463366985321045,
      "learning_rate": 7.842696629213485e-05,
      "loss": 4.8906,
      "step": 405
    },
    {
      "epoch": 45.111111111111114,
      "grad_norm": 2.8382976055145264,
      "learning_rate": 7.837078651685393e-05,
      "loss": 4.9496,
      "step": 406
    },
    {
      "epoch": 45.22222222222222,
      "grad_norm": 3.9749417304992676,
      "learning_rate": 7.831460674157304e-05,
      "loss": 4.572,
      "step": 407
    },
    {
      "epoch": 45.333333333333336,
      "grad_norm": 0.7719923853874207,
      "learning_rate": 7.825842696629214e-05,
      "loss": 4.3301,
      "step": 408
    },
    {
      "epoch": 45.44444444444444,
      "grad_norm": 0.6875223517417908,
      "learning_rate": 7.820224719101124e-05,
      "loss": 4.2199,
      "step": 409
    },
    {
      "epoch": 45.55555555555556,
      "grad_norm": 0.5664939880371094,
      "learning_rate": 7.814606741573034e-05,
      "loss": 4.2829,
      "step": 410
    },
    {
      "epoch": 45.666666666666664,
      "grad_norm": 1.6620351076126099,
      "learning_rate": 7.808988764044944e-05,
      "loss": 4.3639,
      "step": 411
    },
    {
      "epoch": 45.77777777777778,
      "grad_norm": 2.5314412117004395,
      "learning_rate": 7.803370786516854e-05,
      "loss": 4.5617,
      "step": 412
    },
    {
      "epoch": 45.888888888888886,
      "grad_norm": 1.6974513530731201,
      "learning_rate": 7.797752808988764e-05,
      "loss": 4.384,
      "step": 413
    },
    {
      "epoch": 46.0,
      "grad_norm": 3.5987653732299805,
      "learning_rate": 7.792134831460675e-05,
      "loss": 2.3715,
      "step": 414
    },
    {
      "epoch": 46.111111111111114,
      "grad_norm": 0.9477805495262146,
      "learning_rate": 7.786516853932585e-05,
      "loss": 4.2401,
      "step": 415
    },
    {
      "epoch": 46.22222222222222,
      "grad_norm": 0.6705040335655212,
      "learning_rate": 7.780898876404495e-05,
      "loss": 4.2805,
      "step": 416
    },
    {
      "epoch": 46.333333333333336,
      "grad_norm": 1.1834800243377686,
      "learning_rate": 7.775280898876405e-05,
      "loss": 4.3099,
      "step": 417
    },
    {
      "epoch": 46.44444444444444,
      "grad_norm": 1.8867253065109253,
      "learning_rate": 7.769662921348315e-05,
      "loss": 4.218,
      "step": 418
    },
    {
      "epoch": 46.55555555555556,
      "grad_norm": 0.7847224473953247,
      "learning_rate": 7.764044943820225e-05,
      "loss": 4.3443,
      "step": 419
    },
    {
      "epoch": 46.666666666666664,
      "grad_norm": 2.3005247116088867,
      "learning_rate": 7.758426966292135e-05,
      "loss": 4.6018,
      "step": 420
    },
    {
      "epoch": 46.77777777777778,
      "grad_norm": 3.579098701477051,
      "learning_rate": 7.752808988764046e-05,
      "loss": 4.371,
      "step": 421
    },
    {
      "epoch": 46.888888888888886,
      "grad_norm": 1.3968989849090576,
      "learning_rate": 7.747191011235956e-05,
      "loss": 1.8596,
      "step": 422
    },
    {
      "epoch": 47.0,
      "grad_norm": 3.914358615875244,
      "learning_rate": 7.741573033707865e-05,
      "loss": 4.4075,
      "step": 423
    },
    {
      "epoch": 47.111111111111114,
      "grad_norm": 1.4513299465179443,
      "learning_rate": 7.735955056179776e-05,
      "loss": 1.8137,
      "step": 424
    },
    {
      "epoch": 47.22222222222222,
      "grad_norm": 1.5922627449035645,
      "learning_rate": 7.730337078651686e-05,
      "loss": 4.2782,
      "step": 425
    },
    {
      "epoch": 47.333333333333336,
      "grad_norm": 3.7150845527648926,
      "learning_rate": 7.724719101123596e-05,
      "loss": 4.4622,
      "step": 426
    },
    {
      "epoch": 47.44444444444444,
      "grad_norm": 0.9232781529426575,
      "learning_rate": 7.719101123595505e-05,
      "loss": 4.2216,
      "step": 427
    },
    {
      "epoch": 47.55555555555556,
      "grad_norm": 0.712211012840271,
      "learning_rate": 7.713483146067417e-05,
      "loss": 4.2069,
      "step": 428
    },
    {
      "epoch": 47.666666666666664,
      "grad_norm": 0.6520928740501404,
      "learning_rate": 7.707865168539327e-05,
      "loss": 4.1451,
      "step": 429
    },
    {
      "epoch": 47.77777777777778,
      "grad_norm": 0.6939025521278381,
      "learning_rate": 7.702247191011236e-05,
      "loss": 4.225,
      "step": 430
    },
    {
      "epoch": 47.888888888888886,
      "grad_norm": 0.8033818006515503,
      "learning_rate": 7.696629213483147e-05,
      "loss": 4.175,
      "step": 431
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.830778956413269,
      "learning_rate": 7.691011235955057e-05,
      "loss": 4.3094,
      "step": 432
    },
    {
      "epoch": 48.111111111111114,
      "grad_norm": 0.7951034903526306,
      "learning_rate": 7.685393258426966e-05,
      "loss": 4.255,
      "step": 433
    },
    {
      "epoch": 48.22222222222222,
      "grad_norm": 0.5756697654724121,
      "learning_rate": 7.679775280898876e-05,
      "loss": 4.2828,
      "step": 434
    },
    {
      "epoch": 48.333333333333336,
      "grad_norm": 0.7007978558540344,
      "learning_rate": 7.674157303370788e-05,
      "loss": 4.2503,
      "step": 435
    },
    {
      "epoch": 48.44444444444444,
      "grad_norm": 4.248934268951416,
      "learning_rate": 7.668539325842698e-05,
      "loss": 4.3126,
      "step": 436
    },
    {
      "epoch": 48.55555555555556,
      "grad_norm": 0.7660558819770813,
      "learning_rate": 7.662921348314607e-05,
      "loss": 4.1929,
      "step": 437
    },
    {
      "epoch": 48.666666666666664,
      "grad_norm": 0.9505344033241272,
      "learning_rate": 7.657303370786517e-05,
      "loss": 4.1856,
      "step": 438
    },
    {
      "epoch": 48.77777777777778,
      "grad_norm": 4.059176445007324,
      "learning_rate": 7.651685393258428e-05,
      "loss": 4.2163,
      "step": 439
    },
    {
      "epoch": 48.888888888888886,
      "grad_norm": 1.499725341796875,
      "learning_rate": 7.646067415730337e-05,
      "loss": 1.8843,
      "step": 440
    },
    {
      "epoch": 49.0,
      "grad_norm": 0.6221630573272705,
      "learning_rate": 7.640449438202247e-05,
      "loss": 4.1314,
      "step": 441
    },
    {
      "epoch": 49.111111111111114,
      "grad_norm": 0.7565340995788574,
      "learning_rate": 7.634831460674157e-05,
      "loss": 4.1901,
      "step": 442
    },
    {
      "epoch": 49.22222222222222,
      "grad_norm": 0.5513612627983093,
      "learning_rate": 7.629213483146069e-05,
      "loss": 4.1515,
      "step": 443
    },
    {
      "epoch": 49.333333333333336,
      "grad_norm": 0.5107521414756775,
      "learning_rate": 7.623595505617978e-05,
      "loss": 4.0528,
      "step": 444
    },
    {
      "epoch": 49.44444444444444,
      "grad_norm": 0.538292646408081,
      "learning_rate": 7.617977528089888e-05,
      "loss": 4.0683,
      "step": 445
    },
    {
      "epoch": 49.55555555555556,
      "grad_norm": 0.6197450160980225,
      "learning_rate": 7.612359550561798e-05,
      "loss": 4.1166,
      "step": 446
    },
    {
      "epoch": 49.666666666666664,
      "grad_norm": 5.7459940910339355,
      "learning_rate": 7.606741573033708e-05,
      "loss": 4.3128,
      "step": 447
    },
    {
      "epoch": 49.77777777777778,
      "grad_norm": 2.056081533432007,
      "learning_rate": 7.601123595505618e-05,
      "loss": 4.088,
      "step": 448
    },
    {
      "epoch": 49.888888888888886,
      "grad_norm": 0.9531688690185547,
      "learning_rate": 7.595505617977528e-05,
      "loss": 4.0853,
      "step": 449
    },
    {
      "epoch": 50.0,
      "grad_norm": 1.5410635471343994,
      "learning_rate": 7.589887640449438e-05,
      "loss": 2.1604,
      "step": 450
    },
    {
      "epoch": 50.111111111111114,
      "grad_norm": 2.164027690887451,
      "learning_rate": 7.584269662921349e-05,
      "loss": 4.2023,
      "step": 451
    },
    {
      "epoch": 50.22222222222222,
      "grad_norm": 2.580244541168213,
      "learning_rate": 7.578651685393259e-05,
      "loss": 4.397,
      "step": 452
    },
    {
      "epoch": 50.333333333333336,
      "grad_norm": 0.9119625091552734,
      "learning_rate": 7.573033707865169e-05,
      "loss": 4.2925,
      "step": 453
    },
    {
      "epoch": 50.44444444444444,
      "grad_norm": 0.6292696595191956,
      "learning_rate": 7.567415730337079e-05,
      "loss": 4.1651,
      "step": 454
    },
    {
      "epoch": 50.55555555555556,
      "grad_norm": 4.457874774932861,
      "learning_rate": 7.561797752808989e-05,
      "loss": 4.0759,
      "step": 455
    },
    {
      "epoch": 50.666666666666664,
      "grad_norm": 0.7854575514793396,
      "learning_rate": 7.556179775280899e-05,
      "loss": 4.1004,
      "step": 456
    },
    {
      "epoch": 50.77777777777778,
      "grad_norm": 0.754660964012146,
      "learning_rate": 7.55056179775281e-05,
      "loss": 4.0737,
      "step": 457
    },
    {
      "epoch": 50.888888888888886,
      "grad_norm": 1.9497785568237305,
      "learning_rate": 7.54494382022472e-05,
      "loss": 2.1964,
      "step": 458
    },
    {
      "epoch": 51.0,
      "grad_norm": 7.292708396911621,
      "learning_rate": 7.53932584269663e-05,
      "loss": 4.3246,
      "step": 459
    },
    {
      "epoch": 51.111111111111114,
      "grad_norm": 0.9039772152900696,
      "learning_rate": 7.53370786516854e-05,
      "loss": 4.1526,
      "step": 460
    },
    {
      "epoch": 51.22222222222222,
      "grad_norm": 6.996395587921143,
      "learning_rate": 7.52808988764045e-05,
      "loss": 4.2058,
      "step": 461
    },
    {
      "epoch": 51.333333333333336,
      "grad_norm": 0.744179368019104,
      "learning_rate": 7.52247191011236e-05,
      "loss": 4.1421,
      "step": 462
    },
    {
      "epoch": 51.44444444444444,
      "grad_norm": 4.377627372741699,
      "learning_rate": 7.51685393258427e-05,
      "loss": 4.517,
      "step": 463
    },
    {
      "epoch": 51.55555555555556,
      "grad_norm": 1.1855943202972412,
      "learning_rate": 7.51123595505618e-05,
      "loss": 4.0332,
      "step": 464
    },
    {
      "epoch": 51.666666666666664,
      "grad_norm": 1.4652262926101685,
      "learning_rate": 7.50561797752809e-05,
      "loss": 1.7906,
      "step": 465
    },
    {
      "epoch": 51.77777777777778,
      "grad_norm": 1.6508625745773315,
      "learning_rate": 7.500000000000001e-05,
      "loss": 4.0706,
      "step": 466
    },
    {
      "epoch": 51.888888888888886,
      "grad_norm": 0.7006078958511353,
      "learning_rate": 7.494382022471911e-05,
      "loss": 4.1188,
      "step": 467
    },
    {
      "epoch": 52.0,
      "grad_norm": 6.824800968170166,
      "learning_rate": 7.48876404494382e-05,
      "loss": 4.2792,
      "step": 468
    },
    {
      "epoch": 52.111111111111114,
      "grad_norm": 2.3188633918762207,
      "learning_rate": 7.483146067415731e-05,
      "loss": 4.1216,
      "step": 469
    },
    {
      "epoch": 52.22222222222222,
      "grad_norm": 0.5572208762168884,
      "learning_rate": 7.477528089887641e-05,
      "loss": 4.0113,
      "step": 470
    },
    {
      "epoch": 52.333333333333336,
      "grad_norm": 1.925917148590088,
      "learning_rate": 7.471910112359551e-05,
      "loss": 1.7934,
      "step": 471
    },
    {
      "epoch": 52.44444444444444,
      "grad_norm": 2.6559560298919678,
      "learning_rate": 7.46629213483146e-05,
      "loss": 4.1144,
      "step": 472
    },
    {
      "epoch": 52.55555555555556,
      "grad_norm": 0.5978614091873169,
      "learning_rate": 7.460674157303372e-05,
      "loss": 3.9298,
      "step": 473
    },
    {
      "epoch": 52.666666666666664,
      "grad_norm": 2.525822877883911,
      "learning_rate": 7.455056179775282e-05,
      "loss": 4.082,
      "step": 474
    },
    {
      "epoch": 52.77777777777778,
      "grad_norm": 0.6355510354042053,
      "learning_rate": 7.44943820224719e-05,
      "loss": 4.0254,
      "step": 475
    },
    {
      "epoch": 52.888888888888886,
      "grad_norm": 0.5290117859840393,
      "learning_rate": 7.443820224719101e-05,
      "loss": 3.9676,
      "step": 476
    },
    {
      "epoch": 53.0,
      "grad_norm": 10.648406982421875,
      "learning_rate": 7.438202247191012e-05,
      "loss": 4.2289,
      "step": 477
    },
    {
      "epoch": 53.111111111111114,
      "grad_norm": 1.5179731845855713,
      "learning_rate": 7.432584269662922e-05,
      "loss": 3.9907,
      "step": 478
    },
    {
      "epoch": 53.22222222222222,
      "grad_norm": 1.0483946800231934,
      "learning_rate": 7.426966292134831e-05,
      "loss": 3.9323,
      "step": 479
    },
    {
      "epoch": 53.333333333333336,
      "grad_norm": 0.5618447661399841,
      "learning_rate": 7.421348314606741e-05,
      "loss": 3.9261,
      "step": 480
    },
    {
      "epoch": 53.44444444444444,
      "grad_norm": 5.229090213775635,
      "learning_rate": 7.415730337078653e-05,
      "loss": 4.0519,
      "step": 481
    },
    {
      "epoch": 53.55555555555556,
      "grad_norm": 0.8974133133888245,
      "learning_rate": 7.410112359550562e-05,
      "loss": 4.056,
      "step": 482
    },
    {
      "epoch": 53.666666666666664,
      "grad_norm": 4.52929162979126,
      "learning_rate": 7.404494382022472e-05,
      "loss": 4.0326,
      "step": 483
    },
    {
      "epoch": 53.77777777777778,
      "grad_norm": 1.4107342958450317,
      "learning_rate": 7.398876404494382e-05,
      "loss": 1.8429,
      "step": 484
    },
    {
      "epoch": 53.888888888888886,
      "grad_norm": 0.816969096660614,
      "learning_rate": 7.393258426966293e-05,
      "loss": 4.0436,
      "step": 485
    },
    {
      "epoch": 54.0,
      "grad_norm": 0.5859398245811462,
      "learning_rate": 7.387640449438202e-05,
      "loss": 4.0133,
      "step": 486
    },
    {
      "epoch": 54.111111111111114,
      "grad_norm": 1.2314555644989014,
      "learning_rate": 7.382022471910112e-05,
      "loss": 4.0261,
      "step": 487
    },
    {
      "epoch": 54.22222222222222,
      "grad_norm": 0.6669204235076904,
      "learning_rate": 7.376404494382024e-05,
      "loss": 3.8724,
      "step": 488
    },
    {
      "epoch": 54.333333333333336,
      "grad_norm": 11.333233833312988,
      "learning_rate": 7.370786516853933e-05,
      "loss": 4.9286,
      "step": 489
    },
    {
      "epoch": 54.44444444444444,
      "grad_norm": 0.5956844687461853,
      "learning_rate": 7.365168539325843e-05,
      "loss": 3.9337,
      "step": 490
    },
    {
      "epoch": 54.55555555555556,
      "grad_norm": 2.611701250076294,
      "learning_rate": 7.359550561797753e-05,
      "loss": 4.1685,
      "step": 491
    },
    {
      "epoch": 54.666666666666664,
      "grad_norm": 1.5161453485488892,
      "learning_rate": 7.353932584269664e-05,
      "loss": 1.6909,
      "step": 492
    },
    {
      "epoch": 54.77777777777778,
      "grad_norm": 0.5010969042778015,
      "learning_rate": 7.348314606741573e-05,
      "loss": 3.8924,
      "step": 493
    },
    {
      "epoch": 54.888888888888886,
      "grad_norm": 1.7459797859191895,
      "learning_rate": 7.342696629213483e-05,
      "loss": 4.0129,
      "step": 494
    },
    {
      "epoch": 55.0,
      "grad_norm": 0.7455610036849976,
      "learning_rate": 7.337078651685393e-05,
      "loss": 3.9832,
      "step": 495
    },
    {
      "epoch": 55.111111111111114,
      "grad_norm": 7.185990333557129,
      "learning_rate": 7.331460674157304e-05,
      "loss": 4.5296,
      "step": 496
    },
    {
      "epoch": 55.22222222222222,
      "grad_norm": 0.6807833909988403,
      "learning_rate": 7.325842696629214e-05,
      "loss": 3.9153,
      "step": 497
    },
    {
      "epoch": 55.333333333333336,
      "grad_norm": 0.9110586047172546,
      "learning_rate": 7.320224719101124e-05,
      "loss": 3.9463,
      "step": 498
    },
    {
      "epoch": 55.44444444444444,
      "grad_norm": 0.7392917275428772,
      "learning_rate": 7.314606741573034e-05,
      "loss": 3.8822,
      "step": 499
    },
    {
      "epoch": 55.55555555555556,
      "grad_norm": 1.8554496765136719,
      "learning_rate": 7.308988764044944e-05,
      "loss": 2.1236,
      "step": 500
    },
    {
      "epoch": 55.666666666666664,
      "grad_norm": 7.359742641448975,
      "learning_rate": 7.303370786516854e-05,
      "loss": 4.7114,
      "step": 501
    },
    {
      "epoch": 55.77777777777778,
      "grad_norm": 1.7681245803833008,
      "learning_rate": 7.297752808988765e-05,
      "loss": 4.0061,
      "step": 502
    },
    {
      "epoch": 55.888888888888886,
      "grad_norm": 2.491569995880127,
      "learning_rate": 7.292134831460675e-05,
      "loss": 4.0581,
      "step": 503
    },
    {
      "epoch": 56.0,
      "grad_norm": 0.7232329845428467,
      "learning_rate": 7.286516853932585e-05,
      "loss": 3.9117,
      "step": 504
    },
    {
      "epoch": 56.111111111111114,
      "grad_norm": 0.5862314701080322,
      "learning_rate": 7.280898876404495e-05,
      "loss": 3.9027,
      "step": 505
    },
    {
      "epoch": 56.22222222222222,
      "grad_norm": 6.639822959899902,
      "learning_rate": 7.275280898876404e-05,
      "loss": 4.4118,
      "step": 506
    },
    {
      "epoch": 56.333333333333336,
      "grad_norm": 7.801667213439941,
      "learning_rate": 7.269662921348315e-05,
      "loss": 4.196,
      "step": 507
    },
    {
      "epoch": 56.44444444444444,
      "grad_norm": 0.7359230518341064,
      "learning_rate": 7.264044943820225e-05,
      "loss": 3.8659,
      "step": 508
    },
    {
      "epoch": 56.55555555555556,
      "grad_norm": 1.2361568212509155,
      "learning_rate": 7.258426966292136e-05,
      "loss": 3.8526,
      "step": 509
    },
    {
      "epoch": 56.666666666666664,
      "grad_norm": 4.018600940704346,
      "learning_rate": 7.252808988764044e-05,
      "loss": 4.0323,
      "step": 510
    },
    {
      "epoch": 56.77777777777778,
      "grad_norm": 0.610154926776886,
      "learning_rate": 7.247191011235956e-05,
      "loss": 3.8459,
      "step": 511
    },
    {
      "epoch": 56.888888888888886,
      "grad_norm": 0.9416747093200684,
      "learning_rate": 7.241573033707866e-05,
      "loss": 3.968,
      "step": 512
    },
    {
      "epoch": 57.0,
      "grad_norm": 1.3228883743286133,
      "learning_rate": 7.235955056179775e-05,
      "loss": 1.8481,
      "step": 513
    },
    {
      "epoch": 57.111111111111114,
      "grad_norm": 0.595529317855835,
      "learning_rate": 7.230337078651685e-05,
      "loss": 3.8656,
      "step": 514
    },
    {
      "epoch": 57.22222222222222,
      "grad_norm": 4.462839126586914,
      "learning_rate": 7.224719101123596e-05,
      "loss": 3.9582,
      "step": 515
    },
    {
      "epoch": 57.333333333333336,
      "grad_norm": 1.5988178253173828,
      "learning_rate": 7.219101123595507e-05,
      "loss": 2.0694,
      "step": 516
    },
    {
      "epoch": 57.44444444444444,
      "grad_norm": 0.6688471436500549,
      "learning_rate": 7.213483146067415e-05,
      "loss": 3.8365,
      "step": 517
    },
    {
      "epoch": 57.55555555555556,
      "grad_norm": 12.157812118530273,
      "learning_rate": 7.207865168539327e-05,
      "loss": 4.8133,
      "step": 518
    },
    {
      "epoch": 57.666666666666664,
      "grad_norm": 1.578938364982605,
      "learning_rate": 7.202247191011237e-05,
      "loss": 3.8797,
      "step": 519
    },
    {
      "epoch": 57.77777777777778,
      "grad_norm": 0.5177378058433533,
      "learning_rate": 7.196629213483146e-05,
      "loss": 3.832,
      "step": 520
    },
    {
      "epoch": 57.888888888888886,
      "grad_norm": 0.6936913728713989,
      "learning_rate": 7.191011235955056e-05,
      "loss": 3.9167,
      "step": 521
    },
    {
      "epoch": 58.0,
      "grad_norm": 0.9431524872779846,
      "learning_rate": 7.185393258426967e-05,
      "loss": 3.9787,
      "step": 522
    },
    {
      "epoch": 58.111111111111114,
      "grad_norm": 0.9268274307250977,
      "learning_rate": 7.179775280898878e-05,
      "loss": 3.815,
      "step": 523
    },
    {
      "epoch": 58.22222222222222,
      "grad_norm": 0.5516957640647888,
      "learning_rate": 7.174157303370786e-05,
      "loss": 3.7859,
      "step": 524
    },
    {
      "epoch": 58.333333333333336,
      "grad_norm": 0.4835714101791382,
      "learning_rate": 7.168539325842696e-05,
      "loss": 3.7778,
      "step": 525
    },
    {
      "epoch": 58.44444444444444,
      "grad_norm": 1.571570634841919,
      "learning_rate": 7.162921348314608e-05,
      "loss": 3.7814,
      "step": 526
    },
    {
      "epoch": 58.55555555555556,
      "grad_norm": 0.7054765224456787,
      "learning_rate": 7.157303370786517e-05,
      "loss": 3.7801,
      "step": 527
    },
    {
      "epoch": 58.666666666666664,
      "grad_norm": 0.6563925743103027,
      "learning_rate": 7.151685393258427e-05,
      "loss": 3.8305,
      "step": 528
    },
    {
      "epoch": 58.77777777777778,
      "grad_norm": 3.4096715450286865,
      "learning_rate": 7.146067415730337e-05,
      "loss": 3.9948,
      "step": 529
    },
    {
      "epoch": 58.888888888888886,
      "grad_norm": 1.4470548629760742,
      "learning_rate": 7.140449438202249e-05,
      "loss": 1.8688,
      "step": 530
    },
    {
      "epoch": 59.0,
      "grad_norm": 0.6575780510902405,
      "learning_rate": 7.134831460674157e-05,
      "loss": 3.9,
      "step": 531
    },
    {
      "epoch": 59.111111111111114,
      "grad_norm": 4.084133148193359,
      "learning_rate": 7.129213483146067e-05,
      "loss": 3.971,
      "step": 532
    },
    {
      "epoch": 59.22222222222222,
      "grad_norm": 6.052776336669922,
      "learning_rate": 7.123595505617978e-05,
      "loss": 3.8015,
      "step": 533
    },
    {
      "epoch": 59.333333333333336,
      "grad_norm": 10.781280517578125,
      "learning_rate": 7.117977528089888e-05,
      "loss": 4.0903,
      "step": 534
    },
    {
      "epoch": 59.44444444444444,
      "grad_norm": 1.3471729755401611,
      "learning_rate": 7.112359550561798e-05,
      "loss": 3.8683,
      "step": 535
    },
    {
      "epoch": 59.55555555555556,
      "grad_norm": 0.7732910513877869,
      "learning_rate": 7.106741573033708e-05,
      "loss": 3.8194,
      "step": 536
    },
    {
      "epoch": 59.666666666666664,
      "grad_norm": 5.423059463500977,
      "learning_rate": 7.101123595505618e-05,
      "loss": 4.6127,
      "step": 537
    },
    {
      "epoch": 59.77777777777778,
      "grad_norm": 0.5432251691818237,
      "learning_rate": 7.095505617977528e-05,
      "loss": 3.6999,
      "step": 538
    },
    {
      "epoch": 59.888888888888886,
      "grad_norm": 1.8805111646652222,
      "learning_rate": 7.089887640449438e-05,
      "loss": 1.9958,
      "step": 539
    },
    {
      "epoch": 60.0,
      "grad_norm": 1.688449740409851,
      "learning_rate": 7.084269662921349e-05,
      "loss": 3.8035,
      "step": 540
    },
    {
      "epoch": 60.111111111111114,
      "grad_norm": 0.7595365643501282,
      "learning_rate": 7.078651685393259e-05,
      "loss": 3.724,
      "step": 541
    },
    {
      "epoch": 60.22222222222222,
      "grad_norm": 0.5853782892227173,
      "learning_rate": 7.073033707865169e-05,
      "loss": 3.7247,
      "step": 542
    },
    {
      "epoch": 60.333333333333336,
      "grad_norm": 0.5172118544578552,
      "learning_rate": 7.067415730337079e-05,
      "loss": 3.7174,
      "step": 543
    },
    {
      "epoch": 60.44444444444444,
      "grad_norm": 0.7423836588859558,
      "learning_rate": 7.061797752808989e-05,
      "loss": 3.8157,
      "step": 544
    },
    {
      "epoch": 60.55555555555556,
      "grad_norm": 1.9676250219345093,
      "learning_rate": 7.056179775280899e-05,
      "loss": 3.7453,
      "step": 545
    },
    {
      "epoch": 60.666666666666664,
      "grad_norm": 1.4099363088607788,
      "learning_rate": 7.05056179775281e-05,
      "loss": 1.8702,
      "step": 546
    },
    {
      "epoch": 60.77777777777778,
      "grad_norm": 0.5634412169456482,
      "learning_rate": 7.04494382022472e-05,
      "loss": 3.8328,
      "step": 547
    },
    {
      "epoch": 60.888888888888886,
      "grad_norm": 3.0210506916046143,
      "learning_rate": 7.03932584269663e-05,
      "loss": 3.91,
      "step": 548
    },
    {
      "epoch": 61.0,
      "grad_norm": 0.49373698234558105,
      "learning_rate": 7.03370786516854e-05,
      "loss": 3.6645,
      "step": 549
    },
    {
      "epoch": 61.111111111111114,
      "grad_norm": 0.8026669025421143,
      "learning_rate": 7.02808988764045e-05,
      "loss": 3.7679,
      "step": 550
    },
    {
      "epoch": 61.22222222222222,
      "grad_norm": 7.151092052459717,
      "learning_rate": 7.02247191011236e-05,
      "loss": 3.8925,
      "step": 551
    },
    {
      "epoch": 61.333333333333336,
      "grad_norm": 0.4398574233055115,
      "learning_rate": 7.01685393258427e-05,
      "loss": 3.7033,
      "step": 552
    },
    {
      "epoch": 61.44444444444444,
      "grad_norm": 1.7154847383499146,
      "learning_rate": 7.01123595505618e-05,
      "loss": 1.7017,
      "step": 553
    },
    {
      "epoch": 61.55555555555556,
      "grad_norm": 11.954801559448242,
      "learning_rate": 7.00561797752809e-05,
      "loss": 4.0984,
      "step": 554
    },
    {
      "epoch": 61.666666666666664,
      "grad_norm": 4.997720241546631,
      "learning_rate": 7e-05,
      "loss": 3.9404,
      "step": 555
    },
    {
      "epoch": 61.77777777777778,
      "grad_norm": 0.8358919620513916,
      "learning_rate": 6.994382022471911e-05,
      "loss": 3.7898,
      "step": 556
    },
    {
      "epoch": 61.888888888888886,
      "grad_norm": 0.6308313608169556,
      "learning_rate": 6.988764044943821e-05,
      "loss": 3.7465,
      "step": 557
    },
    {
      "epoch": 62.0,
      "grad_norm": 10.2866792678833,
      "learning_rate": 6.983146067415731e-05,
      "loss": 3.9753,
      "step": 558
    },
    {
      "epoch": 62.111111111111114,
      "grad_norm": 0.8027350306510925,
      "learning_rate": 6.97752808988764e-05,
      "loss": 3.8301,
      "step": 559
    },
    {
      "epoch": 62.22222222222222,
      "grad_norm": 6.18491268157959,
      "learning_rate": 6.971910112359551e-05,
      "loss": 3.9298,
      "step": 560
    },
    {
      "epoch": 62.333333333333336,
      "grad_norm": 0.5755250453948975,
      "learning_rate": 6.966292134831462e-05,
      "loss": 3.8057,
      "step": 561
    },
    {
      "epoch": 62.44444444444444,
      "grad_norm": 0.5100914835929871,
      "learning_rate": 6.96067415730337e-05,
      "loss": 3.7444,
      "step": 562
    },
    {
      "epoch": 62.55555555555556,
      "grad_norm": 1.4683107137680054,
      "learning_rate": 6.95505617977528e-05,
      "loss": 1.6673,
      "step": 563
    },
    {
      "epoch": 62.666666666666664,
      "grad_norm": 0.6031835675239563,
      "learning_rate": 6.949438202247192e-05,
      "loss": 3.6756,
      "step": 564
    },
    {
      "epoch": 62.77777777777778,
      "grad_norm": 0.6012634038925171,
      "learning_rate": 6.943820224719102e-05,
      "loss": 3.6685,
      "step": 565
    },
    {
      "epoch": 62.888888888888886,
      "grad_norm": 2.750772476196289,
      "learning_rate": 6.938202247191011e-05,
      "loss": 3.6926,
      "step": 566
    },
    {
      "epoch": 63.0,
      "grad_norm": 7.8995585441589355,
      "learning_rate": 6.932584269662921e-05,
      "loss": 4.0231,
      "step": 567
    },
    {
      "epoch": 63.111111111111114,
      "grad_norm": 6.727544784545898,
      "learning_rate": 6.926966292134833e-05,
      "loss": 3.9647,
      "step": 568
    },
    {
      "epoch": 63.22222222222222,
      "grad_norm": 0.6842086315155029,
      "learning_rate": 6.921348314606741e-05,
      "loss": 3.7077,
      "step": 569
    },
    {
      "epoch": 63.333333333333336,
      "grad_norm": 0.641342043876648,
      "learning_rate": 6.915730337078652e-05,
      "loss": 3.7498,
      "step": 570
    },
    {
      "epoch": 63.44444444444444,
      "grad_norm": 1.7887885570526123,
      "learning_rate": 6.910112359550562e-05,
      "loss": 1.5175,
      "step": 571
    },
    {
      "epoch": 63.55555555555556,
      "grad_norm": 0.45468688011169434,
      "learning_rate": 6.904494382022473e-05,
      "loss": 3.6355,
      "step": 572
    },
    {
      "epoch": 63.666666666666664,
      "grad_norm": 3.6404032707214355,
      "learning_rate": 6.898876404494382e-05,
      "loss": 3.9626,
      "step": 573
    },
    {
      "epoch": 63.77777777777778,
      "grad_norm": 8.936707496643066,
      "learning_rate": 6.893258426966292e-05,
      "loss": 3.888,
      "step": 574
    },
    {
      "epoch": 63.888888888888886,
      "grad_norm": 0.6358141303062439,
      "learning_rate": 6.887640449438204e-05,
      "loss": 3.7152,
      "step": 575
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.9038630723953247,
      "learning_rate": 6.882022471910112e-05,
      "loss": 3.7449,
      "step": 576
    },
    {
      "epoch": 64.11111111111111,
      "grad_norm": 2.8414628505706787,
      "learning_rate": 6.876404494382023e-05,
      "loss": 3.8576,
      "step": 577
    },
    {
      "epoch": 64.22222222222223,
      "grad_norm": 0.5678796768188477,
      "learning_rate": 6.870786516853933e-05,
      "loss": 3.708,
      "step": 578
    },
    {
      "epoch": 64.33333333333333,
      "grad_norm": 0.6010257601737976,
      "learning_rate": 6.865168539325843e-05,
      "loss": 3.6593,
      "step": 579
    },
    {
      "epoch": 64.44444444444444,
      "grad_norm": 0.7188547849655151,
      "learning_rate": 6.859550561797753e-05,
      "loss": 3.6217,
      "step": 580
    },
    {
      "epoch": 64.55555555555556,
      "grad_norm": 0.6017166376113892,
      "learning_rate": 6.853932584269663e-05,
      "loss": 3.6454,
      "step": 581
    },
    {
      "epoch": 64.66666666666667,
      "grad_norm": 0.7024410963058472,
      "learning_rate": 6.848314606741573e-05,
      "loss": 3.7661,
      "step": 582
    },
    {
      "epoch": 64.77777777777777,
      "grad_norm": 3.9049148559570312,
      "learning_rate": 6.842696629213483e-05,
      "loss": 3.7755,
      "step": 583
    },
    {
      "epoch": 64.88888888888889,
      "grad_norm": 1.7846620082855225,
      "learning_rate": 6.837078651685394e-05,
      "loss": 1.7308,
      "step": 584
    },
    {
      "epoch": 65.0,
      "grad_norm": 1.0886354446411133,
      "learning_rate": 6.831460674157304e-05,
      "loss": 3.6688,
      "step": 585
    },
    {
      "epoch": 65.11111111111111,
      "grad_norm": 1.2208971977233887,
      "learning_rate": 6.825842696629214e-05,
      "loss": 3.7123,
      "step": 586
    },
    {
      "epoch": 65.22222222222223,
      "grad_norm": 0.5732718110084534,
      "learning_rate": 6.820224719101124e-05,
      "loss": 3.6246,
      "step": 587
    },
    {
      "epoch": 65.33333333333333,
      "grad_norm": 0.7343327403068542,
      "learning_rate": 6.814606741573034e-05,
      "loss": 3.6687,
      "step": 588
    },
    {
      "epoch": 65.44444444444444,
      "grad_norm": 0.6147677898406982,
      "learning_rate": 6.808988764044944e-05,
      "loss": 3.6319,
      "step": 589
    },
    {
      "epoch": 65.55555555555556,
      "grad_norm": 0.8524129390716553,
      "learning_rate": 6.803370786516854e-05,
      "loss": 3.8157,
      "step": 590
    },
    {
      "epoch": 65.66666666666667,
      "grad_norm": 13.199837684631348,
      "learning_rate": 6.797752808988765e-05,
      "loss": 1.4403,
      "step": 591
    },
    {
      "epoch": 65.77777777777777,
      "grad_norm": 3.7238080501556396,
      "learning_rate": 6.792134831460675e-05,
      "loss": 3.7265,
      "step": 592
    },
    {
      "epoch": 65.88888888888889,
      "grad_norm": 7.341128826141357,
      "learning_rate": 6.786516853932583e-05,
      "loss": 4.251,
      "step": 593
    },
    {
      "epoch": 66.0,
      "grad_norm": 3.4362118244171143,
      "learning_rate": 6.780898876404495e-05,
      "loss": 3.8249,
      "step": 594
    },
    {
      "epoch": 66.11111111111111,
      "grad_norm": 0.8856380581855774,
      "learning_rate": 6.775280898876405e-05,
      "loss": 3.6527,
      "step": 595
    },
    {
      "epoch": 66.22222222222223,
      "grad_norm": 5.307240009307861,
      "learning_rate": 6.769662921348315e-05,
      "loss": 4.1325,
      "step": 596
    },
    {
      "epoch": 66.33333333333333,
      "grad_norm": 9.237442970275879,
      "learning_rate": 6.764044943820224e-05,
      "loss": 4.0866,
      "step": 597
    },
    {
      "epoch": 66.44444444444444,
      "grad_norm": 5.229038715362549,
      "learning_rate": 6.758426966292136e-05,
      "loss": 3.7411,
      "step": 598
    },
    {
      "epoch": 66.55555555555556,
      "grad_norm": 1.5926141738891602,
      "learning_rate": 6.752808988764046e-05,
      "loss": 1.5689,
      "step": 599
    },
    {
      "epoch": 66.66666666666667,
      "grad_norm": 15.895215034484863,
      "learning_rate": 6.747191011235954e-05,
      "loss": 3.9413,
      "step": 600
    },
    {
      "epoch": 66.77777777777777,
      "grad_norm": 0.7101988792419434,
      "learning_rate": 6.741573033707866e-05,
      "loss": 3.6511,
      "step": 601
    },
    {
      "epoch": 66.88888888888889,
      "grad_norm": 6.176919937133789,
      "learning_rate": 6.735955056179776e-05,
      "loss": 4.1425,
      "step": 602
    },
    {
      "epoch": 67.0,
      "grad_norm": 0.7397217750549316,
      "learning_rate": 6.730337078651686e-05,
      "loss": 3.6523,
      "step": 603
    },
    {
      "epoch": 67.11111111111111,
      "grad_norm": 0.702079713344574,
      "learning_rate": 6.724719101123595e-05,
      "loss": 3.6994,
      "step": 604
    },
    {
      "epoch": 67.22222222222223,
      "grad_norm": 1.7410469055175781,
      "learning_rate": 6.719101123595507e-05,
      "loss": 1.5953,
      "step": 605
    },
    {
      "epoch": 67.33333333333333,
      "grad_norm": 0.5683297514915466,
      "learning_rate": 6.713483146067417e-05,
      "loss": 3.5933,
      "step": 606
    },
    {
      "epoch": 67.44444444444444,
      "grad_norm": 0.8281682729721069,
      "learning_rate": 6.707865168539325e-05,
      "loss": 3.5439,
      "step": 607
    },
    {
      "epoch": 67.55555555555556,
      "grad_norm": 0.7399045825004578,
      "learning_rate": 6.702247191011236e-05,
      "loss": 3.7036,
      "step": 608
    },
    {
      "epoch": 67.66666666666667,
      "grad_norm": 1.7158551216125488,
      "learning_rate": 6.696629213483147e-05,
      "loss": 3.5992,
      "step": 609
    },
    {
      "epoch": 67.77777777777777,
      "grad_norm": 0.5983404517173767,
      "learning_rate": 6.691011235955057e-05,
      "loss": 3.5991,
      "step": 610
    },
    {
      "epoch": 67.88888888888889,
      "grad_norm": 0.7013133764266968,
      "learning_rate": 6.685393258426966e-05,
      "loss": 3.6698,
      "step": 611
    },
    {
      "epoch": 68.0,
      "grad_norm": 5.4454169273376465,
      "learning_rate": 6.679775280898876e-05,
      "loss": 3.79,
      "step": 612
    },
    {
      "epoch": 68.11111111111111,
      "grad_norm": 0.7715682983398438,
      "learning_rate": 6.674157303370788e-05,
      "loss": 3.629,
      "step": 613
    },
    {
      "epoch": 68.22222222222223,
      "grad_norm": 0.8624258041381836,
      "learning_rate": 6.668539325842696e-05,
      "loss": 3.6617,
      "step": 614
    },
    {
      "epoch": 68.33333333333333,
      "grad_norm": 0.8935126662254333,
      "learning_rate": 6.662921348314607e-05,
      "loss": 3.6929,
      "step": 615
    },
    {
      "epoch": 68.44444444444444,
      "grad_norm": 2.9726593494415283,
      "learning_rate": 6.657303370786517e-05,
      "loss": 3.775,
      "step": 616
    },
    {
      "epoch": 68.55555555555556,
      "grad_norm": 0.944072961807251,
      "learning_rate": 6.651685393258428e-05,
      "loss": 3.5915,
      "step": 617
    },
    {
      "epoch": 68.66666666666667,
      "grad_norm": 3.3613364696502686,
      "learning_rate": 6.646067415730337e-05,
      "loss": 3.61,
      "step": 618
    },
    {
      "epoch": 68.77777777777777,
      "grad_norm": 0.4865947365760803,
      "learning_rate": 6.640449438202247e-05,
      "loss": 3.5476,
      "step": 619
    },
    {
      "epoch": 68.88888888888889,
      "grad_norm": 0.6777006387710571,
      "learning_rate": 6.634831460674157e-05,
      "loss": 3.6595,
      "step": 620
    },
    {
      "epoch": 69.0,
      "grad_norm": 1.5072658061981201,
      "learning_rate": 6.629213483146067e-05,
      "loss": 1.7346,
      "step": 621
    },
    {
      "epoch": 69.11111111111111,
      "grad_norm": 0.7365115880966187,
      "learning_rate": 6.623595505617978e-05,
      "loss": 3.5952,
      "step": 622
    },
    {
      "epoch": 69.22222222222223,
      "grad_norm": 0.690249502658844,
      "learning_rate": 6.617977528089888e-05,
      "loss": 3.686,
      "step": 623
    },
    {
      "epoch": 69.33333333333333,
      "grad_norm": 0.5518581867218018,
      "learning_rate": 6.612359550561798e-05,
      "loss": 3.5456,
      "step": 624
    },
    {
      "epoch": 69.44444444444444,
      "grad_norm": 0.599065899848938,
      "learning_rate": 6.606741573033708e-05,
      "loss": 3.5716,
      "step": 625
    },
    {
      "epoch": 69.55555555555556,
      "grad_norm": 0.7095762491226196,
      "learning_rate": 6.601123595505618e-05,
      "loss": 3.5725,
      "step": 626
    },
    {
      "epoch": 69.66666666666667,
      "grad_norm": 1.5074032545089722,
      "learning_rate": 6.595505617977528e-05,
      "loss": 1.6903,
      "step": 627
    },
    {
      "epoch": 69.77777777777777,
      "grad_norm": 0.4747276306152344,
      "learning_rate": 6.589887640449438e-05,
      "loss": 3.5259,
      "step": 628
    },
    {
      "epoch": 69.88888888888889,
      "grad_norm": 0.8112412095069885,
      "learning_rate": 6.584269662921349e-05,
      "loss": 3.5169,
      "step": 629
    },
    {
      "epoch": 70.0,
      "grad_norm": 0.8054441809654236,
      "learning_rate": 6.578651685393259e-05,
      "loss": 3.5006,
      "step": 630
    },
    {
      "epoch": 70.11111111111111,
      "grad_norm": 2.810295343399048,
      "learning_rate": 6.573033707865169e-05,
      "loss": 3.6679,
      "step": 631
    },
    {
      "epoch": 70.22222222222223,
      "grad_norm": 17.67693328857422,
      "learning_rate": 6.567415730337079e-05,
      "loss": 3.6458,
      "step": 632
    },
    {
      "epoch": 70.33333333333333,
      "grad_norm": 0.5850254893302917,
      "learning_rate": 6.561797752808989e-05,
      "loss": 3.5884,
      "step": 633
    },
    {
      "epoch": 70.44444444444444,
      "grad_norm": 0.48980557918548584,
      "learning_rate": 6.5561797752809e-05,
      "loss": 3.5355,
      "step": 634
    },
    {
      "epoch": 70.55555555555556,
      "grad_norm": 2.663515329360962,
      "learning_rate": 6.55056179775281e-05,
      "loss": 3.6322,
      "step": 635
    },
    {
      "epoch": 70.66666666666667,
      "grad_norm": 1.5573256015777588,
      "learning_rate": 6.54494382022472e-05,
      "loss": 1.5775,
      "step": 636
    },
    {
      "epoch": 70.77777777777777,
      "grad_norm": 3.819608449935913,
      "learning_rate": 6.53932584269663e-05,
      "loss": 3.7452,
      "step": 637
    },
    {
      "epoch": 70.88888888888889,
      "grad_norm": 0.5365877151489258,
      "learning_rate": 6.53370786516854e-05,
      "loss": 3.4746,
      "step": 638
    },
    {
      "epoch": 71.0,
      "grad_norm": 0.8374391794204712,
      "learning_rate": 6.52808988764045e-05,
      "loss": 3.5906,
      "step": 639
    },
    {
      "epoch": 71.11111111111111,
      "grad_norm": 0.7026082873344421,
      "learning_rate": 6.52247191011236e-05,
      "loss": 3.6098,
      "step": 640
    },
    {
      "epoch": 71.22222222222223,
      "grad_norm": 0.6699534058570862,
      "learning_rate": 6.51685393258427e-05,
      "loss": 3.5615,
      "step": 641
    },
    {
      "epoch": 71.33333333333333,
      "grad_norm": 2.2699131965637207,
      "learning_rate": 6.511235955056179e-05,
      "loss": 1.8015,
      "step": 642
    },
    {
      "epoch": 71.44444444444444,
      "grad_norm": 3.3523643016815186,
      "learning_rate": 6.50561797752809e-05,
      "loss": 3.6029,
      "step": 643
    },
    {
      "epoch": 71.55555555555556,
      "grad_norm": 4.584655284881592,
      "learning_rate": 6.500000000000001e-05,
      "loss": 3.6631,
      "step": 644
    },
    {
      "epoch": 71.66666666666667,
      "grad_norm": 1.2265704870224,
      "learning_rate": 6.494382022471911e-05,
      "loss": 3.5629,
      "step": 645
    },
    {
      "epoch": 71.77777777777777,
      "grad_norm": 1.019614338874817,
      "learning_rate": 6.48876404494382e-05,
      "loss": 3.6807,
      "step": 646
    },
    {
      "epoch": 71.88888888888889,
      "grad_norm": 2.133920431137085,
      "learning_rate": 6.483146067415731e-05,
      "loss": 3.5583,
      "step": 647
    },
    {
      "epoch": 72.0,
      "grad_norm": 0.8138487339019775,
      "learning_rate": 6.477528089887641e-05,
      "loss": 3.5729,
      "step": 648
    },
    {
      "epoch": 72.11111111111111,
      "grad_norm": 1.4724873304367065,
      "learning_rate": 6.47191011235955e-05,
      "loss": 1.4663,
      "step": 649
    },
    {
      "epoch": 72.22222222222223,
      "grad_norm": 9.288995742797852,
      "learning_rate": 6.46629213483146e-05,
      "loss": 3.6934,
      "step": 650
    },
    {
      "epoch": 72.33333333333333,
      "grad_norm": 2.9942119121551514,
      "learning_rate": 6.460674157303372e-05,
      "loss": 3.6481,
      "step": 651
    },
    {
      "epoch": 72.44444444444444,
      "grad_norm": 1.244551420211792,
      "learning_rate": 6.45505617977528e-05,
      "loss": 3.5856,
      "step": 652
    },
    {
      "epoch": 72.55555555555556,
      "grad_norm": 1.8542042970657349,
      "learning_rate": 6.449438202247191e-05,
      "loss": 3.5365,
      "step": 653
    },
    {
      "epoch": 72.66666666666667,
      "grad_norm": 0.4864794611930847,
      "learning_rate": 6.443820224719101e-05,
      "loss": 3.5268,
      "step": 654
    },
    {
      "epoch": 72.77777777777777,
      "grad_norm": 1.1604670286178589,
      "learning_rate": 6.438202247191012e-05,
      "loss": 3.4951,
      "step": 655
    },
    {
      "epoch": 72.88888888888889,
      "grad_norm": 0.5981882214546204,
      "learning_rate": 6.432584269662921e-05,
      "loss": 3.4131,
      "step": 656
    },
    {
      "epoch": 73.0,
      "grad_norm": 1.4772831201553345,
      "learning_rate": 6.426966292134831e-05,
      "loss": 3.4513,
      "step": 657
    },
    {
      "epoch": 73.11111111111111,
      "grad_norm": 0.5820744633674622,
      "learning_rate": 6.421348314606743e-05,
      "loss": 3.4498,
      "step": 658
    },
    {
      "epoch": 73.22222222222223,
      "grad_norm": 0.5415077209472656,
      "learning_rate": 6.415730337078652e-05,
      "loss": 3.4618,
      "step": 659
    },
    {
      "epoch": 73.33333333333333,
      "grad_norm": 0.6909427046775818,
      "learning_rate": 6.410112359550562e-05,
      "loss": 3.513,
      "step": 660
    },
    {
      "epoch": 73.44444444444444,
      "grad_norm": 0.752420961856842,
      "learning_rate": 6.404494382022472e-05,
      "loss": 3.5624,
      "step": 661
    },
    {
      "epoch": 73.55555555555556,
      "grad_norm": 0.7103124856948853,
      "learning_rate": 6.398876404494383e-05,
      "loss": 3.4584,
      "step": 662
    },
    {
      "epoch": 73.66666666666667,
      "grad_norm": 0.7331988215446472,
      "learning_rate": 6.393258426966292e-05,
      "loss": 3.4771,
      "step": 663
    },
    {
      "epoch": 73.77777777777777,
      "grad_norm": 1.5506620407104492,
      "learning_rate": 6.387640449438202e-05,
      "loss": 1.5385,
      "step": 664
    },
    {
      "epoch": 73.88888888888889,
      "grad_norm": 1.141848087310791,
      "learning_rate": 6.382022471910112e-05,
      "loss": 3.518,
      "step": 665
    },
    {
      "epoch": 74.0,
      "grad_norm": 9.2229642868042,
      "learning_rate": 6.376404494382023e-05,
      "loss": 3.6518,
      "step": 666
    },
    {
      "epoch": 74.11111111111111,
      "grad_norm": 6.613229274749756,
      "learning_rate": 6.370786516853933e-05,
      "loss": 3.5681,
      "step": 667
    },
    {
      "epoch": 74.22222222222223,
      "grad_norm": 1.4489048719406128,
      "learning_rate": 6.365168539325843e-05,
      "loss": 3.5044,
      "step": 668
    },
    {
      "epoch": 74.33333333333333,
      "grad_norm": 2.465759038925171,
      "learning_rate": 6.359550561797753e-05,
      "loss": 3.603,
      "step": 669
    },
    {
      "epoch": 74.44444444444444,
      "grad_norm": 1.8331613540649414,
      "learning_rate": 6.353932584269663e-05,
      "loss": 3.5232,
      "step": 670
    },
    {
      "epoch": 74.55555555555556,
      "grad_norm": 8.447427749633789,
      "learning_rate": 6.348314606741573e-05,
      "loss": 3.7174,
      "step": 671
    },
    {
      "epoch": 74.66666666666667,
      "grad_norm": 0.5938025116920471,
      "learning_rate": 6.342696629213483e-05,
      "loss": 3.4383,
      "step": 672
    },
    {
      "epoch": 74.77777777777777,
      "grad_norm": 1.3983449935913086,
      "learning_rate": 6.337078651685394e-05,
      "loss": 1.4715,
      "step": 673
    },
    {
      "epoch": 74.88888888888889,
      "grad_norm": 0.523871898651123,
      "learning_rate": 6.331460674157304e-05,
      "loss": 3.4259,
      "step": 674
    },
    {
      "epoch": 75.0,
      "grad_norm": 0.6093354225158691,
      "learning_rate": 6.325842696629214e-05,
      "loss": 3.421,
      "step": 675
    },
    {
      "epoch": 75.11111111111111,
      "grad_norm": 4.353230953216553,
      "learning_rate": 6.320224719101124e-05,
      "loss": 3.5292,
      "step": 676
    },
    {
      "epoch": 75.22222222222223,
      "grad_norm": 2.6081466674804688,
      "learning_rate": 6.314606741573034e-05,
      "loss": 3.4773,
      "step": 677
    },
    {
      "epoch": 75.33333333333333,
      "grad_norm": 0.7306949496269226,
      "learning_rate": 6.308988764044944e-05,
      "loss": 3.3877,
      "step": 678
    },
    {
      "epoch": 75.44444444444444,
      "grad_norm": 8.142241477966309,
      "learning_rate": 6.303370786516854e-05,
      "loss": 3.7671,
      "step": 679
    },
    {
      "epoch": 75.55555555555556,
      "grad_norm": 5.026628017425537,
      "learning_rate": 6.297752808988763e-05,
      "loss": 3.5998,
      "step": 680
    },
    {
      "epoch": 75.66666666666667,
      "grad_norm": 0.6135873198509216,
      "learning_rate": 6.292134831460675e-05,
      "loss": 3.456,
      "step": 681
    },
    {
      "epoch": 75.77777777777777,
      "grad_norm": 1.7708876132965088,
      "learning_rate": 6.286516853932585e-05,
      "loss": 1.5364,
      "step": 682
    },
    {
      "epoch": 75.88888888888889,
      "grad_norm": 0.7200062870979309,
      "learning_rate": 6.280898876404495e-05,
      "loss": 3.4644,
      "step": 683
    },
    {
      "epoch": 76.0,
      "grad_norm": 7.388645648956299,
      "learning_rate": 6.275280898876404e-05,
      "loss": 3.637,
      "step": 684
    },
    {
      "epoch": 76.11111111111111,
      "grad_norm": 2.513378620147705,
      "learning_rate": 6.269662921348315e-05,
      "loss": 3.5027,
      "step": 685
    },
    {
      "epoch": 76.22222222222223,
      "grad_norm": 0.5268446207046509,
      "learning_rate": 6.264044943820225e-05,
      "loss": 3.4586,
      "step": 686
    },
    {
      "epoch": 76.33333333333333,
      "grad_norm": 0.7098352313041687,
      "learning_rate": 6.258426966292134e-05,
      "loss": 3.4277,
      "step": 687
    },
    {
      "epoch": 76.44444444444444,
      "grad_norm": 4.527770042419434,
      "learning_rate": 6.252808988764046e-05,
      "loss": 3.471,
      "step": 688
    },
    {
      "epoch": 76.55555555555556,
      "grad_norm": 0.5466585755348206,
      "learning_rate": 6.247191011235956e-05,
      "loss": 3.3461,
      "step": 689
    },
    {
      "epoch": 76.66666666666667,
      "grad_norm": 1.0155494213104248,
      "learning_rate": 6.241573033707866e-05,
      "loss": 3.4224,
      "step": 690
    },
    {
      "epoch": 76.77777777777777,
      "grad_norm": 1.3093924522399902,
      "learning_rate": 6.235955056179775e-05,
      "loss": 3.5129,
      "step": 691
    },
    {
      "epoch": 76.88888888888889,
      "grad_norm": 0.6758217811584473,
      "learning_rate": 6.230337078651686e-05,
      "loss": 3.3874,
      "step": 692
    },
    {
      "epoch": 77.0,
      "grad_norm": 1.6876070499420166,
      "learning_rate": 6.224719101123596e-05,
      "loss": 1.5084,
      "step": 693
    },
    {
      "epoch": 77.11111111111111,
      "grad_norm": 1.0919013023376465,
      "learning_rate": 6.219101123595505e-05,
      "loss": 3.4625,
      "step": 694
    },
    {
      "epoch": 77.22222222222223,
      "grad_norm": 0.5495125651359558,
      "learning_rate": 6.213483146067415e-05,
      "loss": 3.4336,
      "step": 695
    },
    {
      "epoch": 77.33333333333333,
      "grad_norm": 3.1993138790130615,
      "learning_rate": 6.207865168539327e-05,
      "loss": 3.5187,
      "step": 696
    },
    {
      "epoch": 77.44444444444444,
      "grad_norm": 3.6063742637634277,
      "learning_rate": 6.202247191011237e-05,
      "loss": 3.4095,
      "step": 697
    },
    {
      "epoch": 77.55555555555556,
      "grad_norm": 0.5361795425415039,
      "learning_rate": 6.196629213483146e-05,
      "loss": 3.4339,
      "step": 698
    },
    {
      "epoch": 77.66666666666667,
      "grad_norm": 4.0774827003479,
      "learning_rate": 6.191011235955056e-05,
      "loss": 3.6963,
      "step": 699
    },
    {
      "epoch": 77.77777777777777,
      "grad_norm": 2.354823112487793,
      "learning_rate": 6.185393258426967e-05,
      "loss": 1.724,
      "step": 700
    },
    {
      "epoch": 77.88888888888889,
      "grad_norm": 0.9098010063171387,
      "learning_rate": 6.179775280898876e-05,
      "loss": 3.4924,
      "step": 701
    },
    {
      "epoch": 78.0,
      "grad_norm": 5.865869045257568,
      "learning_rate": 6.174157303370786e-05,
      "loss": 3.4825,
      "step": 702
    },
    {
      "epoch": 78.11111111111111,
      "grad_norm": 1.11476731300354,
      "learning_rate": 6.168539325842697e-05,
      "loss": 3.3801,
      "step": 703
    },
    {
      "epoch": 78.22222222222223,
      "grad_norm": 5.910319805145264,
      "learning_rate": 6.162921348314608e-05,
      "loss": 3.7205,
      "step": 704
    },
    {
      "epoch": 78.33333333333333,
      "grad_norm": 0.7017174363136292,
      "learning_rate": 6.157303370786517e-05,
      "loss": 3.3752,
      "step": 705
    },
    {
      "epoch": 78.44444444444444,
      "grad_norm": 0.5773692727088928,
      "learning_rate": 6.151685393258427e-05,
      "loss": 3.3793,
      "step": 706
    },
    {
      "epoch": 78.55555555555556,
      "grad_norm": 0.6776307225227356,
      "learning_rate": 6.146067415730337e-05,
      "loss": 3.4175,
      "step": 707
    },
    {
      "epoch": 78.66666666666667,
      "grad_norm": 0.5462271571159363,
      "learning_rate": 6.140449438202247e-05,
      "loss": 3.3766,
      "step": 708
    },
    {
      "epoch": 78.77777777777777,
      "grad_norm": 0.6128978729248047,
      "learning_rate": 6.134831460674157e-05,
      "loss": 3.385,
      "step": 709
    },
    {
      "epoch": 78.88888888888889,
      "grad_norm": 2.4887866973876953,
      "learning_rate": 6.129213483146068e-05,
      "loss": 1.5109,
      "step": 710
    },
    {
      "epoch": 79.0,
      "grad_norm": 0.5848537683486938,
      "learning_rate": 6.123595505617978e-05,
      "loss": 3.3545,
      "step": 711
    },
    {
      "epoch": 79.11111111111111,
      "grad_norm": 1.864337682723999,
      "learning_rate": 6.117977528089888e-05,
      "loss": 1.4526,
      "step": 712
    },
    {
      "epoch": 79.22222222222223,
      "grad_norm": 0.9534807801246643,
      "learning_rate": 6.112359550561798e-05,
      "loss": 3.4065,
      "step": 713
    },
    {
      "epoch": 79.33333333333333,
      "grad_norm": 0.9587534070014954,
      "learning_rate": 6.106741573033708e-05,
      "loss": 3.353,
      "step": 714
    },
    {
      "epoch": 79.44444444444444,
      "grad_norm": 0.7420768737792969,
      "learning_rate": 6.1011235955056176e-05,
      "loss": 3.3418,
      "step": 715
    },
    {
      "epoch": 79.55555555555556,
      "grad_norm": 5.4154229164123535,
      "learning_rate": 6.0955056179775284e-05,
      "loss": 3.5097,
      "step": 716
    },
    {
      "epoch": 79.66666666666667,
      "grad_norm": 1.0789430141448975,
      "learning_rate": 6.0898876404494385e-05,
      "loss": 3.3058,
      "step": 717
    },
    {
      "epoch": 79.77777777777777,
      "grad_norm": 0.6305181384086609,
      "learning_rate": 6.0842696629213494e-05,
      "loss": 3.3344,
      "step": 718
    },
    {
      "epoch": 79.88888888888889,
      "grad_norm": 7.204626560211182,
      "learning_rate": 6.078651685393258e-05,
      "loss": 4.3841,
      "step": 719
    },
    {
      "epoch": 80.0,
      "grad_norm": 1.1663323640823364,
      "learning_rate": 6.073033707865169e-05,
      "loss": 3.4433,
      "step": 720
    },
    {
      "epoch": 80.11111111111111,
      "grad_norm": 0.981940746307373,
      "learning_rate": 6.067415730337079e-05,
      "loss": 3.2948,
      "step": 721
    },
    {
      "epoch": 80.22222222222223,
      "grad_norm": 3.332432985305786,
      "learning_rate": 6.0617977528089886e-05,
      "loss": 3.4384,
      "step": 722
    },
    {
      "epoch": 80.33333333333333,
      "grad_norm": 4.145686626434326,
      "learning_rate": 6.0561797752808994e-05,
      "loss": 4.2143,
      "step": 723
    },
    {
      "epoch": 80.44444444444444,
      "grad_norm": 0.9442002773284912,
      "learning_rate": 6.0505617977528095e-05,
      "loss": 3.3544,
      "step": 724
    },
    {
      "epoch": 80.55555555555556,
      "grad_norm": 2.3321168422698975,
      "learning_rate": 6.04494382022472e-05,
      "loss": 1.4041,
      "step": 725
    },
    {
      "epoch": 80.66666666666667,
      "grad_norm": 1.0637693405151367,
      "learning_rate": 6.039325842696629e-05,
      "loss": 3.3706,
      "step": 726
    },
    {
      "epoch": 80.77777777777777,
      "grad_norm": 1.7242530584335327,
      "learning_rate": 6.03370786516854e-05,
      "loss": 3.4172,
      "step": 727
    },
    {
      "epoch": 80.88888888888889,
      "grad_norm": 0.6890923380851746,
      "learning_rate": 6.02808988764045e-05,
      "loss": 3.3307,
      "step": 728
    },
    {
      "epoch": 81.0,
      "grad_norm": 0.6918596625328064,
      "learning_rate": 6.0224719101123596e-05,
      "loss": 3.2725,
      "step": 729
    },
    {
      "epoch": 81.11111111111111,
      "grad_norm": 10.28349781036377,
      "learning_rate": 6.01685393258427e-05,
      "loss": 3.5567,
      "step": 730
    },
    {
      "epoch": 81.22222222222223,
      "grad_norm": 1.6489083766937256,
      "learning_rate": 6.0112359550561805e-05,
      "loss": 3.3653,
      "step": 731
    },
    {
      "epoch": 81.33333333333333,
      "grad_norm": 16.018741607666016,
      "learning_rate": 6.005617977528089e-05,
      "loss": 3.377,
      "step": 732
    },
    {
      "epoch": 81.44444444444444,
      "grad_norm": 5.107356071472168,
      "learning_rate": 6e-05,
      "loss": 3.6219,
      "step": 733
    },
    {
      "epoch": 81.55555555555556,
      "grad_norm": 1.7371338605880737,
      "learning_rate": 5.99438202247191e-05,
      "loss": 1.3247,
      "step": 734
    },
    {
      "epoch": 81.66666666666667,
      "grad_norm": 0.7073178887367249,
      "learning_rate": 5.988764044943821e-05,
      "loss": 3.2459,
      "step": 735
    },
    {
      "epoch": 81.77777777777777,
      "grad_norm": 1.0643430948257446,
      "learning_rate": 5.9831460674157306e-05,
      "loss": 3.2948,
      "step": 736
    },
    {
      "epoch": 81.88888888888889,
      "grad_norm": 0.8473978638648987,
      "learning_rate": 5.977528089887641e-05,
      "loss": 3.3333,
      "step": 737
    },
    {
      "epoch": 82.0,
      "grad_norm": 1.3523194789886475,
      "learning_rate": 5.971910112359551e-05,
      "loss": 3.4566,
      "step": 738
    },
    {
      "epoch": 82.11111111111111,
      "grad_norm": 0.7883074283599854,
      "learning_rate": 5.96629213483146e-05,
      "loss": 3.3076,
      "step": 739
    },
    {
      "epoch": 82.22222222222223,
      "grad_norm": 0.6684277057647705,
      "learning_rate": 5.960674157303371e-05,
      "loss": 3.248,
      "step": 740
    },
    {
      "epoch": 82.33333333333333,
      "grad_norm": 2.012615919113159,
      "learning_rate": 5.955056179775281e-05,
      "loss": 3.3394,
      "step": 741
    },
    {
      "epoch": 82.44444444444444,
      "grad_norm": 0.7906848192214966,
      "learning_rate": 5.9494382022471914e-05,
      "loss": 3.3652,
      "step": 742
    },
    {
      "epoch": 82.55555555555556,
      "grad_norm": 0.5362800359725952,
      "learning_rate": 5.943820224719101e-05,
      "loss": 3.2899,
      "step": 743
    },
    {
      "epoch": 82.66666666666667,
      "grad_norm": 0.6794449090957642,
      "learning_rate": 5.938202247191012e-05,
      "loss": 3.2793,
      "step": 744
    },
    {
      "epoch": 82.77777777777777,
      "grad_norm": 7.896911144256592,
      "learning_rate": 5.932584269662922e-05,
      "loss": 3.5242,
      "step": 745
    },
    {
      "epoch": 82.88888888888889,
      "grad_norm": 1.5941429138183594,
      "learning_rate": 5.926966292134831e-05,
      "loss": 1.3785,
      "step": 746
    },
    {
      "epoch": 83.0,
      "grad_norm": 0.46225711703300476,
      "learning_rate": 5.9213483146067415e-05,
      "loss": 3.2531,
      "step": 747
    },
    {
      "epoch": 83.11111111111111,
      "grad_norm": 2.0711543560028076,
      "learning_rate": 5.915730337078652e-05,
      "loss": 3.3102,
      "step": 748
    },
    {
      "epoch": 83.22222222222223,
      "grad_norm": 1.8088276386260986,
      "learning_rate": 5.9101123595505624e-05,
      "loss": 1.3901,
      "step": 749
    },
    {
      "epoch": 83.33333333333333,
      "grad_norm": 1.0819692611694336,
      "learning_rate": 5.904494382022472e-05,
      "loss": 3.3249,
      "step": 750
    },
    {
      "epoch": 83.44444444444444,
      "grad_norm": 3.1373233795166016,
      "learning_rate": 5.898876404494382e-05,
      "loss": 3.4496,
      "step": 751
    },
    {
      "epoch": 83.55555555555556,
      "grad_norm": 6.650091171264648,
      "learning_rate": 5.893258426966293e-05,
      "loss": 3.4441,
      "step": 752
    },
    {
      "epoch": 83.66666666666667,
      "grad_norm": 0.9043533802032471,
      "learning_rate": 5.8876404494382023e-05,
      "loss": 3.3315,
      "step": 753
    },
    {
      "epoch": 83.77777777777777,
      "grad_norm": 2.53495717048645,
      "learning_rate": 5.8820224719101125e-05,
      "loss": 3.2739,
      "step": 754
    },
    {
      "epoch": 83.88888888888889,
      "grad_norm": 0.48274585604667664,
      "learning_rate": 5.8764044943820226e-05,
      "loss": 3.2361,
      "step": 755
    },
    {
      "epoch": 84.0,
      "grad_norm": 0.9202749133110046,
      "learning_rate": 5.8707865168539334e-05,
      "loss": 3.2661,
      "step": 756
    },
    {
      "epoch": 84.11111111111111,
      "grad_norm": 0.6308573484420776,
      "learning_rate": 5.865168539325843e-05,
      "loss": 3.2665,
      "step": 757
    },
    {
      "epoch": 84.22222222222223,
      "grad_norm": 2.4727609157562256,
      "learning_rate": 5.859550561797753e-05,
      "loss": 3.2882,
      "step": 758
    },
    {
      "epoch": 84.33333333333333,
      "grad_norm": 1.5698332786560059,
      "learning_rate": 5.853932584269663e-05,
      "loss": 1.5242,
      "step": 759
    },
    {
      "epoch": 84.44444444444444,
      "grad_norm": 1.202921986579895,
      "learning_rate": 5.848314606741573e-05,
      "loss": 3.2785,
      "step": 760
    },
    {
      "epoch": 84.55555555555556,
      "grad_norm": 1.2687318325042725,
      "learning_rate": 5.8426966292134835e-05,
      "loss": 3.2097,
      "step": 761
    },
    {
      "epoch": 84.66666666666667,
      "grad_norm": 0.7731109857559204,
      "learning_rate": 5.8370786516853936e-05,
      "loss": 3.254,
      "step": 762
    },
    {
      "epoch": 84.77777777777777,
      "grad_norm": 2.0148229598999023,
      "learning_rate": 5.8314606741573045e-05,
      "loss": 3.3348,
      "step": 763
    },
    {
      "epoch": 84.88888888888889,
      "grad_norm": 0.8052095174789429,
      "learning_rate": 5.825842696629213e-05,
      "loss": 3.2568,
      "step": 764
    },
    {
      "epoch": 85.0,
      "grad_norm": 0.5842289328575134,
      "learning_rate": 5.820224719101124e-05,
      "loss": 3.1872,
      "step": 765
    },
    {
      "epoch": 85.11111111111111,
      "grad_norm": 1.010829210281372,
      "learning_rate": 5.814606741573034e-05,
      "loss": 3.2336,
      "step": 766
    },
    {
      "epoch": 85.22222222222223,
      "grad_norm": 0.7701702117919922,
      "learning_rate": 5.808988764044944e-05,
      "loss": 3.3031,
      "step": 767
    },
    {
      "epoch": 85.33333333333333,
      "grad_norm": 2.193514823913574,
      "learning_rate": 5.803370786516854e-05,
      "loss": 3.2178,
      "step": 768
    },
    {
      "epoch": 85.44444444444444,
      "grad_norm": 1.7129546403884888,
      "learning_rate": 5.7977528089887646e-05,
      "loss": 1.3122,
      "step": 769
    },
    {
      "epoch": 85.55555555555556,
      "grad_norm": 0.5392634272575378,
      "learning_rate": 5.792134831460675e-05,
      "loss": 3.2195,
      "step": 770
    },
    {
      "epoch": 85.66666666666667,
      "grad_norm": 3.0236077308654785,
      "learning_rate": 5.786516853932584e-05,
      "loss": 3.2991,
      "step": 771
    },
    {
      "epoch": 85.77777777777777,
      "grad_norm": 0.8781850337982178,
      "learning_rate": 5.7808988764044944e-05,
      "loss": 3.2459,
      "step": 772
    },
    {
      "epoch": 85.88888888888889,
      "grad_norm": 4.182947158813477,
      "learning_rate": 5.775280898876405e-05,
      "loss": 3.2901,
      "step": 773
    },
    {
      "epoch": 86.0,
      "grad_norm": 3.8950304985046387,
      "learning_rate": 5.769662921348315e-05,
      "loss": 3.2545,
      "step": 774
    },
    {
      "epoch": 86.11111111111111,
      "grad_norm": 0.5871810913085938,
      "learning_rate": 5.764044943820225e-05,
      "loss": 3.2476,
      "step": 775
    },
    {
      "epoch": 86.22222222222223,
      "grad_norm": 0.63016277551651,
      "learning_rate": 5.758426966292135e-05,
      "loss": 3.2482,
      "step": 776
    },
    {
      "epoch": 86.33333333333333,
      "grad_norm": 0.8434233665466309,
      "learning_rate": 5.752808988764046e-05,
      "loss": 3.301,
      "step": 777
    },
    {
      "epoch": 86.44444444444444,
      "grad_norm": 0.6047738790512085,
      "learning_rate": 5.747191011235955e-05,
      "loss": 3.2391,
      "step": 778
    },
    {
      "epoch": 86.55555555555556,
      "grad_norm": 0.9994331002235413,
      "learning_rate": 5.7415730337078654e-05,
      "loss": 3.2479,
      "step": 779
    },
    {
      "epoch": 86.66666666666667,
      "grad_norm": 5.151962757110596,
      "learning_rate": 5.735955056179776e-05,
      "loss": 3.3088,
      "step": 780
    },
    {
      "epoch": 86.77777777777777,
      "grad_norm": 1.0689071416854858,
      "learning_rate": 5.730337078651685e-05,
      "loss": 3.2454,
      "step": 781
    },
    {
      "epoch": 86.88888888888889,
      "grad_norm": 3.571685314178467,
      "learning_rate": 5.724719101123596e-05,
      "loss": 3.3715,
      "step": 782
    },
    {
      "epoch": 87.0,
      "grad_norm": 2.0072171688079834,
      "learning_rate": 5.719101123595506e-05,
      "loss": 1.2715,
      "step": 783
    },
    {
      "epoch": 87.11111111111111,
      "grad_norm": 0.8580328226089478,
      "learning_rate": 5.713483146067417e-05,
      "loss": 3.2925,
      "step": 784
    },
    {
      "epoch": 87.22222222222223,
      "grad_norm": 4.8416547775268555,
      "learning_rate": 5.7078651685393256e-05,
      "loss": 3.3686,
      "step": 785
    },
    {
      "epoch": 87.33333333333333,
      "grad_norm": 0.9052289724349976,
      "learning_rate": 5.7022471910112364e-05,
      "loss": 3.2058,
      "step": 786
    },
    {
      "epoch": 87.44444444444444,
      "grad_norm": 7.468883514404297,
      "learning_rate": 5.6966292134831465e-05,
      "loss": 3.5019,
      "step": 787
    },
    {
      "epoch": 87.55555555555556,
      "grad_norm": 1.5773663520812988,
      "learning_rate": 5.691011235955056e-05,
      "loss": 1.4145,
      "step": 788
    },
    {
      "epoch": 87.66666666666667,
      "grad_norm": 2.5559210777282715,
      "learning_rate": 5.685393258426966e-05,
      "loss": 3.1828,
      "step": 789
    },
    {
      "epoch": 87.77777777777777,
      "grad_norm": 7.0608811378479,
      "learning_rate": 5.679775280898877e-05,
      "loss": 3.6392,
      "step": 790
    },
    {
      "epoch": 87.88888888888889,
      "grad_norm": 4.013827800750732,
      "learning_rate": 5.674157303370787e-05,
      "loss": 3.2664,
      "step": 791
    },
    {
      "epoch": 88.0,
      "grad_norm": 0.469096839427948,
      "learning_rate": 5.6685393258426966e-05,
      "loss": 3.1364,
      "step": 792
    },
    {
      "epoch": 88.11111111111111,
      "grad_norm": 0.7204036116600037,
      "learning_rate": 5.6629213483146074e-05,
      "loss": 3.2274,
      "step": 793
    },
    {
      "epoch": 88.22222222222223,
      "grad_norm": 0.8622624278068542,
      "learning_rate": 5.6573033707865175e-05,
      "loss": 3.2081,
      "step": 794
    },
    {
      "epoch": 88.33333333333333,
      "grad_norm": 0.5772311687469482,
      "learning_rate": 5.651685393258427e-05,
      "loss": 3.1819,
      "step": 795
    },
    {
      "epoch": 88.44444444444444,
      "grad_norm": 0.8083279728889465,
      "learning_rate": 5.646067415730337e-05,
      "loss": 3.1335,
      "step": 796
    },
    {
      "epoch": 88.55555555555556,
      "grad_norm": 0.699359118938446,
      "learning_rate": 5.640449438202248e-05,
      "loss": 3.1749,
      "step": 797
    },
    {
      "epoch": 88.66666666666667,
      "grad_norm": 0.8798241019248962,
      "learning_rate": 5.634831460674158e-05,
      "loss": 3.2496,
      "step": 798
    },
    {
      "epoch": 88.77777777777777,
      "grad_norm": 2.264472484588623,
      "learning_rate": 5.6292134831460676e-05,
      "loss": 1.6421,
      "step": 799
    },
    {
      "epoch": 88.88888888888889,
      "grad_norm": 3.2171030044555664,
      "learning_rate": 5.623595505617978e-05,
      "loss": 3.2575,
      "step": 800
    },
    {
      "epoch": 89.0,
      "grad_norm": 1.5213068723678589,
      "learning_rate": 5.6179775280898885e-05,
      "loss": 3.1079,
      "step": 801
    },
    {
      "epoch": 89.11111111111111,
      "grad_norm": 19.706314086914062,
      "learning_rate": 5.612359550561797e-05,
      "loss": 3.3102,
      "step": 802
    },
    {
      "epoch": 89.22222222222223,
      "grad_norm": 0.7797144055366516,
      "learning_rate": 5.606741573033708e-05,
      "loss": 3.1808,
      "step": 803
    },
    {
      "epoch": 89.33333333333333,
      "grad_norm": 3.3919591903686523,
      "learning_rate": 5.601123595505618e-05,
      "loss": 3.1992,
      "step": 804
    },
    {
      "epoch": 89.44444444444444,
      "grad_norm": 0.5853109359741211,
      "learning_rate": 5.595505617977528e-05,
      "loss": 3.1961,
      "step": 805
    },
    {
      "epoch": 89.55555555555556,
      "grad_norm": 1.5861550569534302,
      "learning_rate": 5.5898876404494386e-05,
      "loss": 1.247,
      "step": 806
    },
    {
      "epoch": 89.66666666666667,
      "grad_norm": 1.0391550064086914,
      "learning_rate": 5.584269662921349e-05,
      "loss": 3.2101,
      "step": 807
    },
    {
      "epoch": 89.77777777777777,
      "grad_norm": 0.6530684232711792,
      "learning_rate": 5.578651685393259e-05,
      "loss": 3.2336,
      "step": 808
    },
    {
      "epoch": 89.88888888888889,
      "grad_norm": 1.0509536266326904,
      "learning_rate": 5.573033707865168e-05,
      "loss": 3.1571,
      "step": 809
    },
    {
      "epoch": 90.0,
      "grad_norm": 0.6676315665245056,
      "learning_rate": 5.567415730337079e-05,
      "loss": 3.1365,
      "step": 810
    },
    {
      "epoch": 90.11111111111111,
      "grad_norm": 1.037808895111084,
      "learning_rate": 5.561797752808989e-05,
      "loss": 3.1602,
      "step": 811
    },
    {
      "epoch": 90.22222222222223,
      "grad_norm": 0.5334606766700745,
      "learning_rate": 5.556179775280899e-05,
      "loss": 3.0804,
      "step": 812
    },
    {
      "epoch": 90.33333333333333,
      "grad_norm": 0.5362948775291443,
      "learning_rate": 5.550561797752809e-05,
      "loss": 3.1539,
      "step": 813
    },
    {
      "epoch": 90.44444444444444,
      "grad_norm": 2.8026304244995117,
      "learning_rate": 5.54494382022472e-05,
      "loss": 3.3088,
      "step": 814
    },
    {
      "epoch": 90.55555555555556,
      "grad_norm": 1.0910253524780273,
      "learning_rate": 5.53932584269663e-05,
      "loss": 3.1336,
      "step": 815
    },
    {
      "epoch": 90.66666666666667,
      "grad_norm": 0.7959798574447632,
      "learning_rate": 5.533707865168539e-05,
      "loss": 3.1383,
      "step": 816
    },
    {
      "epoch": 90.77777777777777,
      "grad_norm": 0.5651786923408508,
      "learning_rate": 5.5280898876404495e-05,
      "loss": 3.0943,
      "step": 817
    },
    {
      "epoch": 90.88888888888889,
      "grad_norm": 0.6211080551147461,
      "learning_rate": 5.52247191011236e-05,
      "loss": 3.1498,
      "step": 818
    },
    {
      "epoch": 91.0,
      "grad_norm": 1.4246407747268677,
      "learning_rate": 5.516853932584269e-05,
      "loss": 1.3892,
      "step": 819
    },
    {
      "epoch": 91.11111111111111,
      "grad_norm": 2.3386027812957764,
      "learning_rate": 5.51123595505618e-05,
      "loss": 3.2202,
      "step": 820
    },
    {
      "epoch": 91.22222222222223,
      "grad_norm": 1.5293747186660767,
      "learning_rate": 5.50561797752809e-05,
      "loss": 3.1752,
      "step": 821
    },
    {
      "epoch": 91.33333333333333,
      "grad_norm": 1.052068829536438,
      "learning_rate": 5.500000000000001e-05,
      "loss": 3.1784,
      "step": 822
    },
    {
      "epoch": 91.44444444444444,
      "grad_norm": 0.5393769145011902,
      "learning_rate": 5.4943820224719103e-05,
      "loss": 3.074,
      "step": 823
    },
    {
      "epoch": 91.55555555555556,
      "grad_norm": 0.6505395174026489,
      "learning_rate": 5.4887640449438205e-05,
      "loss": 3.0782,
      "step": 824
    },
    {
      "epoch": 91.66666666666667,
      "grad_norm": 1.5511947870254517,
      "learning_rate": 5.4831460674157306e-05,
      "loss": 1.4238,
      "step": 825
    },
    {
      "epoch": 91.77777777777777,
      "grad_norm": 3.472566604614258,
      "learning_rate": 5.47752808988764e-05,
      "loss": 3.1727,
      "step": 826
    },
    {
      "epoch": 91.88888888888889,
      "grad_norm": 0.5363377928733826,
      "learning_rate": 5.471910112359551e-05,
      "loss": 3.1338,
      "step": 827
    },
    {
      "epoch": 92.0,
      "grad_norm": 0.6624653339385986,
      "learning_rate": 5.466292134831461e-05,
      "loss": 3.0514,
      "step": 828
    },
    {
      "epoch": 92.11111111111111,
      "grad_norm": 1.32846200466156,
      "learning_rate": 5.460674157303371e-05,
      "loss": 3.1748,
      "step": 829
    },
    {
      "epoch": 92.22222222222223,
      "grad_norm": 1.828857183456421,
      "learning_rate": 5.455056179775281e-05,
      "loss": 1.263,
      "step": 830
    },
    {
      "epoch": 92.33333333333333,
      "grad_norm": 1.4245145320892334,
      "learning_rate": 5.4494382022471915e-05,
      "loss": 3.0837,
      "step": 831
    },
    {
      "epoch": 92.44444444444444,
      "grad_norm": 4.986386299133301,
      "learning_rate": 5.4438202247191016e-05,
      "loss": 3.2451,
      "step": 832
    },
    {
      "epoch": 92.55555555555556,
      "grad_norm": 6.858752727508545,
      "learning_rate": 5.438202247191011e-05,
      "loss": 3.1495,
      "step": 833
    },
    {
      "epoch": 92.66666666666667,
      "grad_norm": 0.7944014668464661,
      "learning_rate": 5.432584269662921e-05,
      "loss": 3.0434,
      "step": 834
    },
    {
      "epoch": 92.77777777777777,
      "grad_norm": 5.43259334564209,
      "learning_rate": 5.426966292134832e-05,
      "loss": 3.5538,
      "step": 835
    },
    {
      "epoch": 92.88888888888889,
      "grad_norm": 3.2248952388763428,
      "learning_rate": 5.421348314606742e-05,
      "loss": 3.1688,
      "step": 836
    },
    {
      "epoch": 93.0,
      "grad_norm": 2.973865270614624,
      "learning_rate": 5.415730337078652e-05,
      "loss": 3.2237,
      "step": 837
    },
    {
      "epoch": 93.11111111111111,
      "grad_norm": 1.2746787071228027,
      "learning_rate": 5.410112359550562e-05,
      "loss": 3.1337,
      "step": 838
    },
    {
      "epoch": 93.22222222222223,
      "grad_norm": 2.2065303325653076,
      "learning_rate": 5.4044943820224726e-05,
      "loss": 1.4115,
      "step": 839
    },
    {
      "epoch": 93.33333333333333,
      "grad_norm": 5.2848801612854,
      "learning_rate": 5.398876404494382e-05,
      "loss": 3.5248,
      "step": 840
    },
    {
      "epoch": 93.44444444444444,
      "grad_norm": 0.5237935185432434,
      "learning_rate": 5.393258426966292e-05,
      "loss": 3.0979,
      "step": 841
    },
    {
      "epoch": 93.55555555555556,
      "grad_norm": 0.6591679453849792,
      "learning_rate": 5.3876404494382024e-05,
      "loss": 3.1091,
      "step": 842
    },
    {
      "epoch": 93.66666666666667,
      "grad_norm": 6.000957012176514,
      "learning_rate": 5.382022471910113e-05,
      "loss": 3.203,
      "step": 843
    },
    {
      "epoch": 93.77777777777777,
      "grad_norm": 0.9985452890396118,
      "learning_rate": 5.376404494382023e-05,
      "loss": 3.0967,
      "step": 844
    },
    {
      "epoch": 93.88888888888889,
      "grad_norm": 2.905622720718384,
      "learning_rate": 5.370786516853933e-05,
      "loss": 3.0911,
      "step": 845
    },
    {
      "epoch": 94.0,
      "grad_norm": 0.8389124870300293,
      "learning_rate": 5.365168539325843e-05,
      "loss": 3.098,
      "step": 846
    },
    {
      "epoch": 94.11111111111111,
      "grad_norm": 0.9493615627288818,
      "learning_rate": 5.3595505617977524e-05,
      "loss": 3.1037,
      "step": 847
    },
    {
      "epoch": 94.22222222222223,
      "grad_norm": 2.1405367851257324,
      "learning_rate": 5.353932584269663e-05,
      "loss": 1.5021,
      "step": 848
    },
    {
      "epoch": 94.33333333333333,
      "grad_norm": 0.6198232173919678,
      "learning_rate": 5.3483146067415734e-05,
      "loss": 3.0109,
      "step": 849
    },
    {
      "epoch": 94.44444444444444,
      "grad_norm": 2.9673783779144287,
      "learning_rate": 5.342696629213484e-05,
      "loss": 3.1319,
      "step": 850
    },
    {
      "epoch": 94.55555555555556,
      "grad_norm": 0.6907109022140503,
      "learning_rate": 5.337078651685393e-05,
      "loss": 3.0722,
      "step": 851
    },
    {
      "epoch": 94.66666666666667,
      "grad_norm": 0.9370039105415344,
      "learning_rate": 5.331460674157304e-05,
      "loss": 3.0997,
      "step": 852
    },
    {
      "epoch": 94.77777777777777,
      "grad_norm": 0.8763550519943237,
      "learning_rate": 5.325842696629214e-05,
      "loss": 3.0639,
      "step": 853
    },
    {
      "epoch": 94.88888888888889,
      "grad_norm": 0.506118655204773,
      "learning_rate": 5.3202247191011234e-05,
      "loss": 3.0462,
      "step": 854
    },
    {
      "epoch": 95.0,
      "grad_norm": 2.490419626235962,
      "learning_rate": 5.3146067415730336e-05,
      "loss": 3.1759,
      "step": 855
    },
    {
      "epoch": 95.11111111111111,
      "grad_norm": 0.6696360111236572,
      "learning_rate": 5.3089887640449444e-05,
      "loss": 3.0419,
      "step": 856
    },
    {
      "epoch": 95.22222222222223,
      "grad_norm": 3.4130430221557617,
      "learning_rate": 5.3033707865168545e-05,
      "loss": 3.0694,
      "step": 857
    },
    {
      "epoch": 95.33333333333333,
      "grad_norm": 0.5214078426361084,
      "learning_rate": 5.297752808988764e-05,
      "loss": 3.0743,
      "step": 858
    },
    {
      "epoch": 95.44444444444444,
      "grad_norm": 0.6618697643280029,
      "learning_rate": 5.292134831460674e-05,
      "loss": 3.0165,
      "step": 859
    },
    {
      "epoch": 95.55555555555556,
      "grad_norm": 1.0085862874984741,
      "learning_rate": 5.286516853932585e-05,
      "loss": 3.0862,
      "step": 860
    },
    {
      "epoch": 95.66666666666667,
      "grad_norm": 3.161620855331421,
      "learning_rate": 5.2808988764044944e-05,
      "loss": 3.0703,
      "step": 861
    },
    {
      "epoch": 95.77777777777777,
      "grad_norm": 2.4036848545074463,
      "learning_rate": 5.2752808988764046e-05,
      "loss": 1.3404,
      "step": 862
    },
    {
      "epoch": 95.88888888888889,
      "grad_norm": 0.6083126068115234,
      "learning_rate": 5.2696629213483154e-05,
      "loss": 3.1206,
      "step": 863
    },
    {
      "epoch": 96.0,
      "grad_norm": 0.7515280246734619,
      "learning_rate": 5.2640449438202255e-05,
      "loss": 3.038,
      "step": 864
    },
    {
      "epoch": 96.11111111111111,
      "grad_norm": 0.731225848197937,
      "learning_rate": 5.258426966292135e-05,
      "loss": 3.0144,
      "step": 865
    },
    {
      "epoch": 96.22222222222223,
      "grad_norm": 1.4911322593688965,
      "learning_rate": 5.252808988764045e-05,
      "loss": 1.2528,
      "step": 866
    },
    {
      "epoch": 96.33333333333333,
      "grad_norm": 1.7321372032165527,
      "learning_rate": 5.247191011235956e-05,
      "loss": 3.0483,
      "step": 867
    },
    {
      "epoch": 96.44444444444444,
      "grad_norm": 7.452632427215576,
      "learning_rate": 5.241573033707865e-05,
      "loss": 3.1995,
      "step": 868
    },
    {
      "epoch": 96.55555555555556,
      "grad_norm": 0.8279498219490051,
      "learning_rate": 5.2359550561797756e-05,
      "loss": 3.0015,
      "step": 869
    },
    {
      "epoch": 96.66666666666667,
      "grad_norm": 0.6805033087730408,
      "learning_rate": 5.230337078651686e-05,
      "loss": 2.9594,
      "step": 870
    },
    {
      "epoch": 96.77777777777777,
      "grad_norm": 1.266931414604187,
      "learning_rate": 5.2247191011235965e-05,
      "loss": 3.0504,
      "step": 871
    },
    {
      "epoch": 96.88888888888889,
      "grad_norm": 0.5332642793655396,
      "learning_rate": 5.219101123595505e-05,
      "loss": 3.0083,
      "step": 872
    },
    {
      "epoch": 97.0,
      "grad_norm": 5.319929599761963,
      "learning_rate": 5.213483146067416e-05,
      "loss": 3.1983,
      "step": 873
    },
    {
      "epoch": 97.11111111111111,
      "grad_norm": 3.487298011779785,
      "learning_rate": 5.207865168539326e-05,
      "loss": 3.201,
      "step": 874
    },
    {
      "epoch": 97.22222222222223,
      "grad_norm": 1.0632685422897339,
      "learning_rate": 5.202247191011236e-05,
      "loss": 2.9625,
      "step": 875
    },
    {
      "epoch": 97.33333333333333,
      "grad_norm": 3.8044161796569824,
      "learning_rate": 5.1966292134831466e-05,
      "loss": 3.0716,
      "step": 876
    },
    {
      "epoch": 97.44444444444444,
      "grad_norm": 0.6933154463768005,
      "learning_rate": 5.191011235955057e-05,
      "loss": 2.9483,
      "step": 877
    },
    {
      "epoch": 97.55555555555556,
      "grad_norm": 0.6457312703132629,
      "learning_rate": 5.185393258426966e-05,
      "loss": 2.9328,
      "step": 878
    },
    {
      "epoch": 97.66666666666667,
      "grad_norm": 0.7549877166748047,
      "learning_rate": 5.179775280898876e-05,
      "loss": 3.0063,
      "step": 879
    },
    {
      "epoch": 97.77777777777777,
      "grad_norm": 4.371006965637207,
      "learning_rate": 5.174157303370787e-05,
      "loss": 3.3895,
      "step": 880
    },
    {
      "epoch": 97.88888888888889,
      "grad_norm": 1.815604329109192,
      "learning_rate": 5.168539325842697e-05,
      "loss": 1.4093,
      "step": 881
    },
    {
      "epoch": 98.0,
      "grad_norm": 0.673871636390686,
      "learning_rate": 5.162921348314607e-05,
      "loss": 3.0441,
      "step": 882
    },
    {
      "epoch": 98.11111111111111,
      "grad_norm": 0.6024383902549744,
      "learning_rate": 5.157303370786517e-05,
      "loss": 2.9807,
      "step": 883
    },
    {
      "epoch": 98.22222222222223,
      "grad_norm": 1.5966615676879883,
      "learning_rate": 5.151685393258428e-05,
      "loss": 2.9739,
      "step": 884
    },
    {
      "epoch": 98.33333333333333,
      "grad_norm": 0.7319871783256531,
      "learning_rate": 5.1460674157303365e-05,
      "loss": 3.0338,
      "step": 885
    },
    {
      "epoch": 98.44444444444444,
      "grad_norm": 0.6924373507499695,
      "learning_rate": 5.140449438202247e-05,
      "loss": 2.9507,
      "step": 886
    },
    {
      "epoch": 98.55555555555556,
      "grad_norm": 1.507617473602295,
      "learning_rate": 5.1348314606741575e-05,
      "loss": 3.004,
      "step": 887
    },
    {
      "epoch": 98.66666666666667,
      "grad_norm": 1.201844573020935,
      "learning_rate": 5.129213483146068e-05,
      "loss": 3.0231,
      "step": 888
    },
    {
      "epoch": 98.77777777777777,
      "grad_norm": 1.6462903022766113,
      "learning_rate": 5.123595505617977e-05,
      "loss": 1.3259,
      "step": 889
    },
    {
      "epoch": 98.88888888888889,
      "grad_norm": 1.3651015758514404,
      "learning_rate": 5.117977528089888e-05,
      "loss": 3.001,
      "step": 890
    },
    {
      "epoch": 99.0,
      "grad_norm": 2.578427791595459,
      "learning_rate": 5.112359550561798e-05,
      "loss": 3.0409,
      "step": 891
    },
    {
      "epoch": 99.11111111111111,
      "grad_norm": 1.2971224784851074,
      "learning_rate": 5.1067415730337075e-05,
      "loss": 2.9928,
      "step": 892
    },
    {
      "epoch": 99.22222222222223,
      "grad_norm": 0.5691296458244324,
      "learning_rate": 5.101123595505618e-05,
      "loss": 2.9838,
      "step": 893
    },
    {
      "epoch": 99.33333333333333,
      "grad_norm": 0.6388169527053833,
      "learning_rate": 5.0955056179775285e-05,
      "loss": 2.9636,
      "step": 894
    },
    {
      "epoch": 99.44444444444444,
      "grad_norm": 1.5925389528274536,
      "learning_rate": 5.0898876404494386e-05,
      "loss": 1.2569,
      "step": 895
    },
    {
      "epoch": 99.55555555555556,
      "grad_norm": 0.6385071277618408,
      "learning_rate": 5.084269662921348e-05,
      "loss": 2.9944,
      "step": 896
    },
    {
      "epoch": 99.66666666666667,
      "grad_norm": 7.374309062957764,
      "learning_rate": 5.078651685393259e-05,
      "loss": 3.2104,
      "step": 897
    },
    {
      "epoch": 99.77777777777777,
      "grad_norm": 3.091789722442627,
      "learning_rate": 5.073033707865169e-05,
      "loss": 3.0256,
      "step": 898
    },
    {
      "epoch": 99.88888888888889,
      "grad_norm": 6.984325408935547,
      "learning_rate": 5.0674157303370785e-05,
      "loss": 3.0088,
      "step": 899
    },
    {
      "epoch": 100.0,
      "grad_norm": 0.5777992606163025,
      "learning_rate": 5.061797752808989e-05,
      "loss": 2.9496,
      "step": 900
    },
    {
      "epoch": 100.11111111111111,
      "grad_norm": 0.6015335917472839,
      "learning_rate": 5.0561797752808995e-05,
      "loss": 2.9545,
      "step": 901
    },
    {
      "epoch": 100.22222222222223,
      "grad_norm": 0.5916070938110352,
      "learning_rate": 5.0505617977528096e-05,
      "loss": 2.9523,
      "step": 902
    },
    {
      "epoch": 100.33333333333333,
      "grad_norm": 3.9837026596069336,
      "learning_rate": 5.044943820224719e-05,
      "loss": 2.972,
      "step": 903
    },
    {
      "epoch": 100.44444444444444,
      "grad_norm": 6.524044036865234,
      "learning_rate": 5.039325842696629e-05,
      "loss": 3.1916,
      "step": 904
    },
    {
      "epoch": 100.55555555555556,
      "grad_norm": 0.7494065761566162,
      "learning_rate": 5.03370786516854e-05,
      "loss": 2.9584,
      "step": 905
    },
    {
      "epoch": 100.66666666666667,
      "grad_norm": 1.1515482664108276,
      "learning_rate": 5.0280898876404495e-05,
      "loss": 2.9928,
      "step": 906
    },
    {
      "epoch": 100.77777777777777,
      "grad_norm": 0.5858829021453857,
      "learning_rate": 5.02247191011236e-05,
      "loss": 2.9378,
      "step": 907
    },
    {
      "epoch": 100.88888888888889,
      "grad_norm": 1.5024771690368652,
      "learning_rate": 5.01685393258427e-05,
      "loss": 1.3962,
      "step": 908
    },
    {
      "epoch": 101.0,
      "grad_norm": 0.6740236282348633,
      "learning_rate": 5.0112359550561806e-05,
      "loss": 2.9604,
      "step": 909
    },
    {
      "epoch": 101.11111111111111,
      "grad_norm": 0.617791473865509,
      "learning_rate": 5.00561797752809e-05,
      "loss": 2.946,
      "step": 910
    },
    {
      "epoch": 101.22222222222223,
      "grad_norm": 1.3068022727966309,
      "learning_rate": 5e-05,
      "loss": 0.9526,
      "step": 911
    },
    {
      "epoch": 101.33333333333333,
      "grad_norm": 0.49011296033859253,
      "learning_rate": 4.9943820224719104e-05,
      "loss": 2.9275,
      "step": 912
    },
    {
      "epoch": 101.44444444444444,
      "grad_norm": 0.764618992805481,
      "learning_rate": 4.9887640449438205e-05,
      "loss": 2.947,
      "step": 913
    },
    {
      "epoch": 101.55555555555556,
      "grad_norm": 0.6094720959663391,
      "learning_rate": 4.983146067415731e-05,
      "loss": 2.9813,
      "step": 914
    },
    {
      "epoch": 101.66666666666667,
      "grad_norm": 6.544856548309326,
      "learning_rate": 4.977528089887641e-05,
      "loss": 3.0444,
      "step": 915
    },
    {
      "epoch": 101.77777777777777,
      "grad_norm": 4.362738609313965,
      "learning_rate": 4.971910112359551e-05,
      "loss": 3.1013,
      "step": 916
    },
    {
      "epoch": 101.88888888888889,
      "grad_norm": 0.664115846157074,
      "learning_rate": 4.966292134831461e-05,
      "loss": 2.9075,
      "step": 917
    },
    {
      "epoch": 102.0,
      "grad_norm": 0.8350979089736938,
      "learning_rate": 4.960674157303371e-05,
      "loss": 2.9557,
      "step": 918
    },
    {
      "epoch": 102.11111111111111,
      "grad_norm": 2.9368836879730225,
      "learning_rate": 4.955056179775281e-05,
      "loss": 1.1232,
      "step": 919
    },
    {
      "epoch": 102.22222222222223,
      "grad_norm": 0.8038212656974792,
      "learning_rate": 4.9494382022471915e-05,
      "loss": 2.9395,
      "step": 920
    },
    {
      "epoch": 102.33333333333333,
      "grad_norm": 1.8705309629440308,
      "learning_rate": 4.943820224719101e-05,
      "loss": 2.915,
      "step": 921
    },
    {
      "epoch": 102.44444444444444,
      "grad_norm": 0.7681350708007812,
      "learning_rate": 4.938202247191012e-05,
      "loss": 2.9172,
      "step": 922
    },
    {
      "epoch": 102.55555555555556,
      "grad_norm": 5.300412654876709,
      "learning_rate": 4.932584269662921e-05,
      "loss": 3.1727,
      "step": 923
    },
    {
      "epoch": 102.66666666666667,
      "grad_norm": 0.8001471757888794,
      "learning_rate": 4.926966292134832e-05,
      "loss": 2.9053,
      "step": 924
    },
    {
      "epoch": 102.77777777777777,
      "grad_norm": 0.5182119011878967,
      "learning_rate": 4.9213483146067416e-05,
      "loss": 2.8576,
      "step": 925
    },
    {
      "epoch": 102.88888888888889,
      "grad_norm": 0.8368144035339355,
      "learning_rate": 4.915730337078652e-05,
      "loss": 2.8639,
      "step": 926
    },
    {
      "epoch": 103.0,
      "grad_norm": 0.5378433465957642,
      "learning_rate": 4.910112359550562e-05,
      "loss": 2.9145,
      "step": 927
    },
    {
      "epoch": 103.11111111111111,
      "grad_norm": 0.983766496181488,
      "learning_rate": 4.904494382022472e-05,
      "loss": 2.9115,
      "step": 928
    },
    {
      "epoch": 103.22222222222223,
      "grad_norm": 1.3570315837860107,
      "learning_rate": 4.898876404494382e-05,
      "loss": 2.9024,
      "step": 929
    },
    {
      "epoch": 103.33333333333333,
      "grad_norm": 0.7864910364151001,
      "learning_rate": 4.893258426966292e-05,
      "loss": 2.9037,
      "step": 930
    },
    {
      "epoch": 103.44444444444444,
      "grad_norm": 0.5228701829910278,
      "learning_rate": 4.8876404494382024e-05,
      "loss": 2.8851,
      "step": 931
    },
    {
      "epoch": 103.55555555555556,
      "grad_norm": 2.3260910511016846,
      "learning_rate": 4.8820224719101126e-05,
      "loss": 1.19,
      "step": 932
    },
    {
      "epoch": 103.66666666666667,
      "grad_norm": 0.5652424693107605,
      "learning_rate": 4.876404494382023e-05,
      "loss": 2.899,
      "step": 933
    },
    {
      "epoch": 103.77777777777777,
      "grad_norm": 0.6658638119697571,
      "learning_rate": 4.870786516853933e-05,
      "loss": 2.9157,
      "step": 934
    },
    {
      "epoch": 103.88888888888889,
      "grad_norm": 2.984459638595581,
      "learning_rate": 4.865168539325843e-05,
      "loss": 3.0697,
      "step": 935
    },
    {
      "epoch": 104.0,
      "grad_norm": 0.6283673048019409,
      "learning_rate": 4.859550561797753e-05,
      "loss": 2.8442,
      "step": 936
    },
    {
      "epoch": 104.11111111111111,
      "grad_norm": 1.036523699760437,
      "learning_rate": 4.853932584269663e-05,
      "loss": 2.8815,
      "step": 937
    },
    {
      "epoch": 104.22222222222223,
      "grad_norm": 0.8095662593841553,
      "learning_rate": 4.848314606741573e-05,
      "loss": 2.9125,
      "step": 938
    },
    {
      "epoch": 104.33333333333333,
      "grad_norm": 2.7579965591430664,
      "learning_rate": 4.8426966292134836e-05,
      "loss": 2.8808,
      "step": 939
    },
    {
      "epoch": 104.44444444444444,
      "grad_norm": 3.9493799209594727,
      "learning_rate": 4.837078651685393e-05,
      "loss": 3.2436,
      "step": 940
    },
    {
      "epoch": 104.55555555555556,
      "grad_norm": 1.7925760746002197,
      "learning_rate": 4.831460674157304e-05,
      "loss": 1.2212,
      "step": 941
    },
    {
      "epoch": 104.66666666666667,
      "grad_norm": 0.773827850818634,
      "learning_rate": 4.825842696629213e-05,
      "loss": 2.8769,
      "step": 942
    },
    {
      "epoch": 104.77777777777777,
      "grad_norm": 4.388983726501465,
      "learning_rate": 4.820224719101124e-05,
      "loss": 2.9989,
      "step": 943
    },
    {
      "epoch": 104.88888888888889,
      "grad_norm": 3.422429084777832,
      "learning_rate": 4.8146067415730336e-05,
      "loss": 3.056,
      "step": 944
    },
    {
      "epoch": 105.0,
      "grad_norm": 0.5941075682640076,
      "learning_rate": 4.808988764044944e-05,
      "loss": 2.8473,
      "step": 945
    },
    {
      "epoch": 105.11111111111111,
      "grad_norm": 3.7638564109802246,
      "learning_rate": 4.803370786516854e-05,
      "loss": 3.1715,
      "step": 946
    },
    {
      "epoch": 105.22222222222223,
      "grad_norm": 2.776057243347168,
      "learning_rate": 4.797752808988764e-05,
      "loss": 2.8591,
      "step": 947
    },
    {
      "epoch": 105.33333333333333,
      "grad_norm": 1.480230450630188,
      "learning_rate": 4.792134831460675e-05,
      "loss": 2.9701,
      "step": 948
    },
    {
      "epoch": 105.44444444444444,
      "grad_norm": 0.4925660192966461,
      "learning_rate": 4.786516853932584e-05,
      "loss": 2.8428,
      "step": 949
    },
    {
      "epoch": 105.55555555555556,
      "grad_norm": 0.7257344722747803,
      "learning_rate": 4.780898876404495e-05,
      "loss": 2.8636,
      "step": 950
    },
    {
      "epoch": 105.66666666666667,
      "grad_norm": 0.8824461102485657,
      "learning_rate": 4.7752808988764046e-05,
      "loss": 2.8649,
      "step": 951
    },
    {
      "epoch": 105.77777777777777,
      "grad_norm": 0.6826027035713196,
      "learning_rate": 4.769662921348315e-05,
      "loss": 2.8674,
      "step": 952
    },
    {
      "epoch": 105.88888888888889,
      "grad_norm": 3.68265962600708,
      "learning_rate": 4.764044943820225e-05,
      "loss": 3.0359,
      "step": 953
    },
    {
      "epoch": 106.0,
      "grad_norm": 2.525419235229492,
      "learning_rate": 4.758426966292135e-05,
      "loss": 1.503,
      "step": 954
    },
    {
      "epoch": 106.11111111111111,
      "grad_norm": 0.6714555025100708,
      "learning_rate": 4.752808988764045e-05,
      "loss": 2.8633,
      "step": 955
    },
    {
      "epoch": 106.22222222222223,
      "grad_norm": 3.0598950386047363,
      "learning_rate": 4.747191011235955e-05,
      "loss": 3.0035,
      "step": 956
    },
    {
      "epoch": 106.33333333333333,
      "grad_norm": 3.6820552349090576,
      "learning_rate": 4.7415730337078655e-05,
      "loss": 3.3553,
      "step": 957
    },
    {
      "epoch": 106.44444444444444,
      "grad_norm": 1.0530351400375366,
      "learning_rate": 4.7359550561797756e-05,
      "loss": 2.9303,
      "step": 958
    },
    {
      "epoch": 106.55555555555556,
      "grad_norm": 0.6730227470397949,
      "learning_rate": 4.730337078651685e-05,
      "loss": 2.8981,
      "step": 959
    },
    {
      "epoch": 106.66666666666667,
      "grad_norm": 0.5864951610565186,
      "learning_rate": 4.724719101123596e-05,
      "loss": 2.8096,
      "step": 960
    },
    {
      "epoch": 106.77777777777777,
      "grad_norm": 7.1456217765808105,
      "learning_rate": 4.719101123595506e-05,
      "loss": 3.328,
      "step": 961
    },
    {
      "epoch": 106.88888888888889,
      "grad_norm": 2.3820621967315674,
      "learning_rate": 4.713483146067416e-05,
      "loss": 1.4334,
      "step": 962
    },
    {
      "epoch": 107.0,
      "grad_norm": 5.80103874206543,
      "learning_rate": 4.707865168539326e-05,
      "loss": 3.5727,
      "step": 963
    },
    {
      "epoch": 107.11111111111111,
      "grad_norm": 2.3984134197235107,
      "learning_rate": 4.7022471910112365e-05,
      "loss": 2.9567,
      "step": 964
    },
    {
      "epoch": 107.22222222222223,
      "grad_norm": 3.7039849758148193,
      "learning_rate": 4.6966292134831466e-05,
      "loss": 2.9006,
      "step": 965
    },
    {
      "epoch": 107.33333333333333,
      "grad_norm": 3.5737743377685547,
      "learning_rate": 4.691011235955056e-05,
      "loss": 3.0957,
      "step": 966
    },
    {
      "epoch": 107.44444444444444,
      "grad_norm": 0.54817134141922,
      "learning_rate": 4.685393258426967e-05,
      "loss": 2.8448,
      "step": 967
    },
    {
      "epoch": 107.55555555555556,
      "grad_norm": 1.9096612930297852,
      "learning_rate": 4.6797752808988764e-05,
      "loss": 1.2491,
      "step": 968
    },
    {
      "epoch": 107.66666666666667,
      "grad_norm": 0.5560027956962585,
      "learning_rate": 4.674157303370787e-05,
      "loss": 2.79,
      "step": 969
    },
    {
      "epoch": 107.77777777777777,
      "grad_norm": 5.711716175079346,
      "learning_rate": 4.668539325842697e-05,
      "loss": 3.6343,
      "step": 970
    },
    {
      "epoch": 107.88888888888889,
      "grad_norm": 0.6604973673820496,
      "learning_rate": 4.662921348314607e-05,
      "loss": 2.9021,
      "step": 971
    },
    {
      "epoch": 108.0,
      "grad_norm": 1.5429376363754272,
      "learning_rate": 4.657303370786517e-05,
      "loss": 2.8753,
      "step": 972
    },
    {
      "epoch": 108.11111111111111,
      "grad_norm": 0.953145444393158,
      "learning_rate": 4.651685393258427e-05,
      "loss": 2.8986,
      "step": 973
    },
    {
      "epoch": 108.22222222222223,
      "grad_norm": 0.6083997488021851,
      "learning_rate": 4.646067415730337e-05,
      "loss": 2.8257,
      "step": 974
    },
    {
      "epoch": 108.33333333333333,
      "grad_norm": 0.6282479763031006,
      "learning_rate": 4.6404494382022474e-05,
      "loss": 2.7847,
      "step": 975
    },
    {
      "epoch": 108.44444444444444,
      "grad_norm": 0.8396974205970764,
      "learning_rate": 4.6348314606741575e-05,
      "loss": 2.8479,
      "step": 976
    },
    {
      "epoch": 108.55555555555556,
      "grad_norm": 0.8128193616867065,
      "learning_rate": 4.629213483146068e-05,
      "loss": 2.8476,
      "step": 977
    },
    {
      "epoch": 108.66666666666667,
      "grad_norm": 2.2926557064056396,
      "learning_rate": 4.623595505617978e-05,
      "loss": 1.2366,
      "step": 978
    },
    {
      "epoch": 108.77777777777777,
      "grad_norm": 3.2514374256134033,
      "learning_rate": 4.617977528089888e-05,
      "loss": 2.9479,
      "step": 979
    },
    {
      "epoch": 108.88888888888889,
      "grad_norm": 2.7986834049224854,
      "learning_rate": 4.612359550561798e-05,
      "loss": 2.9047,
      "step": 980
    },
    {
      "epoch": 109.0,
      "grad_norm": 0.8011327385902405,
      "learning_rate": 4.606741573033708e-05,
      "loss": 2.9456,
      "step": 981
    },
    {
      "epoch": 109.11111111111111,
      "grad_norm": 3.5584828853607178,
      "learning_rate": 4.6011235955056184e-05,
      "loss": 2.8688,
      "step": 982
    },
    {
      "epoch": 109.22222222222223,
      "grad_norm": 3.828228712081909,
      "learning_rate": 4.5955056179775285e-05,
      "loss": 2.9521,
      "step": 983
    },
    {
      "epoch": 109.33333333333333,
      "grad_norm": 0.8258352279663086,
      "learning_rate": 4.589887640449439e-05,
      "loss": 2.804,
      "step": 984
    },
    {
      "epoch": 109.44444444444444,
      "grad_norm": 3.255162239074707,
      "learning_rate": 4.584269662921348e-05,
      "loss": 2.9429,
      "step": 985
    },
    {
      "epoch": 109.55555555555556,
      "grad_norm": 0.6407783031463623,
      "learning_rate": 4.578651685393259e-05,
      "loss": 2.8172,
      "step": 986
    },
    {
      "epoch": 109.66666666666667,
      "grad_norm": 1.753906488418579,
      "learning_rate": 4.5730337078651684e-05,
      "loss": 1.1655,
      "step": 987
    },
    {
      "epoch": 109.77777777777777,
      "grad_norm": 2.8885388374328613,
      "learning_rate": 4.567415730337079e-05,
      "loss": 2.8702,
      "step": 988
    },
    {
      "epoch": 109.88888888888889,
      "grad_norm": 1.9797956943511963,
      "learning_rate": 4.561797752808989e-05,
      "loss": 2.818,
      "step": 989
    },
    {
      "epoch": 110.0,
      "grad_norm": 1.0137465000152588,
      "learning_rate": 4.5561797752808995e-05,
      "loss": 2.8116,
      "step": 990
    },
    {
      "epoch": 110.11111111111111,
      "grad_norm": 1.3653050661087036,
      "learning_rate": 4.550561797752809e-05,
      "loss": 2.9243,
      "step": 991
    },
    {
      "epoch": 110.22222222222223,
      "grad_norm": 3.2032151222229004,
      "learning_rate": 4.544943820224719e-05,
      "loss": 2.8503,
      "step": 992
    },
    {
      "epoch": 110.33333333333333,
      "grad_norm": 0.8565147519111633,
      "learning_rate": 4.539325842696629e-05,
      "loss": 2.8001,
      "step": 993
    },
    {
      "epoch": 110.44444444444444,
      "grad_norm": 8.428500175476074,
      "learning_rate": 4.5337078651685394e-05,
      "loss": 3.6799,
      "step": 994
    },
    {
      "epoch": 110.55555555555556,
      "grad_norm": 1.4792696237564087,
      "learning_rate": 4.5280898876404496e-05,
      "loss": 0.8378,
      "step": 995
    },
    {
      "epoch": 110.66666666666667,
      "grad_norm": 2.7597382068634033,
      "learning_rate": 4.52247191011236e-05,
      "loss": 2.8357,
      "step": 996
    },
    {
      "epoch": 110.77777777777777,
      "grad_norm": 0.6624487042427063,
      "learning_rate": 4.51685393258427e-05,
      "loss": 2.8477,
      "step": 997
    },
    {
      "epoch": 110.88888888888889,
      "grad_norm": 5.417117118835449,
      "learning_rate": 4.51123595505618e-05,
      "loss": 2.9834,
      "step": 998
    },
    {
      "epoch": 111.0,
      "grad_norm": 0.6226736903190613,
      "learning_rate": 4.50561797752809e-05,
      "loss": 2.8086,
      "step": 999
    },
    {
      "epoch": 111.11111111111111,
      "grad_norm": 0.7927370667457581,
      "learning_rate": 4.5e-05,
      "loss": 2.8134,
      "step": 1000
    },
    {
      "epoch": 111.22222222222223,
      "grad_norm": 0.5695257186889648,
      "learning_rate": 4.4943820224719104e-05,
      "loss": 2.8196,
      "step": 1001
    },
    {
      "epoch": 111.33333333333333,
      "grad_norm": 0.9042918682098389,
      "learning_rate": 4.4887640449438206e-05,
      "loss": 2.8515,
      "step": 1002
    },
    {
      "epoch": 111.44444444444444,
      "grad_norm": 21.233919143676758,
      "learning_rate": 4.483146067415731e-05,
      "loss": 4.5515,
      "step": 1003
    },
    {
      "epoch": 111.55555555555556,
      "grad_norm": 0.7137892842292786,
      "learning_rate": 4.47752808988764e-05,
      "loss": 2.7749,
      "step": 1004
    },
    {
      "epoch": 111.66666666666667,
      "grad_norm": 4.967580318450928,
      "learning_rate": 4.471910112359551e-05,
      "loss": 3.3063,
      "step": 1005
    },
    {
      "epoch": 111.77777777777777,
      "grad_norm": 1.7972605228424072,
      "learning_rate": 4.4662921348314605e-05,
      "loss": 1.2101,
      "step": 1006
    },
    {
      "epoch": 111.88888888888889,
      "grad_norm": 4.0937628746032715,
      "learning_rate": 4.460674157303371e-05,
      "loss": 2.9285,
      "step": 1007
    },
    {
      "epoch": 112.0,
      "grad_norm": 3.707564115524292,
      "learning_rate": 4.455056179775281e-05,
      "loss": 2.9669,
      "step": 1008
    },
    {
      "epoch": 112.11111111111111,
      "grad_norm": 0.9988823533058167,
      "learning_rate": 4.4494382022471916e-05,
      "loss": 2.8697,
      "step": 1009
    },
    {
      "epoch": 112.22222222222223,
      "grad_norm": 2.0884881019592285,
      "learning_rate": 4.443820224719101e-05,
      "loss": 2.9007,
      "step": 1010
    },
    {
      "epoch": 112.33333333333333,
      "grad_norm": 0.7267173528671265,
      "learning_rate": 4.438202247191011e-05,
      "loss": 2.7908,
      "step": 1011
    },
    {
      "epoch": 112.44444444444444,
      "grad_norm": 7.336358070373535,
      "learning_rate": 4.432584269662921e-05,
      "loss": 3.3309,
      "step": 1012
    },
    {
      "epoch": 112.55555555555556,
      "grad_norm": 0.6244440078735352,
      "learning_rate": 4.4269662921348315e-05,
      "loss": 2.7968,
      "step": 1013
    },
    {
      "epoch": 112.66666666666667,
      "grad_norm": 1.2672268152236938,
      "learning_rate": 4.4213483146067416e-05,
      "loss": 2.8067,
      "step": 1014
    },
    {
      "epoch": 112.77777777777777,
      "grad_norm": 1.5338855981826782,
      "learning_rate": 4.415730337078652e-05,
      "loss": 1.2278,
      "step": 1015
    },
    {
      "epoch": 112.88888888888889,
      "grad_norm": 3.3920416831970215,
      "learning_rate": 4.410112359550562e-05,
      "loss": 2.879,
      "step": 1016
    },
    {
      "epoch": 113.0,
      "grad_norm": 0.7087801694869995,
      "learning_rate": 4.404494382022472e-05,
      "loss": 2.7595,
      "step": 1017
    },
    {
      "epoch": 113.11111111111111,
      "grad_norm": 1.0431573390960693,
      "learning_rate": 4.398876404494382e-05,
      "loss": 2.8339,
      "step": 1018
    },
    {
      "epoch": 113.22222222222223,
      "grad_norm": 5.351095676422119,
      "learning_rate": 4.393258426966292e-05,
      "loss": 2.8904,
      "step": 1019
    },
    {
      "epoch": 113.33333333333333,
      "grad_norm": 0.531162440776825,
      "learning_rate": 4.3876404494382025e-05,
      "loss": 2.8367,
      "step": 1020
    },
    {
      "epoch": 113.44444444444444,
      "grad_norm": 2.1611921787261963,
      "learning_rate": 4.3820224719101126e-05,
      "loss": 2.8062,
      "step": 1021
    },
    {
      "epoch": 113.55555555555556,
      "grad_norm": 0.5754994750022888,
      "learning_rate": 4.376404494382023e-05,
      "loss": 2.7948,
      "step": 1022
    },
    {
      "epoch": 113.66666666666667,
      "grad_norm": 1.090951681137085,
      "learning_rate": 4.370786516853933e-05,
      "loss": 2.7828,
      "step": 1023
    },
    {
      "epoch": 113.77777777777777,
      "grad_norm": 1.653485894203186,
      "learning_rate": 4.365168539325843e-05,
      "loss": 2.7866,
      "step": 1024
    },
    {
      "epoch": 113.88888888888889,
      "grad_norm": 0.6637227535247803,
      "learning_rate": 4.3595505617977525e-05,
      "loss": 2.7979,
      "step": 1025
    },
    {
      "epoch": 114.0,
      "grad_norm": 1.4370698928833008,
      "learning_rate": 4.353932584269663e-05,
      "loss": 1.1295,
      "step": 1026
    },
    {
      "epoch": 114.11111111111111,
      "grad_norm": 2.744929552078247,
      "learning_rate": 4.348314606741573e-05,
      "loss": 2.8546,
      "step": 1027
    },
    {
      "epoch": 114.22222222222223,
      "grad_norm": 0.7733280062675476,
      "learning_rate": 4.3426966292134836e-05,
      "loss": 2.7898,
      "step": 1028
    },
    {
      "epoch": 114.33333333333333,
      "grad_norm": 3.693690538406372,
      "learning_rate": 4.337078651685393e-05,
      "loss": 2.8785,
      "step": 1029
    },
    {
      "epoch": 114.44444444444444,
      "grad_norm": 0.7536188364028931,
      "learning_rate": 4.331460674157304e-05,
      "loss": 2.7447,
      "step": 1030
    },
    {
      "epoch": 114.55555555555556,
      "grad_norm": 2.116504192352295,
      "learning_rate": 4.3258426966292134e-05,
      "loss": 0.9987,
      "step": 1031
    },
    {
      "epoch": 114.66666666666667,
      "grad_norm": 7.486969470977783,
      "learning_rate": 4.3202247191011235e-05,
      "loss": 3.1581,
      "step": 1032
    },
    {
      "epoch": 114.77777777777777,
      "grad_norm": 0.901515781879425,
      "learning_rate": 4.314606741573034e-05,
      "loss": 2.802,
      "step": 1033
    },
    {
      "epoch": 114.88888888888889,
      "grad_norm": 0.8285298347473145,
      "learning_rate": 4.308988764044944e-05,
      "loss": 2.8122,
      "step": 1034
    },
    {
      "epoch": 115.0,
      "grad_norm": 2.944478988647461,
      "learning_rate": 4.3033707865168546e-05,
      "loss": 2.8732,
      "step": 1035
    },
    {
      "epoch": 115.11111111111111,
      "grad_norm": 0.9723432660102844,
      "learning_rate": 4.297752808988764e-05,
      "loss": 2.786,
      "step": 1036
    },
    {
      "epoch": 115.22222222222223,
      "grad_norm": 5.453938961029053,
      "learning_rate": 4.292134831460675e-05,
      "loss": 3.0282,
      "step": 1037
    },
    {
      "epoch": 115.33333333333333,
      "grad_norm": 0.5672189593315125,
      "learning_rate": 4.2865168539325844e-05,
      "loss": 2.8238,
      "step": 1038
    },
    {
      "epoch": 115.44444444444444,
      "grad_norm": 2.331986427307129,
      "learning_rate": 4.2808988764044945e-05,
      "loss": 1.3463,
      "step": 1039
    },
    {
      "epoch": 115.55555555555556,
      "grad_norm": 0.8636431097984314,
      "learning_rate": 4.2752808988764047e-05,
      "loss": 2.8453,
      "step": 1040
    },
    {
      "epoch": 115.66666666666667,
      "grad_norm": 1.6454999446868896,
      "learning_rate": 4.269662921348315e-05,
      "loss": 2.8228,
      "step": 1041
    },
    {
      "epoch": 115.77777777777777,
      "grad_norm": 0.7495819926261902,
      "learning_rate": 4.264044943820225e-05,
      "loss": 2.7486,
      "step": 1042
    },
    {
      "epoch": 115.88888888888889,
      "grad_norm": 0.5917243957519531,
      "learning_rate": 4.258426966292135e-05,
      "loss": 2.7085,
      "step": 1043
    },
    {
      "epoch": 116.0,
      "grad_norm": 1.041593074798584,
      "learning_rate": 4.2528089887640446e-05,
      "loss": 2.8505,
      "step": 1044
    },
    {
      "epoch": 116.11111111111111,
      "grad_norm": 0.556868314743042,
      "learning_rate": 4.2471910112359554e-05,
      "loss": 2.7676,
      "step": 1045
    },
    {
      "epoch": 116.22222222222223,
      "grad_norm": 0.5750336647033691,
      "learning_rate": 4.2415730337078655e-05,
      "loss": 2.7151,
      "step": 1046
    },
    {
      "epoch": 116.33333333333333,
      "grad_norm": 0.8345291614532471,
      "learning_rate": 4.235955056179776e-05,
      "loss": 2.7785,
      "step": 1047
    },
    {
      "epoch": 116.44444444444444,
      "grad_norm": 1.9430071115493774,
      "learning_rate": 4.230337078651686e-05,
      "loss": 2.8123,
      "step": 1048
    },
    {
      "epoch": 116.55555555555556,
      "grad_norm": 0.6999562382698059,
      "learning_rate": 4.224719101123596e-05,
      "loss": 2.7945,
      "step": 1049
    },
    {
      "epoch": 116.66666666666667,
      "grad_norm": 2.5139975547790527,
      "learning_rate": 4.219101123595506e-05,
      "loss": 2.8042,
      "step": 1050
    },
    {
      "epoch": 116.77777777777777,
      "grad_norm": 1.8500884771347046,
      "learning_rate": 4.2134831460674156e-05,
      "loss": 1.1675,
      "step": 1051
    },
    {
      "epoch": 116.88888888888889,
      "grad_norm": 1.2957841157913208,
      "learning_rate": 4.2078651685393264e-05,
      "loss": 2.7329,
      "step": 1052
    },
    {
      "epoch": 117.0,
      "grad_norm": 0.5411546230316162,
      "learning_rate": 4.202247191011236e-05,
      "loss": 2.6959,
      "step": 1053
    },
    {
      "epoch": 117.11111111111111,
      "grad_norm": 0.6290262341499329,
      "learning_rate": 4.196629213483147e-05,
      "loss": 2.7487,
      "step": 1054
    },
    {
      "epoch": 117.22222222222223,
      "grad_norm": 0.5680067539215088,
      "learning_rate": 4.191011235955056e-05,
      "loss": 2.7102,
      "step": 1055
    },
    {
      "epoch": 117.33333333333333,
      "grad_norm": 0.645845353603363,
      "learning_rate": 4.185393258426967e-05,
      "loss": 2.7348,
      "step": 1056
    },
    {
      "epoch": 117.44444444444444,
      "grad_norm": 0.7957825064659119,
      "learning_rate": 4.1797752808988764e-05,
      "loss": 2.7703,
      "step": 1057
    },
    {
      "epoch": 117.55555555555556,
      "grad_norm": 6.8588995933532715,
      "learning_rate": 4.1741573033707866e-05,
      "loss": 2.8909,
      "step": 1058
    },
    {
      "epoch": 117.66666666666667,
      "grad_norm": 0.5567198991775513,
      "learning_rate": 4.168539325842697e-05,
      "loss": 2.7413,
      "step": 1059
    },
    {
      "epoch": 117.77777777777777,
      "grad_norm": 2.1659505367279053,
      "learning_rate": 4.162921348314607e-05,
      "loss": 1.2792,
      "step": 1060
    },
    {
      "epoch": 117.88888888888889,
      "grad_norm": 7.395113468170166,
      "learning_rate": 4.157303370786517e-05,
      "loss": 3.1685,
      "step": 1061
    },
    {
      "epoch": 118.0,
      "grad_norm": 0.6056838631629944,
      "learning_rate": 4.151685393258427e-05,
      "loss": 2.7417,
      "step": 1062
    },
    {
      "epoch": 118.11111111111111,
      "grad_norm": 5.574851989746094,
      "learning_rate": 4.146067415730337e-05,
      "loss": 2.8313,
      "step": 1063
    },
    {
      "epoch": 118.22222222222223,
      "grad_norm": 1.6582140922546387,
      "learning_rate": 4.1404494382022474e-05,
      "loss": 2.7222,
      "step": 1064
    },
    {
      "epoch": 118.33333333333333,
      "grad_norm": 1.760368824005127,
      "learning_rate": 4.1348314606741576e-05,
      "loss": 1.1514,
      "step": 1065
    },
    {
      "epoch": 118.44444444444444,
      "grad_norm": 1.3142699003219604,
      "learning_rate": 4.129213483146068e-05,
      "loss": 2.8454,
      "step": 1066
    },
    {
      "epoch": 118.55555555555556,
      "grad_norm": 8.821016311645508,
      "learning_rate": 4.123595505617978e-05,
      "loss": 2.9607,
      "step": 1067
    },
    {
      "epoch": 118.66666666666667,
      "grad_norm": 0.5422412157058716,
      "learning_rate": 4.117977528089888e-05,
      "loss": 2.7017,
      "step": 1068
    },
    {
      "epoch": 118.77777777777777,
      "grad_norm": 0.8206367492675781,
      "learning_rate": 4.112359550561798e-05,
      "loss": 2.69,
      "step": 1069
    },
    {
      "epoch": 118.88888888888889,
      "grad_norm": 0.5433792471885681,
      "learning_rate": 4.106741573033708e-05,
      "loss": 2.7556,
      "step": 1070
    },
    {
      "epoch": 119.0,
      "grad_norm": 8.0418701171875,
      "learning_rate": 4.1011235955056184e-05,
      "loss": 3.2569,
      "step": 1071
    },
    {
      "epoch": 119.11111111111111,
      "grad_norm": 0.624508261680603,
      "learning_rate": 4.095505617977528e-05,
      "loss": 2.6672,
      "step": 1072
    },
    {
      "epoch": 119.22222222222223,
      "grad_norm": 1.1624482870101929,
      "learning_rate": 4.089887640449439e-05,
      "loss": 2.8299,
      "step": 1073
    },
    {
      "epoch": 119.33333333333333,
      "grad_norm": 0.5092486143112183,
      "learning_rate": 4.084269662921348e-05,
      "loss": 2.6866,
      "step": 1074
    },
    {
      "epoch": 119.44444444444444,
      "grad_norm": 2.7517895698547363,
      "learning_rate": 4.078651685393259e-05,
      "loss": 2.9408,
      "step": 1075
    },
    {
      "epoch": 119.55555555555556,
      "grad_norm": 2.8968727588653564,
      "learning_rate": 4.0730337078651685e-05,
      "loss": 2.7788,
      "step": 1076
    },
    {
      "epoch": 119.66666666666667,
      "grad_norm": 5.915564060211182,
      "learning_rate": 4.067415730337079e-05,
      "loss": 2.8775,
      "step": 1077
    },
    {
      "epoch": 119.77777777777777,
      "grad_norm": 1.5327283143997192,
      "learning_rate": 4.061797752808989e-05,
      "loss": 2.8533,
      "step": 1078
    },
    {
      "epoch": 119.88888888888889,
      "grad_norm": 1.7473031282424927,
      "learning_rate": 4.056179775280899e-05,
      "loss": 1.0528,
      "step": 1079
    },
    {
      "epoch": 120.0,
      "grad_norm": 0.8950269818305969,
      "learning_rate": 4.050561797752809e-05,
      "loss": 2.8334,
      "step": 1080
    },
    {
      "epoch": 120.11111111111111,
      "grad_norm": 0.6020804643630981,
      "learning_rate": 4.044943820224719e-05,
      "loss": 2.7069,
      "step": 1081
    },
    {
      "epoch": 120.22222222222223,
      "grad_norm": 0.9146314263343811,
      "learning_rate": 4.039325842696629e-05,
      "loss": 2.78,
      "step": 1082
    },
    {
      "epoch": 120.33333333333333,
      "grad_norm": 5.004517555236816,
      "learning_rate": 4.0337078651685395e-05,
      "loss": 3.0447,
      "step": 1083
    },
    {
      "epoch": 120.44444444444444,
      "grad_norm": 0.537715494632721,
      "learning_rate": 4.0280898876404496e-05,
      "loss": 2.703,
      "step": 1084
    },
    {
      "epoch": 120.55555555555556,
      "grad_norm": 0.7355660200119019,
      "learning_rate": 4.02247191011236e-05,
      "loss": 2.6756,
      "step": 1085
    },
    {
      "epoch": 120.66666666666667,
      "grad_norm": 0.8205580711364746,
      "learning_rate": 4.01685393258427e-05,
      "loss": 2.7288,
      "step": 1086
    },
    {
      "epoch": 120.77777777777777,
      "grad_norm": 1.9967200756072998,
      "learning_rate": 4.01123595505618e-05,
      "loss": 1.0057,
      "step": 1087
    },
    {
      "epoch": 120.88888888888889,
      "grad_norm": 2.3595495223999023,
      "learning_rate": 4.00561797752809e-05,
      "loss": 2.7706,
      "step": 1088
    },
    {
      "epoch": 121.0,
      "grad_norm": 1.1544153690338135,
      "learning_rate": 4e-05,
      "loss": 2.7605,
      "step": 1089
    },
    {
      "epoch": 121.11111111111111,
      "grad_norm": 0.9770340919494629,
      "learning_rate": 3.9943820224719105e-05,
      "loss": 2.7937,
      "step": 1090
    },
    {
      "epoch": 121.22222222222223,
      "grad_norm": 0.7393724918365479,
      "learning_rate": 3.98876404494382e-05,
      "loss": 2.6579,
      "step": 1091
    },
    {
      "epoch": 121.33333333333333,
      "grad_norm": 0.5138930082321167,
      "learning_rate": 3.983146067415731e-05,
      "loss": 2.6944,
      "step": 1092
    },
    {
      "epoch": 121.44444444444444,
      "grad_norm": 0.7415362000465393,
      "learning_rate": 3.97752808988764e-05,
      "loss": 2.7376,
      "step": 1093
    },
    {
      "epoch": 121.55555555555556,
      "grad_norm": 1.7446749210357666,
      "learning_rate": 3.971910112359551e-05,
      "loss": 2.7188,
      "step": 1094
    },
    {
      "epoch": 121.66666666666667,
      "grad_norm": 2.4155185222625732,
      "learning_rate": 3.9662921348314605e-05,
      "loss": 2.6966,
      "step": 1095
    },
    {
      "epoch": 121.77777777777777,
      "grad_norm": 0.5675355195999146,
      "learning_rate": 3.960674157303371e-05,
      "loss": 2.6322,
      "step": 1096
    },
    {
      "epoch": 121.88888888888889,
      "grad_norm": 0.588842511177063,
      "learning_rate": 3.955056179775281e-05,
      "loss": 2.6986,
      "step": 1097
    },
    {
      "epoch": 122.0,
      "grad_norm": 1.7217129468917847,
      "learning_rate": 3.949438202247191e-05,
      "loss": 1.1712,
      "step": 1098
    },
    {
      "epoch": 122.11111111111111,
      "grad_norm": 2.8092756271362305,
      "learning_rate": 3.943820224719101e-05,
      "loss": 2.8244,
      "step": 1099
    },
    {
      "epoch": 122.22222222222223,
      "grad_norm": 1.7442567348480225,
      "learning_rate": 3.938202247191011e-05,
      "loss": 1.0133,
      "step": 1100
    },
    {
      "epoch": 122.33333333333333,
      "grad_norm": 3.851349115371704,
      "learning_rate": 3.9325842696629214e-05,
      "loss": 2.7523,
      "step": 1101
    },
    {
      "epoch": 122.44444444444444,
      "grad_norm": 0.5892805457115173,
      "learning_rate": 3.9269662921348315e-05,
      "loss": 2.6557,
      "step": 1102
    },
    {
      "epoch": 122.55555555555556,
      "grad_norm": 3.435349702835083,
      "learning_rate": 3.921348314606742e-05,
      "loss": 2.7261,
      "step": 1103
    },
    {
      "epoch": 122.66666666666667,
      "grad_norm": 0.9732338190078735,
      "learning_rate": 3.915730337078652e-05,
      "loss": 2.7494,
      "step": 1104
    },
    {
      "epoch": 122.77777777777777,
      "grad_norm": 4.332823276519775,
      "learning_rate": 3.910112359550562e-05,
      "loss": 2.838,
      "step": 1105
    },
    {
      "epoch": 122.88888888888889,
      "grad_norm": 4.572086334228516,
      "learning_rate": 3.904494382022472e-05,
      "loss": 3.0015,
      "step": 1106
    },
    {
      "epoch": 123.0,
      "grad_norm": 0.577297031879425,
      "learning_rate": 3.898876404494382e-05,
      "loss": 2.6389,
      "step": 1107
    },
    {
      "epoch": 123.11111111111111,
      "grad_norm": 3.7268049716949463,
      "learning_rate": 3.8932584269662924e-05,
      "loss": 2.7039,
      "step": 1108
    },
    {
      "epoch": 123.22222222222223,
      "grad_norm": 1.8657786846160889,
      "learning_rate": 3.8876404494382025e-05,
      "loss": 0.9484,
      "step": 1109
    },
    {
      "epoch": 123.33333333333333,
      "grad_norm": 2.096954584121704,
      "learning_rate": 3.8820224719101127e-05,
      "loss": 2.699,
      "step": 1110
    },
    {
      "epoch": 123.44444444444444,
      "grad_norm": 2.4509172439575195,
      "learning_rate": 3.876404494382023e-05,
      "loss": 2.7173,
      "step": 1111
    },
    {
      "epoch": 123.55555555555556,
      "grad_norm": 0.6318029165267944,
      "learning_rate": 3.870786516853932e-05,
      "loss": 2.6652,
      "step": 1112
    },
    {
      "epoch": 123.66666666666667,
      "grad_norm": 1.3412572145462036,
      "learning_rate": 3.865168539325843e-05,
      "loss": 2.7227,
      "step": 1113
    },
    {
      "epoch": 123.77777777777777,
      "grad_norm": 0.6182957887649536,
      "learning_rate": 3.8595505617977526e-05,
      "loss": 2.6598,
      "step": 1114
    },
    {
      "epoch": 123.88888888888889,
      "grad_norm": 2.83673095703125,
      "learning_rate": 3.8539325842696634e-05,
      "loss": 2.8891,
      "step": 1115
    },
    {
      "epoch": 124.0,
      "grad_norm": 0.5775184035301208,
      "learning_rate": 3.8483146067415735e-05,
      "loss": 2.6497,
      "step": 1116
    },
    {
      "epoch": 124.11111111111111,
      "grad_norm": 3.7146778106689453,
      "learning_rate": 3.842696629213483e-05,
      "loss": 2.9361,
      "step": 1117
    },
    {
      "epoch": 124.22222222222223,
      "grad_norm": 2.5611069202423096,
      "learning_rate": 3.837078651685394e-05,
      "loss": 2.8962,
      "step": 1118
    },
    {
      "epoch": 124.33333333333333,
      "grad_norm": 0.44519802927970886,
      "learning_rate": 3.831460674157303e-05,
      "loss": 2.6774,
      "step": 1119
    },
    {
      "epoch": 124.44444444444444,
      "grad_norm": 11.83144760131836,
      "learning_rate": 3.825842696629214e-05,
      "loss": 3.5795,
      "step": 1120
    },
    {
      "epoch": 124.55555555555556,
      "grad_norm": 3.764294385910034,
      "learning_rate": 3.8202247191011236e-05,
      "loss": 2.7909,
      "step": 1121
    },
    {
      "epoch": 124.66666666666667,
      "grad_norm": 0.7159907221794128,
      "learning_rate": 3.8146067415730344e-05,
      "loss": 2.7329,
      "step": 1122
    },
    {
      "epoch": 124.77777777777777,
      "grad_norm": 0.8565533757209778,
      "learning_rate": 3.808988764044944e-05,
      "loss": 2.6854,
      "step": 1123
    },
    {
      "epoch": 124.88888888888889,
      "grad_norm": 0.5203150510787964,
      "learning_rate": 3.803370786516854e-05,
      "loss": 2.5991,
      "step": 1124
    },
    {
      "epoch": 125.0,
      "grad_norm": 1.5912078619003296,
      "learning_rate": 3.797752808988764e-05,
      "loss": 1.0009,
      "step": 1125
    },
    {
      "epoch": 125.11111111111111,
      "grad_norm": 0.649666965007782,
      "learning_rate": 3.792134831460674e-05,
      "loss": 2.66,
      "step": 1126
    },
    {
      "epoch": 125.22222222222223,
      "grad_norm": 0.4882543683052063,
      "learning_rate": 3.7865168539325844e-05,
      "loss": 2.6119,
      "step": 1127
    },
    {
      "epoch": 125.33333333333333,
      "grad_norm": 0.6898497343063354,
      "learning_rate": 3.7808988764044946e-05,
      "loss": 2.7134,
      "step": 1128
    },
    {
      "epoch": 125.44444444444444,
      "grad_norm": 1.0831880569458008,
      "learning_rate": 3.775280898876405e-05,
      "loss": 2.6937,
      "step": 1129
    },
    {
      "epoch": 125.55555555555556,
      "grad_norm": 0.5899055600166321,
      "learning_rate": 3.769662921348315e-05,
      "loss": 2.6774,
      "step": 1130
    },
    {
      "epoch": 125.66666666666667,
      "grad_norm": 2.0514323711395264,
      "learning_rate": 3.764044943820225e-05,
      "loss": 2.7075,
      "step": 1131
    },
    {
      "epoch": 125.77777777777777,
      "grad_norm": 0.7477720379829407,
      "learning_rate": 3.758426966292135e-05,
      "loss": 2.6808,
      "step": 1132
    },
    {
      "epoch": 125.88888888888889,
      "grad_norm": 0.49855104088783264,
      "learning_rate": 3.752808988764045e-05,
      "loss": 2.6225,
      "step": 1133
    },
    {
      "epoch": 126.0,
      "grad_norm": 2.3762478828430176,
      "learning_rate": 3.7471910112359554e-05,
      "loss": 1.0691,
      "step": 1134
    },
    {
      "epoch": 126.11111111111111,
      "grad_norm": 7.316212177276611,
      "learning_rate": 3.7415730337078656e-05,
      "loss": 2.9911,
      "step": 1135
    },
    {
      "epoch": 126.22222222222223,
      "grad_norm": 1.6006516218185425,
      "learning_rate": 3.735955056179776e-05,
      "loss": 0.8428,
      "step": 1136
    },
    {
      "epoch": 126.33333333333333,
      "grad_norm": 6.1666789054870605,
      "learning_rate": 3.730337078651686e-05,
      "loss": 2.9328,
      "step": 1137
    },
    {
      "epoch": 126.44444444444444,
      "grad_norm": 2.3711133003234863,
      "learning_rate": 3.724719101123595e-05,
      "loss": 2.7417,
      "step": 1138
    },
    {
      "epoch": 126.55555555555556,
      "grad_norm": 0.6378403306007385,
      "learning_rate": 3.719101123595506e-05,
      "loss": 2.6889,
      "step": 1139
    },
    {
      "epoch": 126.66666666666667,
      "grad_norm": 3.8110709190368652,
      "learning_rate": 3.7134831460674156e-05,
      "loss": 2.7832,
      "step": 1140
    },
    {
      "epoch": 126.77777777777777,
      "grad_norm": 2.0860300064086914,
      "learning_rate": 3.7078651685393264e-05,
      "loss": 2.6418,
      "step": 1141
    },
    {
      "epoch": 126.88888888888889,
      "grad_norm": 4.120660305023193,
      "learning_rate": 3.702247191011236e-05,
      "loss": 2.778,
      "step": 1142
    },
    {
      "epoch": 127.0,
      "grad_norm": 0.6063642501831055,
      "learning_rate": 3.696629213483147e-05,
      "loss": 2.648,
      "step": 1143
    },
    {
      "epoch": 127.11111111111111,
      "grad_norm": 4.600405216217041,
      "learning_rate": 3.691011235955056e-05,
      "loss": 2.8814,
      "step": 1144
    },
    {
      "epoch": 127.22222222222223,
      "grad_norm": 6.161039352416992,
      "learning_rate": 3.685393258426966e-05,
      "loss": 3.0518,
      "step": 1145
    },
    {
      "epoch": 127.33333333333333,
      "grad_norm": 3.8845109939575195,
      "learning_rate": 3.6797752808988765e-05,
      "loss": 2.7049,
      "step": 1146
    },
    {
      "epoch": 127.44444444444444,
      "grad_norm": 0.712857186794281,
      "learning_rate": 3.6741573033707866e-05,
      "loss": 2.6272,
      "step": 1147
    },
    {
      "epoch": 127.55555555555556,
      "grad_norm": 4.5460100173950195,
      "learning_rate": 3.668539325842697e-05,
      "loss": 2.7136,
      "step": 1148
    },
    {
      "epoch": 127.66666666666667,
      "grad_norm": 0.5798310041427612,
      "learning_rate": 3.662921348314607e-05,
      "loss": 2.6924,
      "step": 1149
    },
    {
      "epoch": 127.77777777777777,
      "grad_norm": 6.430018424987793,
      "learning_rate": 3.657303370786517e-05,
      "loss": 2.8594,
      "step": 1150
    },
    {
      "epoch": 127.88888888888889,
      "grad_norm": 1.7713478803634644,
      "learning_rate": 3.651685393258427e-05,
      "loss": 0.9462,
      "step": 1151
    },
    {
      "epoch": 128.0,
      "grad_norm": 1.9307353496551514,
      "learning_rate": 3.646067415730337e-05,
      "loss": 2.7026,
      "step": 1152
    },
    {
      "epoch": 128.11111111111111,
      "grad_norm": 0.6582085490226746,
      "learning_rate": 3.6404494382022475e-05,
      "loss": 2.6238,
      "step": 1153
    },
    {
      "epoch": 128.22222222222223,
      "grad_norm": 0.7070684432983398,
      "learning_rate": 3.6348314606741576e-05,
      "loss": 2.6824,
      "step": 1154
    },
    {
      "epoch": 128.33333333333334,
      "grad_norm": 1.334386944770813,
      "learning_rate": 3.629213483146068e-05,
      "loss": 2.6054,
      "step": 1155
    },
    {
      "epoch": 128.44444444444446,
      "grad_norm": 4.9994916915893555,
      "learning_rate": 3.623595505617978e-05,
      "loss": 2.6994,
      "step": 1156
    },
    {
      "epoch": 128.55555555555554,
      "grad_norm": 0.705837607383728,
      "learning_rate": 3.6179775280898874e-05,
      "loss": 2.7127,
      "step": 1157
    },
    {
      "epoch": 128.66666666666666,
      "grad_norm": 0.5970171093940735,
      "learning_rate": 3.612359550561798e-05,
      "loss": 2.6467,
      "step": 1158
    },
    {
      "epoch": 128.77777777777777,
      "grad_norm": 0.6464208960533142,
      "learning_rate": 3.6067415730337076e-05,
      "loss": 2.5727,
      "step": 1159
    },
    {
      "epoch": 128.88888888888889,
      "grad_norm": 1.850324273109436,
      "learning_rate": 3.6011235955056185e-05,
      "loss": 0.9819,
      "step": 1160
    },
    {
      "epoch": 129.0,
      "grad_norm": 0.7868307828903198,
      "learning_rate": 3.595505617977528e-05,
      "loss": 2.6604,
      "step": 1161
    },
    {
      "epoch": 129.11111111111111,
      "grad_norm": 0.9278706312179565,
      "learning_rate": 3.589887640449439e-05,
      "loss": 2.7035,
      "step": 1162
    },
    {
      "epoch": 129.22222222222223,
      "grad_norm": 5.886099338531494,
      "learning_rate": 3.584269662921348e-05,
      "loss": 2.8035,
      "step": 1163
    },
    {
      "epoch": 129.33333333333334,
      "grad_norm": 0.49894073605537415,
      "learning_rate": 3.5786516853932584e-05,
      "loss": 2.6374,
      "step": 1164
    },
    {
      "epoch": 129.44444444444446,
      "grad_norm": 1.047865390777588,
      "learning_rate": 3.5730337078651685e-05,
      "loss": 2.668,
      "step": 1165
    },
    {
      "epoch": 129.55555555555554,
      "grad_norm": 0.6391425132751465,
      "learning_rate": 3.5674157303370787e-05,
      "loss": 2.6954,
      "step": 1166
    },
    {
      "epoch": 129.66666666666666,
      "grad_norm": 0.752392590045929,
      "learning_rate": 3.561797752808989e-05,
      "loss": 2.6325,
      "step": 1167
    },
    {
      "epoch": 129.77777777777777,
      "grad_norm": 2.2709484100341797,
      "learning_rate": 3.556179775280899e-05,
      "loss": 1.149,
      "step": 1168
    },
    {
      "epoch": 129.88888888888889,
      "grad_norm": 5.284289836883545,
      "learning_rate": 3.550561797752809e-05,
      "loss": 3.0225,
      "step": 1169
    },
    {
      "epoch": 130.0,
      "grad_norm": 5.206356048583984,
      "learning_rate": 3.544943820224719e-05,
      "loss": 2.8954,
      "step": 1170
    },
    {
      "epoch": 130.11111111111111,
      "grad_norm": 2.6125073432922363,
      "learning_rate": 3.5393258426966294e-05,
      "loss": 2.7154,
      "step": 1171
    },
    {
      "epoch": 130.22222222222223,
      "grad_norm": 3.09474515914917,
      "learning_rate": 3.5337078651685395e-05,
      "loss": 2.7472,
      "step": 1172
    },
    {
      "epoch": 130.33333333333334,
      "grad_norm": 0.6202564239501953,
      "learning_rate": 3.5280898876404497e-05,
      "loss": 2.6665,
      "step": 1173
    },
    {
      "epoch": 130.44444444444446,
      "grad_norm": 0.7739221453666687,
      "learning_rate": 3.52247191011236e-05,
      "loss": 2.655,
      "step": 1174
    },
    {
      "epoch": 130.55555555555554,
      "grad_norm": 1.7105282545089722,
      "learning_rate": 3.51685393258427e-05,
      "loss": 0.9492,
      "step": 1175
    },
    {
      "epoch": 130.66666666666666,
      "grad_norm": 0.7670508623123169,
      "learning_rate": 3.51123595505618e-05,
      "loss": 2.6806,
      "step": 1176
    },
    {
      "epoch": 130.77777777777777,
      "grad_norm": 0.5757406949996948,
      "learning_rate": 3.50561797752809e-05,
      "loss": 2.7029,
      "step": 1177
    },
    {
      "epoch": 130.88888888888889,
      "grad_norm": 2.82169246673584,
      "learning_rate": 3.5e-05,
      "loss": 2.6794,
      "step": 1178
    },
    {
      "epoch": 131.0,
      "grad_norm": 0.7108612656593323,
      "learning_rate": 3.4943820224719105e-05,
      "loss": 2.6638,
      "step": 1179
    },
    {
      "epoch": 131.11111111111111,
      "grad_norm": 2.573638439178467,
      "learning_rate": 3.48876404494382e-05,
      "loss": 2.6936,
      "step": 1180
    },
    {
      "epoch": 131.22222222222223,
      "grad_norm": 4.660914421081543,
      "learning_rate": 3.483146067415731e-05,
      "loss": 3.6125,
      "step": 1181
    },
    {
      "epoch": 131.33333333333334,
      "grad_norm": 0.7307197451591492,
      "learning_rate": 3.47752808988764e-05,
      "loss": 2.6755,
      "step": 1182
    },
    {
      "epoch": 131.44444444444446,
      "grad_norm": 2.003828287124634,
      "learning_rate": 3.471910112359551e-05,
      "loss": 2.7172,
      "step": 1183
    },
    {
      "epoch": 131.55555555555554,
      "grad_norm": 2.53985333442688,
      "learning_rate": 3.4662921348314606e-05,
      "loss": 1.1413,
      "step": 1184
    },
    {
      "epoch": 131.66666666666666,
      "grad_norm": 1.2055587768554688,
      "learning_rate": 3.460674157303371e-05,
      "loss": 2.5991,
      "step": 1185
    },
    {
      "epoch": 131.77777777777777,
      "grad_norm": 6.206266403198242,
      "learning_rate": 3.455056179775281e-05,
      "loss": 2.735,
      "step": 1186
    },
    {
      "epoch": 131.88888888888889,
      "grad_norm": 0.7125216722488403,
      "learning_rate": 3.449438202247191e-05,
      "loss": 2.6682,
      "step": 1187
    },
    {
      "epoch": 132.0,
      "grad_norm": 4.779767990112305,
      "learning_rate": 3.443820224719102e-05,
      "loss": 2.7351,
      "step": 1188
    },
    {
      "epoch": 132.11111111111111,
      "grad_norm": 0.6141129732131958,
      "learning_rate": 3.438202247191011e-05,
      "loss": 2.585,
      "step": 1189
    },
    {
      "epoch": 132.22222222222223,
      "grad_norm": 0.7464039921760559,
      "learning_rate": 3.4325842696629214e-05,
      "loss": 2.6705,
      "step": 1190
    },
    {
      "epoch": 132.33333333333334,
      "grad_norm": 0.7473056316375732,
      "learning_rate": 3.4269662921348316e-05,
      "loss": 2.6129,
      "step": 1191
    },
    {
      "epoch": 132.44444444444446,
      "grad_norm": 4.0844316482543945,
      "learning_rate": 3.421348314606742e-05,
      "loss": 2.7624,
      "step": 1192
    },
    {
      "epoch": 132.55555555555554,
      "grad_norm": 0.5849827527999878,
      "learning_rate": 3.415730337078652e-05,
      "loss": 2.5747,
      "step": 1193
    },
    {
      "epoch": 132.66666666666666,
      "grad_norm": 0.5229830145835876,
      "learning_rate": 3.410112359550562e-05,
      "loss": 2.573,
      "step": 1194
    },
    {
      "epoch": 132.77777777777777,
      "grad_norm": 2.1484878063201904,
      "learning_rate": 3.404494382022472e-05,
      "loss": 2.6811,
      "step": 1195
    },
    {
      "epoch": 132.88888888888889,
      "grad_norm": 1.3574763536453247,
      "learning_rate": 3.398876404494382e-05,
      "loss": 1.0083,
      "step": 1196
    },
    {
      "epoch": 133.0,
      "grad_norm": 0.7832595705986023,
      "learning_rate": 3.393258426966292e-05,
      "loss": 2.6182,
      "step": 1197
    },
    {
      "epoch": 133.11111111111111,
      "grad_norm": 0.4965781569480896,
      "learning_rate": 3.3876404494382026e-05,
      "loss": 2.6174,
      "step": 1198
    },
    {
      "epoch": 133.22222222222223,
      "grad_norm": 0.5578709244728088,
      "learning_rate": 3.382022471910112e-05,
      "loss": 2.5974,
      "step": 1199
    },
    {
      "epoch": 133.33333333333334,
      "grad_norm": 2.025442361831665,
      "learning_rate": 3.376404494382023e-05,
      "loss": 1.0782,
      "step": 1200
    },
    {
      "epoch": 133.44444444444446,
      "grad_norm": 0.5337744951248169,
      "learning_rate": 3.370786516853933e-05,
      "loss": 2.5834,
      "step": 1201
    },
    {
      "epoch": 133.55555555555554,
      "grad_norm": 0.5566045641899109,
      "learning_rate": 3.365168539325843e-05,
      "loss": 2.6128,
      "step": 1202
    },
    {
      "epoch": 133.66666666666666,
      "grad_norm": 0.7431209683418274,
      "learning_rate": 3.359550561797753e-05,
      "loss": 2.6162,
      "step": 1203
    },
    {
      "epoch": 133.77777777777777,
      "grad_norm": 0.4797523319721222,
      "learning_rate": 3.353932584269663e-05,
      "loss": 2.6137,
      "step": 1204
    },
    {
      "epoch": 133.88888888888889,
      "grad_norm": 1.4745500087738037,
      "learning_rate": 3.3483146067415736e-05,
      "loss": 2.655,
      "step": 1205
    },
    {
      "epoch": 134.0,
      "grad_norm": 0.710418701171875,
      "learning_rate": 3.342696629213483e-05,
      "loss": 2.597,
      "step": 1206
    },
    {
      "epoch": 134.11111111111111,
      "grad_norm": 2.457294225692749,
      "learning_rate": 3.337078651685394e-05,
      "loss": 2.8527,
      "step": 1207
    },
    {
      "epoch": 134.22222222222223,
      "grad_norm": 0.5623713731765747,
      "learning_rate": 3.331460674157303e-05,
      "loss": 2.5777,
      "step": 1208
    },
    {
      "epoch": 134.33333333333334,
      "grad_norm": 0.5921882390975952,
      "learning_rate": 3.325842696629214e-05,
      "loss": 2.5845,
      "step": 1209
    },
    {
      "epoch": 134.44444444444446,
      "grad_norm": 1.4020812511444092,
      "learning_rate": 3.3202247191011236e-05,
      "loss": 2.5982,
      "step": 1210
    },
    {
      "epoch": 134.55555555555554,
      "grad_norm": 2.5631957054138184,
      "learning_rate": 3.314606741573034e-05,
      "loss": 1.1032,
      "step": 1211
    },
    {
      "epoch": 134.66666666666666,
      "grad_norm": 4.274522304534912,
      "learning_rate": 3.308988764044944e-05,
      "loss": 2.7993,
      "step": 1212
    },
    {
      "epoch": 134.77777777777777,
      "grad_norm": 0.5835435390472412,
      "learning_rate": 3.303370786516854e-05,
      "loss": 2.5981,
      "step": 1213
    },
    {
      "epoch": 134.88888888888889,
      "grad_norm": 0.5675833225250244,
      "learning_rate": 3.297752808988764e-05,
      "loss": 2.606,
      "step": 1214
    },
    {
      "epoch": 135.0,
      "grad_norm": 1.109958291053772,
      "learning_rate": 3.292134831460674e-05,
      "loss": 2.6199,
      "step": 1215
    },
    {
      "epoch": 135.11111111111111,
      "grad_norm": 7.322094440460205,
      "learning_rate": 3.2865168539325845e-05,
      "loss": 2.7748,
      "step": 1216
    },
    {
      "epoch": 135.22222222222223,
      "grad_norm": 1.9797207117080688,
      "learning_rate": 3.2808988764044946e-05,
      "loss": 1.1461,
      "step": 1217
    },
    {
      "epoch": 135.33333333333334,
      "grad_norm": 3.0272858142852783,
      "learning_rate": 3.275280898876405e-05,
      "loss": 2.6657,
      "step": 1218
    },
    {
      "epoch": 135.44444444444446,
      "grad_norm": 5.490553855895996,
      "learning_rate": 3.269662921348315e-05,
      "loss": 2.8222,
      "step": 1219
    },
    {
      "epoch": 135.55555555555554,
      "grad_norm": 0.5340441465377808,
      "learning_rate": 3.264044943820225e-05,
      "loss": 2.6062,
      "step": 1220
    },
    {
      "epoch": 135.66666666666666,
      "grad_norm": 0.9420080780982971,
      "learning_rate": 3.258426966292135e-05,
      "loss": 2.5996,
      "step": 1221
    },
    {
      "epoch": 135.77777777777777,
      "grad_norm": 4.106889724731445,
      "learning_rate": 3.252808988764045e-05,
      "loss": 2.7012,
      "step": 1222
    },
    {
      "epoch": 135.88888888888889,
      "grad_norm": 1.6498125791549683,
      "learning_rate": 3.2471910112359555e-05,
      "loss": 2.6294,
      "step": 1223
    },
    {
      "epoch": 136.0,
      "grad_norm": 10.423210144042969,
      "learning_rate": 3.2415730337078656e-05,
      "loss": 3.1376,
      "step": 1224
    },
    {
      "epoch": 136.11111111111111,
      "grad_norm": 0.9257247447967529,
      "learning_rate": 3.235955056179775e-05,
      "loss": 2.5884,
      "step": 1225
    },
    {
      "epoch": 136.22222222222223,
      "grad_norm": 4.430121898651123,
      "learning_rate": 3.230337078651686e-05,
      "loss": 2.7304,
      "step": 1226
    },
    {
      "epoch": 136.33333333333334,
      "grad_norm": 2.5254673957824707,
      "learning_rate": 3.2247191011235954e-05,
      "loss": 2.8754,
      "step": 1227
    },
    {
      "epoch": 136.44444444444446,
      "grad_norm": 0.7473694086074829,
      "learning_rate": 3.219101123595506e-05,
      "loss": 2.5922,
      "step": 1228
    },
    {
      "epoch": 136.55555555555554,
      "grad_norm": 0.6707175374031067,
      "learning_rate": 3.2134831460674156e-05,
      "loss": 2.5817,
      "step": 1229
    },
    {
      "epoch": 136.66666666666666,
      "grad_norm": 7.789554119110107,
      "learning_rate": 3.207865168539326e-05,
      "loss": 2.8499,
      "step": 1230
    },
    {
      "epoch": 136.77777777777777,
      "grad_norm": 0.49371233582496643,
      "learning_rate": 3.202247191011236e-05,
      "loss": 2.5604,
      "step": 1231
    },
    {
      "epoch": 136.88888888888889,
      "grad_norm": 1.60886549949646,
      "learning_rate": 3.196629213483146e-05,
      "loss": 0.9751,
      "step": 1232
    },
    {
      "epoch": 137.0,
      "grad_norm": 0.8687248826026917,
      "learning_rate": 3.191011235955056e-05,
      "loss": 2.6736,
      "step": 1233
    },
    {
      "epoch": 137.11111111111111,
      "grad_norm": 7.255692481994629,
      "learning_rate": 3.1853932584269664e-05,
      "loss": 2.8502,
      "step": 1234
    },
    {
      "epoch": 137.22222222222223,
      "grad_norm": 10.097939491271973,
      "learning_rate": 3.1797752808988765e-05,
      "loss": 3.0791,
      "step": 1235
    },
    {
      "epoch": 137.33333333333334,
      "grad_norm": 7.602878570556641,
      "learning_rate": 3.1741573033707866e-05,
      "loss": 2.8499,
      "step": 1236
    },
    {
      "epoch": 137.44444444444446,
      "grad_norm": 2.494906187057495,
      "learning_rate": 3.168539325842697e-05,
      "loss": 2.6594,
      "step": 1237
    },
    {
      "epoch": 137.55555555555554,
      "grad_norm": 0.6643630862236023,
      "learning_rate": 3.162921348314607e-05,
      "loss": 2.5932,
      "step": 1238
    },
    {
      "epoch": 137.66666666666666,
      "grad_norm": 0.4800410866737366,
      "learning_rate": 3.157303370786517e-05,
      "loss": 2.5761,
      "step": 1239
    },
    {
      "epoch": 137.77777777777777,
      "grad_norm": 0.7431023716926575,
      "learning_rate": 3.151685393258427e-05,
      "loss": 2.5771,
      "step": 1240
    },
    {
      "epoch": 137.88888888888889,
      "grad_norm": 1.3398112058639526,
      "learning_rate": 3.1460674157303374e-05,
      "loss": 0.9411,
      "step": 1241
    },
    {
      "epoch": 138.0,
      "grad_norm": 0.5546737313270569,
      "learning_rate": 3.1404494382022475e-05,
      "loss": 2.578,
      "step": 1242
    },
    {
      "epoch": 138.11111111111111,
      "grad_norm": 0.7025308012962341,
      "learning_rate": 3.1348314606741577e-05,
      "loss": 2.5795,
      "step": 1243
    },
    {
      "epoch": 138.22222222222223,
      "grad_norm": 0.723200798034668,
      "learning_rate": 3.129213483146067e-05,
      "loss": 2.6305,
      "step": 1244
    },
    {
      "epoch": 138.33333333333334,
      "grad_norm": 2.7304129600524902,
      "learning_rate": 3.123595505617978e-05,
      "loss": 1.2661,
      "step": 1245
    },
    {
      "epoch": 138.44444444444446,
      "grad_norm": 0.6214211583137512,
      "learning_rate": 3.1179775280898874e-05,
      "loss": 2.6036,
      "step": 1246
    },
    {
      "epoch": 138.55555555555554,
      "grad_norm": 0.5228791832923889,
      "learning_rate": 3.112359550561798e-05,
      "loss": 2.5411,
      "step": 1247
    },
    {
      "epoch": 138.66666666666666,
      "grad_norm": 0.7452292442321777,
      "learning_rate": 3.106741573033708e-05,
      "loss": 2.5472,
      "step": 1248
    },
    {
      "epoch": 138.77777777777777,
      "grad_norm": 0.6350393295288086,
      "learning_rate": 3.1011235955056185e-05,
      "loss": 2.5901,
      "step": 1249
    },
    {
      "epoch": 138.88888888888889,
      "grad_norm": 0.7201810479164124,
      "learning_rate": 3.095505617977528e-05,
      "loss": 2.6051,
      "step": 1250
    },
    {
      "epoch": 139.0,
      "grad_norm": 7.003787994384766,
      "learning_rate": 3.089887640449438e-05,
      "loss": 2.8182,
      "step": 1251
    },
    {
      "epoch": 139.11111111111111,
      "grad_norm": 3.358189821243286,
      "learning_rate": 3.084269662921348e-05,
      "loss": 2.6695,
      "step": 1252
    },
    {
      "epoch": 139.22222222222223,
      "grad_norm": 2.3209357261657715,
      "learning_rate": 3.0786516853932584e-05,
      "loss": 0.997,
      "step": 1253
    },
    {
      "epoch": 139.33333333333334,
      "grad_norm": 0.49636778235435486,
      "learning_rate": 3.0730337078651686e-05,
      "loss": 2.5476,
      "step": 1254
    },
    {
      "epoch": 139.44444444444446,
      "grad_norm": 5.878370761871338,
      "learning_rate": 3.067415730337079e-05,
      "loss": 2.97,
      "step": 1255
    },
    {
      "epoch": 139.55555555555554,
      "grad_norm": 0.5363266468048096,
      "learning_rate": 3.061797752808989e-05,
      "loss": 2.5609,
      "step": 1256
    },
    {
      "epoch": 139.66666666666666,
      "grad_norm": 0.7623980641365051,
      "learning_rate": 3.056179775280899e-05,
      "loss": 2.5294,
      "step": 1257
    },
    {
      "epoch": 139.77777777777777,
      "grad_norm": 0.5374128818511963,
      "learning_rate": 3.0505617977528088e-05,
      "loss": 2.5728,
      "step": 1258
    },
    {
      "epoch": 139.88888888888889,
      "grad_norm": 0.8346835374832153,
      "learning_rate": 3.0449438202247193e-05,
      "loss": 2.6112,
      "step": 1259
    },
    {
      "epoch": 140.0,
      "grad_norm": 1.0082526206970215,
      "learning_rate": 3.039325842696629e-05,
      "loss": 2.5718,
      "step": 1260
    },
    {
      "epoch": 140.11111111111111,
      "grad_norm": 0.7582518458366394,
      "learning_rate": 3.0337078651685396e-05,
      "loss": 2.5698,
      "step": 1261
    },
    {
      "epoch": 140.22222222222223,
      "grad_norm": 3.89794659614563,
      "learning_rate": 3.0280898876404497e-05,
      "loss": 2.6996,
      "step": 1262
    },
    {
      "epoch": 140.33333333333334,
      "grad_norm": 0.8740755915641785,
      "learning_rate": 3.02247191011236e-05,
      "loss": 2.5895,
      "step": 1263
    },
    {
      "epoch": 140.44444444444446,
      "grad_norm": 1.8526607751846313,
      "learning_rate": 3.01685393258427e-05,
      "loss": 0.9092,
      "step": 1264
    },
    {
      "epoch": 140.55555555555554,
      "grad_norm": 0.539044201374054,
      "learning_rate": 3.0112359550561798e-05,
      "loss": 2.5475,
      "step": 1265
    },
    {
      "epoch": 140.66666666666666,
      "grad_norm": 5.691719055175781,
      "learning_rate": 3.0056179775280903e-05,
      "loss": 2.7595,
      "step": 1266
    },
    {
      "epoch": 140.77777777777777,
      "grad_norm": 1.8750897645950317,
      "learning_rate": 3e-05,
      "loss": 2.5733,
      "step": 1267
    },
    {
      "epoch": 140.88888888888889,
      "grad_norm": 0.6257858276367188,
      "learning_rate": 2.9943820224719106e-05,
      "loss": 2.5972,
      "step": 1268
    },
    {
      "epoch": 141.0,
      "grad_norm": 0.8218543529510498,
      "learning_rate": 2.9887640449438204e-05,
      "loss": 2.5691,
      "step": 1269
    },
    {
      "epoch": 141.11111111111111,
      "grad_norm": 0.7081440687179565,
      "learning_rate": 2.98314606741573e-05,
      "loss": 2.5661,
      "step": 1270
    },
    {
      "epoch": 141.22222222222223,
      "grad_norm": 0.5639961957931519,
      "learning_rate": 2.9775280898876406e-05,
      "loss": 2.564,
      "step": 1271
    },
    {
      "epoch": 141.33333333333334,
      "grad_norm": 0.79343181848526,
      "learning_rate": 2.9719101123595505e-05,
      "loss": 2.5939,
      "step": 1272
    },
    {
      "epoch": 141.44444444444446,
      "grad_norm": 1.9064075946807861,
      "learning_rate": 2.966292134831461e-05,
      "loss": 1.0177,
      "step": 1273
    },
    {
      "epoch": 141.55555555555554,
      "grad_norm": 0.6013826131820679,
      "learning_rate": 2.9606741573033707e-05,
      "loss": 2.5943,
      "step": 1274
    },
    {
      "epoch": 141.66666666666666,
      "grad_norm": 2.570542335510254,
      "learning_rate": 2.9550561797752812e-05,
      "loss": 2.5944,
      "step": 1275
    },
    {
      "epoch": 141.77777777777777,
      "grad_norm": 0.5711947679519653,
      "learning_rate": 2.949438202247191e-05,
      "loss": 2.5518,
      "step": 1276
    },
    {
      "epoch": 141.88888888888889,
      "grad_norm": 2.7923223972320557,
      "learning_rate": 2.9438202247191012e-05,
      "loss": 2.6726,
      "step": 1277
    },
    {
      "epoch": 142.0,
      "grad_norm": 2.3902103900909424,
      "learning_rate": 2.9382022471910113e-05,
      "loss": 2.7747,
      "step": 1278
    },
    {
      "epoch": 142.11111111111111,
      "grad_norm": 1.2990437746047974,
      "learning_rate": 2.9325842696629215e-05,
      "loss": 2.5243,
      "step": 1279
    },
    {
      "epoch": 142.22222222222223,
      "grad_norm": 0.585560142993927,
      "learning_rate": 2.9269662921348316e-05,
      "loss": 2.5728,
      "step": 1280
    },
    {
      "epoch": 142.33333333333334,
      "grad_norm": 0.6832206845283508,
      "learning_rate": 2.9213483146067417e-05,
      "loss": 2.5268,
      "step": 1281
    },
    {
      "epoch": 142.44444444444446,
      "grad_norm": 0.4908377528190613,
      "learning_rate": 2.9157303370786522e-05,
      "loss": 2.5443,
      "step": 1282
    },
    {
      "epoch": 142.55555555555554,
      "grad_norm": 2.372370958328247,
      "learning_rate": 2.910112359550562e-05,
      "loss": 2.6128,
      "step": 1283
    },
    {
      "epoch": 142.66666666666666,
      "grad_norm": 0.5656682252883911,
      "learning_rate": 2.904494382022472e-05,
      "loss": 2.5421,
      "step": 1284
    },
    {
      "epoch": 142.77777777777777,
      "grad_norm": 1.5985755920410156,
      "learning_rate": 2.8988764044943823e-05,
      "loss": 0.9348,
      "step": 1285
    },
    {
      "epoch": 142.88888888888889,
      "grad_norm": 0.6299189329147339,
      "learning_rate": 2.893258426966292e-05,
      "loss": 2.5646,
      "step": 1286
    },
    {
      "epoch": 143.0,
      "grad_norm": 0.5363426804542542,
      "learning_rate": 2.8876404494382026e-05,
      "loss": 2.5353,
      "step": 1287
    },
    {
      "epoch": 143.11111111111111,
      "grad_norm": 3.937633991241455,
      "learning_rate": 2.8820224719101124e-05,
      "loss": 2.5492,
      "step": 1288
    },
    {
      "epoch": 143.22222222222223,
      "grad_norm": 7.511236667633057,
      "learning_rate": 2.876404494382023e-05,
      "loss": 2.6566,
      "step": 1289
    },
    {
      "epoch": 143.33333333333334,
      "grad_norm": 10.574472427368164,
      "learning_rate": 2.8707865168539327e-05,
      "loss": 3.1607,
      "step": 1290
    },
    {
      "epoch": 143.44444444444446,
      "grad_norm": 2.8978030681610107,
      "learning_rate": 2.8651685393258425e-05,
      "loss": 2.6283,
      "step": 1291
    },
    {
      "epoch": 143.55555555555554,
      "grad_norm": 4.8130364418029785,
      "learning_rate": 2.859550561797753e-05,
      "loss": 2.9079,
      "step": 1292
    },
    {
      "epoch": 143.66666666666666,
      "grad_norm": 6.0121588706970215,
      "learning_rate": 2.8539325842696628e-05,
      "loss": 2.5942,
      "step": 1293
    },
    {
      "epoch": 143.77777777777777,
      "grad_norm": 4.114785671234131,
      "learning_rate": 2.8483146067415733e-05,
      "loss": 2.6251,
      "step": 1294
    },
    {
      "epoch": 143.88888888888889,
      "grad_norm": 0.6098710298538208,
      "learning_rate": 2.842696629213483e-05,
      "loss": 2.5042,
      "step": 1295
    },
    {
      "epoch": 144.0,
      "grad_norm": 1.4305086135864258,
      "learning_rate": 2.8370786516853936e-05,
      "loss": 0.963,
      "step": 1296
    },
    {
      "epoch": 144.11111111111111,
      "grad_norm": 0.5532388091087341,
      "learning_rate": 2.8314606741573037e-05,
      "loss": 2.5464,
      "step": 1297
    },
    {
      "epoch": 144.22222222222223,
      "grad_norm": 5.9651713371276855,
      "learning_rate": 2.8258426966292135e-05,
      "loss": 2.5782,
      "step": 1298
    },
    {
      "epoch": 144.33333333333334,
      "grad_norm": 2.0939507484436035,
      "learning_rate": 2.820224719101124e-05,
      "loss": 2.5664,
      "step": 1299
    },
    {
      "epoch": 144.44444444444446,
      "grad_norm": 4.763273239135742,
      "learning_rate": 2.8146067415730338e-05,
      "loss": 2.8341,
      "step": 1300
    },
    {
      "epoch": 144.55555555555554,
      "grad_norm": 1.9319180250167847,
      "learning_rate": 2.8089887640449443e-05,
      "loss": 2.5805,
      "step": 1301
    },
    {
      "epoch": 144.66666666666666,
      "grad_norm": 1.5261210203170776,
      "learning_rate": 2.803370786516854e-05,
      "loss": 0.9436,
      "step": 1302
    },
    {
      "epoch": 144.77777777777777,
      "grad_norm": 2.004542350769043,
      "learning_rate": 2.797752808988764e-05,
      "loss": 2.6436,
      "step": 1303
    },
    {
      "epoch": 144.88888888888889,
      "grad_norm": 3.2411584854125977,
      "learning_rate": 2.7921348314606744e-05,
      "loss": 2.7106,
      "step": 1304
    },
    {
      "epoch": 145.0,
      "grad_norm": 0.5673665404319763,
      "learning_rate": 2.786516853932584e-05,
      "loss": 2.5048,
      "step": 1305
    },
    {
      "epoch": 145.11111111111111,
      "grad_norm": 7.196498870849609,
      "learning_rate": 2.7808988764044946e-05,
      "loss": 2.7009,
      "step": 1306
    },
    {
      "epoch": 145.22222222222223,
      "grad_norm": 0.6737168431282043,
      "learning_rate": 2.7752808988764045e-05,
      "loss": 2.5685,
      "step": 1307
    },
    {
      "epoch": 145.33333333333334,
      "grad_norm": 2.380781650543213,
      "learning_rate": 2.769662921348315e-05,
      "loss": 2.6011,
      "step": 1308
    },
    {
      "epoch": 145.44444444444446,
      "grad_norm": 0.7092120051383972,
      "learning_rate": 2.7640449438202247e-05,
      "loss": 2.635,
      "step": 1309
    },
    {
      "epoch": 145.55555555555554,
      "grad_norm": 0.7700417637825012,
      "learning_rate": 2.7584269662921345e-05,
      "loss": 2.5436,
      "step": 1310
    },
    {
      "epoch": 145.66666666666666,
      "grad_norm": 0.5965874195098877,
      "learning_rate": 2.752808988764045e-05,
      "loss": 2.5953,
      "step": 1311
    },
    {
      "epoch": 145.77777777777777,
      "grad_norm": 1.7176016569137573,
      "learning_rate": 2.7471910112359552e-05,
      "loss": 0.9695,
      "step": 1312
    },
    {
      "epoch": 145.88888888888889,
      "grad_norm": 2.221121311187744,
      "learning_rate": 2.7415730337078653e-05,
      "loss": 2.6321,
      "step": 1313
    },
    {
      "epoch": 146.0,
      "grad_norm": 0.4790748357772827,
      "learning_rate": 2.7359550561797755e-05,
      "loss": 2.4959,
      "step": 1314
    },
    {
      "epoch": 146.11111111111111,
      "grad_norm": 0.8524045348167419,
      "learning_rate": 2.7303370786516856e-05,
      "loss": 2.56,
      "step": 1315
    },
    {
      "epoch": 146.22222222222223,
      "grad_norm": 5.244569778442383,
      "learning_rate": 2.7247191011235957e-05,
      "loss": 2.9276,
      "step": 1316
    },
    {
      "epoch": 146.33333333333334,
      "grad_norm": 0.7286125421524048,
      "learning_rate": 2.7191011235955055e-05,
      "loss": 2.5642,
      "step": 1317
    },
    {
      "epoch": 146.44444444444446,
      "grad_norm": 1.0521240234375,
      "learning_rate": 2.713483146067416e-05,
      "loss": 2.54,
      "step": 1318
    },
    {
      "epoch": 146.55555555555554,
      "grad_norm": 0.5596203804016113,
      "learning_rate": 2.707865168539326e-05,
      "loss": 2.5464,
      "step": 1319
    },
    {
      "epoch": 146.66666666666666,
      "grad_norm": 3.5487611293792725,
      "learning_rate": 2.7022471910112363e-05,
      "loss": 0.8945,
      "step": 1320
    },
    {
      "epoch": 146.77777777777777,
      "grad_norm": 1.1957554817199707,
      "learning_rate": 2.696629213483146e-05,
      "loss": 2.5594,
      "step": 1321
    },
    {
      "epoch": 146.88888888888889,
      "grad_norm": 4.782747745513916,
      "learning_rate": 2.6910112359550566e-05,
      "loss": 2.607,
      "step": 1322
    },
    {
      "epoch": 147.0,
      "grad_norm": 0.7464610934257507,
      "learning_rate": 2.6853932584269664e-05,
      "loss": 2.5646,
      "step": 1323
    },
    {
      "epoch": 147.11111111111111,
      "grad_norm": 1.1916117668151855,
      "learning_rate": 2.6797752808988762e-05,
      "loss": 2.5995,
      "step": 1324
    },
    {
      "epoch": 147.22222222222223,
      "grad_norm": 0.5510404706001282,
      "learning_rate": 2.6741573033707867e-05,
      "loss": 2.5431,
      "step": 1325
    },
    {
      "epoch": 147.33333333333334,
      "grad_norm": 0.7479584217071533,
      "learning_rate": 2.6685393258426965e-05,
      "loss": 2.559,
      "step": 1326
    },
    {
      "epoch": 147.44444444444446,
      "grad_norm": 2.5699009895324707,
      "learning_rate": 2.662921348314607e-05,
      "loss": 1.044,
      "step": 1327
    },
    {
      "epoch": 147.55555555555554,
      "grad_norm": 0.6040319204330444,
      "learning_rate": 2.6573033707865168e-05,
      "loss": 2.5009,
      "step": 1328
    },
    {
      "epoch": 147.66666666666666,
      "grad_norm": 1.216593861579895,
      "learning_rate": 2.6516853932584273e-05,
      "loss": 2.5942,
      "step": 1329
    },
    {
      "epoch": 147.77777777777777,
      "grad_norm": 0.7146031260490417,
      "learning_rate": 2.646067415730337e-05,
      "loss": 2.539,
      "step": 1330
    },
    {
      "epoch": 147.88888888888889,
      "grad_norm": 3.3663265705108643,
      "learning_rate": 2.6404494382022472e-05,
      "loss": 2.6873,
      "step": 1331
    },
    {
      "epoch": 148.0,
      "grad_norm": 0.6311789155006409,
      "learning_rate": 2.6348314606741577e-05,
      "loss": 2.483,
      "step": 1332
    },
    {
      "epoch": 148.11111111111111,
      "grad_norm": 0.8054523468017578,
      "learning_rate": 2.6292134831460675e-05,
      "loss": 2.5556,
      "step": 1333
    },
    {
      "epoch": 148.22222222222223,
      "grad_norm": 0.6980705857276917,
      "learning_rate": 2.623595505617978e-05,
      "loss": 2.5609,
      "step": 1334
    },
    {
      "epoch": 148.33333333333334,
      "grad_norm": 60.283905029296875,
      "learning_rate": 2.6179775280898878e-05,
      "loss": 3.1196,
      "step": 1335
    },
    {
      "epoch": 148.44444444444446,
      "grad_norm": 4.905217170715332,
      "learning_rate": 2.6123595505617983e-05,
      "loss": 2.533,
      "step": 1336
    },
    {
      "epoch": 148.55555555555554,
      "grad_norm": 0.6210505366325378,
      "learning_rate": 2.606741573033708e-05,
      "loss": 2.5657,
      "step": 1337
    },
    {
      "epoch": 148.66666666666666,
      "grad_norm": 2.749685287475586,
      "learning_rate": 2.601123595505618e-05,
      "loss": 2.5203,
      "step": 1338
    },
    {
      "epoch": 148.77777777777777,
      "grad_norm": 2.0913712978363037,
      "learning_rate": 2.5955056179775284e-05,
      "loss": 2.5863,
      "step": 1339
    },
    {
      "epoch": 148.88888888888889,
      "grad_norm": 1.4750173091888428,
      "learning_rate": 2.589887640449438e-05,
      "loss": 0.8218,
      "step": 1340
    },
    {
      "epoch": 149.0,
      "grad_norm": 0.597233772277832,
      "learning_rate": 2.5842696629213486e-05,
      "loss": 2.4995,
      "step": 1341
    },
    {
      "epoch": 149.11111111111111,
      "grad_norm": 5.379366874694824,
      "learning_rate": 2.5786516853932585e-05,
      "loss": 2.6932,
      "step": 1342
    },
    {
      "epoch": 149.22222222222223,
      "grad_norm": 0.6075872182846069,
      "learning_rate": 2.5730337078651683e-05,
      "loss": 2.4814,
      "step": 1343
    },
    {
      "epoch": 149.33333333333334,
      "grad_norm": 3.505704879760742,
      "learning_rate": 2.5674157303370787e-05,
      "loss": 2.6926,
      "step": 1344
    },
    {
      "epoch": 149.44444444444446,
      "grad_norm": 2.371382713317871,
      "learning_rate": 2.5617977528089885e-05,
      "loss": 1.0393,
      "step": 1345
    },
    {
      "epoch": 149.55555555555554,
      "grad_norm": 1.6345144510269165,
      "learning_rate": 2.556179775280899e-05,
      "loss": 2.6088,
      "step": 1346
    },
    {
      "epoch": 149.66666666666666,
      "grad_norm": 3.4604599475860596,
      "learning_rate": 2.550561797752809e-05,
      "loss": 2.6251,
      "step": 1347
    },
    {
      "epoch": 149.77777777777777,
      "grad_norm": 1.1096551418304443,
      "learning_rate": 2.5449438202247193e-05,
      "loss": 2.5427,
      "step": 1348
    },
    {
      "epoch": 149.88888888888889,
      "grad_norm": 0.5952734351158142,
      "learning_rate": 2.5393258426966295e-05,
      "loss": 2.5508,
      "step": 1349
    },
    {
      "epoch": 150.0,
      "grad_norm": 1.471171259880066,
      "learning_rate": 2.5337078651685393e-05,
      "loss": 2.5975,
      "step": 1350
    },
    {
      "epoch": 150.11111111111111,
      "grad_norm": 7.236491680145264,
      "learning_rate": 2.5280898876404497e-05,
      "loss": 2.7514,
      "step": 1351
    },
    {
      "epoch": 150.22222222222223,
      "grad_norm": 0.7319106459617615,
      "learning_rate": 2.5224719101123595e-05,
      "loss": 2.5922,
      "step": 1352
    },
    {
      "epoch": 150.33333333333334,
      "grad_norm": 1.833791732788086,
      "learning_rate": 2.51685393258427e-05,
      "loss": 0.9283,
      "step": 1353
    },
    {
      "epoch": 150.44444444444446,
      "grad_norm": 0.5499405264854431,
      "learning_rate": 2.51123595505618e-05,
      "loss": 2.5037,
      "step": 1354
    },
    {
      "epoch": 150.55555555555554,
      "grad_norm": 0.6021754145622253,
      "learning_rate": 2.5056179775280903e-05,
      "loss": 2.5037,
      "step": 1355
    },
    {
      "epoch": 150.66666666666666,
      "grad_norm": 2.6460349559783936,
      "learning_rate": 2.5e-05,
      "loss": 2.5672,
      "step": 1356
    },
    {
      "epoch": 150.77777777777777,
      "grad_norm": 0.5502268671989441,
      "learning_rate": 2.4943820224719103e-05,
      "loss": 2.5412,
      "step": 1357
    },
    {
      "epoch": 150.88888888888889,
      "grad_norm": 2.371540069580078,
      "learning_rate": 2.4887640449438204e-05,
      "loss": 2.571,
      "step": 1358
    },
    {
      "epoch": 151.0,
      "grad_norm": 0.447543203830719,
      "learning_rate": 2.4831460674157305e-05,
      "loss": 2.5442,
      "step": 1359
    },
    {
      "epoch": 151.11111111111111,
      "grad_norm": 5.456248760223389,
      "learning_rate": 2.4775280898876404e-05,
      "loss": 2.5439,
      "step": 1360
    },
    {
      "epoch": 151.22222222222223,
      "grad_norm": 0.5406797528266907,
      "learning_rate": 2.4719101123595505e-05,
      "loss": 2.5169,
      "step": 1361
    },
    {
      "epoch": 151.33333333333334,
      "grad_norm": 0.6364647150039673,
      "learning_rate": 2.4662921348314606e-05,
      "loss": 2.4618,
      "step": 1362
    },
    {
      "epoch": 151.44444444444446,
      "grad_norm": 0.613446831703186,
      "learning_rate": 2.4606741573033708e-05,
      "loss": 2.5386,
      "step": 1363
    },
    {
      "epoch": 151.55555555555554,
      "grad_norm": 2.491982936859131,
      "learning_rate": 2.455056179775281e-05,
      "loss": 2.5509,
      "step": 1364
    },
    {
      "epoch": 151.66666666666666,
      "grad_norm": 0.48752740025520325,
      "learning_rate": 2.449438202247191e-05,
      "loss": 2.5323,
      "step": 1365
    },
    {
      "epoch": 151.77777777777777,
      "grad_norm": 0.8873234987258911,
      "learning_rate": 2.4438202247191012e-05,
      "loss": 2.4767,
      "step": 1366
    },
    {
      "epoch": 151.88888888888889,
      "grad_norm": 1.1119384765625,
      "learning_rate": 2.4382022471910114e-05,
      "loss": 2.4925,
      "step": 1367
    },
    {
      "epoch": 152.0,
      "grad_norm": 1.7077360153198242,
      "learning_rate": 2.4325842696629215e-05,
      "loss": 0.8307,
      "step": 1368
    },
    {
      "epoch": 152.11111111111111,
      "grad_norm": 0.8259750604629517,
      "learning_rate": 2.4269662921348316e-05,
      "loss": 2.5226,
      "step": 1369
    },
    {
      "epoch": 152.22222222222223,
      "grad_norm": 5.11215877532959,
      "learning_rate": 2.4213483146067418e-05,
      "loss": 2.5429,
      "step": 1370
    },
    {
      "epoch": 152.33333333333334,
      "grad_norm": 0.4970577657222748,
      "learning_rate": 2.415730337078652e-05,
      "loss": 2.5159,
      "step": 1371
    },
    {
      "epoch": 152.44444444444446,
      "grad_norm": 0.46891653537750244,
      "learning_rate": 2.410112359550562e-05,
      "loss": 2.4631,
      "step": 1372
    },
    {
      "epoch": 152.55555555555554,
      "grad_norm": 3.3292789459228516,
      "learning_rate": 2.404494382022472e-05,
      "loss": 1.2252,
      "step": 1373
    },
    {
      "epoch": 152.66666666666666,
      "grad_norm": 2.9320685863494873,
      "learning_rate": 2.398876404494382e-05,
      "loss": 2.5244,
      "step": 1374
    },
    {
      "epoch": 152.77777777777777,
      "grad_norm": 3.5099048614501953,
      "learning_rate": 2.393258426966292e-05,
      "loss": 2.6173,
      "step": 1375
    },
    {
      "epoch": 152.88888888888889,
      "grad_norm": 1.551513910293579,
      "learning_rate": 2.3876404494382023e-05,
      "loss": 2.4995,
      "step": 1376
    },
    {
      "epoch": 153.0,
      "grad_norm": 0.6115351319313049,
      "learning_rate": 2.3820224719101125e-05,
      "loss": 2.5001,
      "step": 1377
    },
    {
      "epoch": 153.11111111111111,
      "grad_norm": 0.7605888843536377,
      "learning_rate": 2.3764044943820226e-05,
      "loss": 2.5956,
      "step": 1378
    },
    {
      "epoch": 153.22222222222223,
      "grad_norm": 0.5843494534492493,
      "learning_rate": 2.3707865168539327e-05,
      "loss": 2.5583,
      "step": 1379
    },
    {
      "epoch": 153.33333333333334,
      "grad_norm": 3.4741241931915283,
      "learning_rate": 2.3651685393258425e-05,
      "loss": 2.5417,
      "step": 1380
    },
    {
      "epoch": 153.44444444444446,
      "grad_norm": 1.2709001302719116,
      "learning_rate": 2.359550561797753e-05,
      "loss": 2.5663,
      "step": 1381
    },
    {
      "epoch": 153.55555555555554,
      "grad_norm": 0.6050911545753479,
      "learning_rate": 2.353932584269663e-05,
      "loss": 2.4874,
      "step": 1382
    },
    {
      "epoch": 153.66666666666666,
      "grad_norm": 0.5240396857261658,
      "learning_rate": 2.3483146067415733e-05,
      "loss": 2.4983,
      "step": 1383
    },
    {
      "epoch": 153.77777777777777,
      "grad_norm": 0.6302904486656189,
      "learning_rate": 2.3426966292134835e-05,
      "loss": 2.4993,
      "step": 1384
    },
    {
      "epoch": 153.88888888888889,
      "grad_norm": 0.6808223128318787,
      "learning_rate": 2.3370786516853936e-05,
      "loss": 2.4432,
      "step": 1385
    },
    {
      "epoch": 154.0,
      "grad_norm": 1.797210931777954,
      "learning_rate": 2.3314606741573034e-05,
      "loss": 1.1356,
      "step": 1386
    },
    {
      "epoch": 154.11111111111111,
      "grad_norm": 0.5417118072509766,
      "learning_rate": 2.3258426966292135e-05,
      "loss": 2.5218,
      "step": 1387
    },
    {
      "epoch": 154.22222222222223,
      "grad_norm": 3.463045835494995,
      "learning_rate": 2.3202247191011237e-05,
      "loss": 2.6727,
      "step": 1388
    },
    {
      "epoch": 154.33333333333334,
      "grad_norm": 0.603766918182373,
      "learning_rate": 2.314606741573034e-05,
      "loss": 2.4728,
      "step": 1389
    },
    {
      "epoch": 154.44444444444446,
      "grad_norm": 2.513108253479004,
      "learning_rate": 2.308988764044944e-05,
      "loss": 0.9986,
      "step": 1390
    },
    {
      "epoch": 154.55555555555554,
      "grad_norm": 1.303702473640442,
      "learning_rate": 2.303370786516854e-05,
      "loss": 2.4759,
      "step": 1391
    },
    {
      "epoch": 154.66666666666666,
      "grad_norm": 1.8602958917617798,
      "learning_rate": 2.2977528089887643e-05,
      "loss": 2.5278,
      "step": 1392
    },
    {
      "epoch": 154.77777777777777,
      "grad_norm": 4.445986747741699,
      "learning_rate": 2.292134831460674e-05,
      "loss": 2.6569,
      "step": 1393
    },
    {
      "epoch": 154.88888888888889,
      "grad_norm": 0.8364962339401245,
      "learning_rate": 2.2865168539325842e-05,
      "loss": 2.5856,
      "step": 1394
    },
    {
      "epoch": 155.0,
      "grad_norm": 0.43063443899154663,
      "learning_rate": 2.2808988764044944e-05,
      "loss": 2.4756,
      "step": 1395
    },
    {
      "epoch": 155.11111111111111,
      "grad_norm": 0.7071862816810608,
      "learning_rate": 2.2752808988764045e-05,
      "loss": 2.4552,
      "step": 1396
    },
    {
      "epoch": 155.22222222222223,
      "grad_norm": 0.5392018556594849,
      "learning_rate": 2.2696629213483146e-05,
      "loss": 2.4377,
      "step": 1397
    },
    {
      "epoch": 155.33333333333334,
      "grad_norm": 3.5753049850463867,
      "learning_rate": 2.2640449438202248e-05,
      "loss": 2.6188,
      "step": 1398
    },
    {
      "epoch": 155.44444444444446,
      "grad_norm": 0.48452726006507874,
      "learning_rate": 2.258426966292135e-05,
      "loss": 2.5131,
      "step": 1399
    },
    {
      "epoch": 155.55555555555554,
      "grad_norm": 0.5886984467506409,
      "learning_rate": 2.252808988764045e-05,
      "loss": 2.5004,
      "step": 1400
    },
    {
      "epoch": 155.66666666666666,
      "grad_norm": 0.5246302485466003,
      "learning_rate": 2.2471910112359552e-05,
      "loss": 2.4388,
      "step": 1401
    },
    {
      "epoch": 155.77777777777777,
      "grad_norm": 10.134716033935547,
      "learning_rate": 2.2415730337078654e-05,
      "loss": 2.6671,
      "step": 1402
    },
    {
      "epoch": 155.88888888888889,
      "grad_norm": 1.6832990646362305,
      "learning_rate": 2.2359550561797755e-05,
      "loss": 0.789,
      "step": 1403
    },
    {
      "epoch": 156.0,
      "grad_norm": 0.6786897778511047,
      "learning_rate": 2.2303370786516856e-05,
      "loss": 2.5123,
      "step": 1404
    },
    {
      "epoch": 156.11111111111111,
      "grad_norm": 1.9589637517929077,
      "learning_rate": 2.2247191011235958e-05,
      "loss": 0.944,
      "step": 1405
    },
    {
      "epoch": 156.22222222222223,
      "grad_norm": 0.7218514084815979,
      "learning_rate": 2.2191011235955056e-05,
      "loss": 2.5204,
      "step": 1406
    },
    {
      "epoch": 156.33333333333334,
      "grad_norm": 0.5067522525787354,
      "learning_rate": 2.2134831460674157e-05,
      "loss": 2.5118,
      "step": 1407
    },
    {
      "epoch": 156.44444444444446,
      "grad_norm": 0.8884571194648743,
      "learning_rate": 2.207865168539326e-05,
      "loss": 2.5104,
      "step": 1408
    },
    {
      "epoch": 156.55555555555554,
      "grad_norm": 2.8202435970306396,
      "learning_rate": 2.202247191011236e-05,
      "loss": 2.6595,
      "step": 1409
    },
    {
      "epoch": 156.66666666666666,
      "grad_norm": 1.849579095840454,
      "learning_rate": 2.196629213483146e-05,
      "loss": 2.5469,
      "step": 1410
    },
    {
      "epoch": 156.77777777777777,
      "grad_norm": 2.568603754043579,
      "learning_rate": 2.1910112359550563e-05,
      "loss": 2.5586,
      "step": 1411
    },
    {
      "epoch": 156.88888888888889,
      "grad_norm": 1.4109468460083008,
      "learning_rate": 2.1853932584269665e-05,
      "loss": 2.6174,
      "step": 1412
    },
    {
      "epoch": 157.0,
      "grad_norm": 0.6455388069152832,
      "learning_rate": 2.1797752808988763e-05,
      "loss": 2.4893,
      "step": 1413
    },
    {
      "epoch": 157.11111111111111,
      "grad_norm": 0.7303566932678223,
      "learning_rate": 2.1741573033707864e-05,
      "loss": 2.4704,
      "step": 1414
    },
    {
      "epoch": 157.22222222222223,
      "grad_norm": 0.8498337864875793,
      "learning_rate": 2.1685393258426965e-05,
      "loss": 2.6094,
      "step": 1415
    },
    {
      "epoch": 157.33333333333334,
      "grad_norm": 1.8427402973175049,
      "learning_rate": 2.1629213483146067e-05,
      "loss": 2.5096,
      "step": 1416
    },
    {
      "epoch": 157.44444444444446,
      "grad_norm": 6.171651840209961,
      "learning_rate": 2.157303370786517e-05,
      "loss": 2.7118,
      "step": 1417
    },
    {
      "epoch": 157.55555555555554,
      "grad_norm": 0.7199141383171082,
      "learning_rate": 2.1516853932584273e-05,
      "loss": 2.4982,
      "step": 1418
    },
    {
      "epoch": 157.66666666666666,
      "grad_norm": 0.6351877450942993,
      "learning_rate": 2.1460674157303375e-05,
      "loss": 2.4746,
      "step": 1419
    },
    {
      "epoch": 157.77777777777777,
      "grad_norm": 3.7582733631134033,
      "learning_rate": 2.1404494382022473e-05,
      "loss": 3.0179,
      "step": 1420
    },
    {
      "epoch": 157.88888888888889,
      "grad_norm": 2.2936902046203613,
      "learning_rate": 2.1348314606741574e-05,
      "loss": 0.9081,
      "step": 1421
    },
    {
      "epoch": 158.0,
      "grad_norm": 0.5126445889472961,
      "learning_rate": 2.1292134831460675e-05,
      "loss": 2.4672,
      "step": 1422
    },
    {
      "epoch": 158.11111111111111,
      "grad_norm": 2.0239531993865967,
      "learning_rate": 2.1235955056179777e-05,
      "loss": 2.5293,
      "step": 1423
    },
    {
      "epoch": 158.22222222222223,
      "grad_norm": 5.832435607910156,
      "learning_rate": 2.117977528089888e-05,
      "loss": 2.5891,
      "step": 1424
    },
    {
      "epoch": 158.33333333333334,
      "grad_norm": 1.9055850505828857,
      "learning_rate": 2.112359550561798e-05,
      "loss": 0.978,
      "step": 1425
    },
    {
      "epoch": 158.44444444444446,
      "grad_norm": 3.934408664703369,
      "learning_rate": 2.1067415730337078e-05,
      "loss": 2.5537,
      "step": 1426
    },
    {
      "epoch": 158.55555555555554,
      "grad_norm": 0.9916118383407593,
      "learning_rate": 2.101123595505618e-05,
      "loss": 2.5127,
      "step": 1427
    },
    {
      "epoch": 158.66666666666666,
      "grad_norm": 0.5576302409172058,
      "learning_rate": 2.095505617977528e-05,
      "loss": 2.5155,
      "step": 1428
    },
    {
      "epoch": 158.77777777777777,
      "grad_norm": 0.46585503220558167,
      "learning_rate": 2.0898876404494382e-05,
      "loss": 2.4455,
      "step": 1429
    },
    {
      "epoch": 158.88888888888889,
      "grad_norm": 0.6096718907356262,
      "learning_rate": 2.0842696629213484e-05,
      "loss": 2.5013,
      "step": 1430
    },
    {
      "epoch": 159.0,
      "grad_norm": 0.7891363501548767,
      "learning_rate": 2.0786516853932585e-05,
      "loss": 2.4979,
      "step": 1431
    },
    {
      "epoch": 159.11111111111111,
      "grad_norm": 1.0249838829040527,
      "learning_rate": 2.0730337078651686e-05,
      "loss": 2.5452,
      "step": 1432
    },
    {
      "epoch": 159.22222222222223,
      "grad_norm": 1.975898027420044,
      "learning_rate": 2.0674157303370788e-05,
      "loss": 1.0646,
      "step": 1433
    },
    {
      "epoch": 159.33333333333334,
      "grad_norm": 1.5102934837341309,
      "learning_rate": 2.061797752808989e-05,
      "loss": 2.5111,
      "step": 1434
    },
    {
      "epoch": 159.44444444444446,
      "grad_norm": 0.4857742190361023,
      "learning_rate": 2.056179775280899e-05,
      "loss": 2.474,
      "step": 1435
    },
    {
      "epoch": 159.55555555555554,
      "grad_norm": 3.9677069187164307,
      "learning_rate": 2.0505617977528092e-05,
      "loss": 2.6126,
      "step": 1436
    },
    {
      "epoch": 159.66666666666666,
      "grad_norm": 2.42397403717041,
      "learning_rate": 2.0449438202247194e-05,
      "loss": 2.5668,
      "step": 1437
    },
    {
      "epoch": 159.77777777777777,
      "grad_norm": 0.7325953841209412,
      "learning_rate": 2.0393258426966295e-05,
      "loss": 2.4573,
      "step": 1438
    },
    {
      "epoch": 159.88888888888889,
      "grad_norm": 1.669822096824646,
      "learning_rate": 2.0337078651685396e-05,
      "loss": 2.4989,
      "step": 1439
    },
    {
      "epoch": 160.0,
      "grad_norm": 0.6279271245002747,
      "learning_rate": 2.0280898876404494e-05,
      "loss": 2.5,
      "step": 1440
    },
    {
      "epoch": 160.11111111111111,
      "grad_norm": 2.2305188179016113,
      "learning_rate": 2.0224719101123596e-05,
      "loss": 2.6198,
      "step": 1441
    },
    {
      "epoch": 160.22222222222223,
      "grad_norm": 0.7101181745529175,
      "learning_rate": 2.0168539325842697e-05,
      "loss": 2.4549,
      "step": 1442
    },
    {
      "epoch": 160.33333333333334,
      "grad_norm": 0.6363927721977234,
      "learning_rate": 2.01123595505618e-05,
      "loss": 2.4143,
      "step": 1443
    },
    {
      "epoch": 160.44444444444446,
      "grad_norm": 1.7699692249298096,
      "learning_rate": 2.00561797752809e-05,
      "loss": 0.9499,
      "step": 1444
    },
    {
      "epoch": 160.55555555555554,
      "grad_norm": 0.669029712677002,
      "learning_rate": 2e-05,
      "loss": 2.4392,
      "step": 1445
    },
    {
      "epoch": 160.66666666666666,
      "grad_norm": 2.891702175140381,
      "learning_rate": 1.99438202247191e-05,
      "loss": 2.5446,
      "step": 1446
    },
    {
      "epoch": 160.77777777777777,
      "grad_norm": 0.6998870968818665,
      "learning_rate": 1.98876404494382e-05,
      "loss": 2.47,
      "step": 1447
    },
    {
      "epoch": 160.88888888888889,
      "grad_norm": 0.5981021523475647,
      "learning_rate": 1.9831460674157303e-05,
      "loss": 2.4732,
      "step": 1448
    },
    {
      "epoch": 161.0,
      "grad_norm": 0.6478576064109802,
      "learning_rate": 1.9775280898876404e-05,
      "loss": 2.5066,
      "step": 1449
    },
    {
      "epoch": 161.11111111111111,
      "grad_norm": 1.4510197639465332,
      "learning_rate": 1.9719101123595505e-05,
      "loss": 2.5067,
      "step": 1450
    },
    {
      "epoch": 161.22222222222223,
      "grad_norm": 0.5301709771156311,
      "learning_rate": 1.9662921348314607e-05,
      "loss": 2.4644,
      "step": 1451
    },
    {
      "epoch": 161.33333333333334,
      "grad_norm": 2.6959762573242188,
      "learning_rate": 1.960674157303371e-05,
      "loss": 2.5514,
      "step": 1452
    },
    {
      "epoch": 161.44444444444446,
      "grad_norm": 1.0063351392745972,
      "learning_rate": 1.955056179775281e-05,
      "loss": 2.4356,
      "step": 1453
    },
    {
      "epoch": 161.55555555555554,
      "grad_norm": 1.572582483291626,
      "learning_rate": 1.949438202247191e-05,
      "loss": 2.5339,
      "step": 1454
    },
    {
      "epoch": 161.66666666666666,
      "grad_norm": 0.798433780670166,
      "learning_rate": 1.9438202247191013e-05,
      "loss": 2.508,
      "step": 1455
    },
    {
      "epoch": 161.77777777777777,
      "grad_norm": 2.980558156967163,
      "learning_rate": 1.9382022471910114e-05,
      "loss": 2.624,
      "step": 1456
    },
    {
      "epoch": 161.88888888888889,
      "grad_norm": 4.072150707244873,
      "learning_rate": 1.9325842696629215e-05,
      "loss": 2.5277,
      "step": 1457
    },
    {
      "epoch": 162.0,
      "grad_norm": 1.5633724927902222,
      "learning_rate": 1.9269662921348317e-05,
      "loss": 0.6999,
      "step": 1458
    },
    {
      "epoch": 162.11111111111111,
      "grad_norm": 3.5509097576141357,
      "learning_rate": 1.9213483146067415e-05,
      "loss": 2.7226,
      "step": 1459
    },
    {
      "epoch": 162.22222222222223,
      "grad_norm": 3.4520912170410156,
      "learning_rate": 1.9157303370786516e-05,
      "loss": 2.6504,
      "step": 1460
    },
    {
      "epoch": 162.33333333333334,
      "grad_norm": 0.5099198818206787,
      "learning_rate": 1.9101123595505618e-05,
      "loss": 2.451,
      "step": 1461
    },
    {
      "epoch": 162.44444444444446,
      "grad_norm": 0.46330153942108154,
      "learning_rate": 1.904494382022472e-05,
      "loss": 2.4337,
      "step": 1462
    },
    {
      "epoch": 162.55555555555554,
      "grad_norm": 0.9260491132736206,
      "learning_rate": 1.898876404494382e-05,
      "loss": 2.4379,
      "step": 1463
    },
    {
      "epoch": 162.66666666666666,
      "grad_norm": 4.081838130950928,
      "learning_rate": 1.8932584269662922e-05,
      "loss": 2.5504,
      "step": 1464
    },
    {
      "epoch": 162.77777777777777,
      "grad_norm": 2.3034722805023193,
      "learning_rate": 1.8876404494382024e-05,
      "loss": 2.5512,
      "step": 1465
    },
    {
      "epoch": 162.88888888888889,
      "grad_norm": 1.4942797422409058,
      "learning_rate": 1.8820224719101125e-05,
      "loss": 0.7974,
      "step": 1466
    },
    {
      "epoch": 163.0,
      "grad_norm": 0.6551836133003235,
      "learning_rate": 1.8764044943820226e-05,
      "loss": 2.4576,
      "step": 1467
    },
    {
      "epoch": 163.11111111111111,
      "grad_norm": 1.4936954975128174,
      "learning_rate": 1.8707865168539328e-05,
      "loss": 0.8923,
      "step": 1468
    },
    {
      "epoch": 163.22222222222223,
      "grad_norm": 0.8221333622932434,
      "learning_rate": 1.865168539325843e-05,
      "loss": 2.579,
      "step": 1469
    },
    {
      "epoch": 163.33333333333334,
      "grad_norm": 0.587325930595398,
      "learning_rate": 1.859550561797753e-05,
      "loss": 2.4919,
      "step": 1470
    },
    {
      "epoch": 163.44444444444446,
      "grad_norm": 0.7541699409484863,
      "learning_rate": 1.8539325842696632e-05,
      "loss": 2.4625,
      "step": 1471
    },
    {
      "epoch": 163.55555555555554,
      "grad_norm": 0.9183405041694641,
      "learning_rate": 1.8483146067415734e-05,
      "loss": 2.5463,
      "step": 1472
    },
    {
      "epoch": 163.66666666666666,
      "grad_norm": 1.72849440574646,
      "learning_rate": 1.842696629213483e-05,
      "loss": 2.546,
      "step": 1473
    },
    {
      "epoch": 163.77777777777777,
      "grad_norm": 0.6730735301971436,
      "learning_rate": 1.8370786516853933e-05,
      "loss": 2.5182,
      "step": 1474
    },
    {
      "epoch": 163.88888888888889,
      "grad_norm": 0.7657908201217651,
      "learning_rate": 1.8314606741573034e-05,
      "loss": 2.5152,
      "step": 1475
    },
    {
      "epoch": 164.0,
      "grad_norm": 0.6709287166595459,
      "learning_rate": 1.8258426966292136e-05,
      "loss": 2.5122,
      "step": 1476
    },
    {
      "epoch": 164.11111111111111,
      "grad_norm": 0.6345697641372681,
      "learning_rate": 1.8202247191011237e-05,
      "loss": 2.505,
      "step": 1477
    },
    {
      "epoch": 164.22222222222223,
      "grad_norm": 0.5636401176452637,
      "learning_rate": 1.814606741573034e-05,
      "loss": 2.4655,
      "step": 1478
    },
    {
      "epoch": 164.33333333333334,
      "grad_norm": 0.4114125072956085,
      "learning_rate": 1.8089887640449437e-05,
      "loss": 2.4395,
      "step": 1479
    },
    {
      "epoch": 164.44444444444446,
      "grad_norm": 2.201955556869507,
      "learning_rate": 1.8033707865168538e-05,
      "loss": 0.9364,
      "step": 1480
    },
    {
      "epoch": 164.55555555555554,
      "grad_norm": 1.128419280052185,
      "learning_rate": 1.797752808988764e-05,
      "loss": 2.5115,
      "step": 1481
    },
    {
      "epoch": 164.66666666666666,
      "grad_norm": 4.829741477966309,
      "learning_rate": 1.792134831460674e-05,
      "loss": 2.5251,
      "step": 1482
    },
    {
      "epoch": 164.77777777777777,
      "grad_norm": 5.699843406677246,
      "learning_rate": 1.7865168539325843e-05,
      "loss": 2.6198,
      "step": 1483
    },
    {
      "epoch": 164.88888888888889,
      "grad_norm": 0.7999274730682373,
      "learning_rate": 1.7808988764044944e-05,
      "loss": 2.4497,
      "step": 1484
    },
    {
      "epoch": 165.0,
      "grad_norm": 1.2804675102233887,
      "learning_rate": 1.7752808988764045e-05,
      "loss": 2.5084,
      "step": 1485
    },
    {
      "epoch": 165.11111111111111,
      "grad_norm": 0.4840632677078247,
      "learning_rate": 1.7696629213483147e-05,
      "loss": 2.4563,
      "step": 1486
    },
    {
      "epoch": 165.22222222222223,
      "grad_norm": 1.674721598625183,
      "learning_rate": 1.7640449438202248e-05,
      "loss": 2.5002,
      "step": 1487
    },
    {
      "epoch": 165.33333333333334,
      "grad_norm": 0.5466835498809814,
      "learning_rate": 1.758426966292135e-05,
      "loss": 2.4098,
      "step": 1488
    },
    {
      "epoch": 165.44444444444446,
      "grad_norm": 3.143425703048706,
      "learning_rate": 1.752808988764045e-05,
      "loss": 2.505,
      "step": 1489
    },
    {
      "epoch": 165.55555555555554,
      "grad_norm": 0.7650307416915894,
      "learning_rate": 1.7471910112359553e-05,
      "loss": 2.4582,
      "step": 1490
    },
    {
      "epoch": 165.66666666666666,
      "grad_norm": 1.3465120792388916,
      "learning_rate": 1.7415730337078654e-05,
      "loss": 2.4791,
      "step": 1491
    },
    {
      "epoch": 165.77777777777777,
      "grad_norm": 0.6417565941810608,
      "learning_rate": 1.7359550561797755e-05,
      "loss": 2.4909,
      "step": 1492
    },
    {
      "epoch": 165.88888888888889,
      "grad_norm": 2.9209952354431152,
      "learning_rate": 1.7303370786516853e-05,
      "loss": 0.8806,
      "step": 1493
    },
    {
      "epoch": 166.0,
      "grad_norm": 12.610395431518555,
      "learning_rate": 1.7247191011235955e-05,
      "loss": 2.6621,
      "step": 1494
    },
    {
      "epoch": 166.11111111111111,
      "grad_norm": 3.6117796897888184,
      "learning_rate": 1.7191011235955056e-05,
      "loss": 2.537,
      "step": 1495
    },
    {
      "epoch": 166.22222222222223,
      "grad_norm": 1.7069017887115479,
      "learning_rate": 1.7134831460674158e-05,
      "loss": 0.8867,
      "step": 1496
    },
    {
      "epoch": 166.33333333333334,
      "grad_norm": 0.6028741598129272,
      "learning_rate": 1.707865168539326e-05,
      "loss": 2.4308,
      "step": 1497
    },
    {
      "epoch": 166.44444444444446,
      "grad_norm": 0.7765946388244629,
      "learning_rate": 1.702247191011236e-05,
      "loss": 2.4144,
      "step": 1498
    },
    {
      "epoch": 166.55555555555554,
      "grad_norm": 0.46517062187194824,
      "learning_rate": 1.696629213483146e-05,
      "loss": 2.4028,
      "step": 1499
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 5.745325088500977,
      "learning_rate": 1.691011235955056e-05,
      "loss": 2.7923,
      "step": 1500
    },
    {
      "epoch": 166.77777777777777,
      "grad_norm": 0.8801541328430176,
      "learning_rate": 1.6853932584269665e-05,
      "loss": 2.3852,
      "step": 1501
    },
    {
      "epoch": 166.88888888888889,
      "grad_norm": 4.431697845458984,
      "learning_rate": 1.6797752808988766e-05,
      "loss": 2.5849,
      "step": 1502
    },
    {
      "epoch": 167.0,
      "grad_norm": 1.876097559928894,
      "learning_rate": 1.6741573033707868e-05,
      "loss": 2.4805,
      "step": 1503
    },
    {
      "epoch": 167.11111111111111,
      "grad_norm": 0.46710285544395447,
      "learning_rate": 1.668539325842697e-05,
      "loss": 2.4433,
      "step": 1504
    },
    {
      "epoch": 167.22222222222223,
      "grad_norm": 0.700307309627533,
      "learning_rate": 1.662921348314607e-05,
      "loss": 2.4434,
      "step": 1505
    },
    {
      "epoch": 167.33333333333334,
      "grad_norm": 7.008300304412842,
      "learning_rate": 1.657303370786517e-05,
      "loss": 3.1036,
      "step": 1506
    },
    {
      "epoch": 167.44444444444446,
      "grad_norm": 6.356454372406006,
      "learning_rate": 1.651685393258427e-05,
      "loss": 2.6908,
      "step": 1507
    },
    {
      "epoch": 167.55555555555554,
      "grad_norm": 1.9017759561538696,
      "learning_rate": 1.646067415730337e-05,
      "loss": 0.9353,
      "step": 1508
    },
    {
      "epoch": 167.66666666666666,
      "grad_norm": 0.4502837359905243,
      "learning_rate": 1.6404494382022473e-05,
      "loss": 2.4003,
      "step": 1509
    },
    {
      "epoch": 167.77777777777777,
      "grad_norm": 0.5966396927833557,
      "learning_rate": 1.6348314606741574e-05,
      "loss": 2.5216,
      "step": 1510
    },
    {
      "epoch": 167.88888888888889,
      "grad_norm": 0.8150554895401001,
      "learning_rate": 1.6292134831460676e-05,
      "loss": 2.4264,
      "step": 1511
    },
    {
      "epoch": 168.0,
      "grad_norm": 0.6298760175704956,
      "learning_rate": 1.6235955056179777e-05,
      "loss": 2.4894,
      "step": 1512
    },
    {
      "epoch": 168.11111111111111,
      "grad_norm": 2.391003370285034,
      "learning_rate": 1.6179775280898875e-05,
      "loss": 1.059,
      "step": 1513
    },
    {
      "epoch": 168.22222222222223,
      "grad_norm": 0.5871722102165222,
      "learning_rate": 1.6123595505617977e-05,
      "loss": 2.4571,
      "step": 1514
    },
    {
      "epoch": 168.33333333333334,
      "grad_norm": 5.077696800231934,
      "learning_rate": 1.6067415730337078e-05,
      "loss": 2.6305,
      "step": 1515
    },
    {
      "epoch": 168.44444444444446,
      "grad_norm": 1.2113274335861206,
      "learning_rate": 1.601123595505618e-05,
      "loss": 2.4413,
      "step": 1516
    },
    {
      "epoch": 168.55555555555554,
      "grad_norm": 0.814215898513794,
      "learning_rate": 1.595505617977528e-05,
      "loss": 2.4853,
      "step": 1517
    },
    {
      "epoch": 168.66666666666666,
      "grad_norm": 0.50284343957901,
      "learning_rate": 1.5898876404494383e-05,
      "loss": 2.4794,
      "step": 1518
    },
    {
      "epoch": 168.77777777777777,
      "grad_norm": 2.234572172164917,
      "learning_rate": 1.5842696629213484e-05,
      "loss": 2.5039,
      "step": 1519
    },
    {
      "epoch": 168.88888888888889,
      "grad_norm": 3.877476215362549,
      "learning_rate": 1.5786516853932585e-05,
      "loss": 2.6741,
      "step": 1520
    },
    {
      "epoch": 169.0,
      "grad_norm": 6.134846210479736,
      "learning_rate": 1.5730337078651687e-05,
      "loss": 2.6705,
      "step": 1521
    },
    {
      "epoch": 169.11111111111111,
      "grad_norm": 5.081109046936035,
      "learning_rate": 1.5674157303370788e-05,
      "loss": 2.6623,
      "step": 1522
    },
    {
      "epoch": 169.22222222222223,
      "grad_norm": 0.6169549822807312,
      "learning_rate": 1.561797752808989e-05,
      "loss": 2.4732,
      "step": 1523
    },
    {
      "epoch": 169.33333333333334,
      "grad_norm": 2.79971981048584,
      "learning_rate": 1.556179775280899e-05,
      "loss": 1.1558,
      "step": 1524
    },
    {
      "epoch": 169.44444444444446,
      "grad_norm": 2.6660211086273193,
      "learning_rate": 1.5505617977528093e-05,
      "loss": 2.6552,
      "step": 1525
    },
    {
      "epoch": 169.55555555555554,
      "grad_norm": 1.9130727052688599,
      "learning_rate": 1.544943820224719e-05,
      "loss": 2.4202,
      "step": 1526
    },
    {
      "epoch": 169.66666666666666,
      "grad_norm": 1.0406501293182373,
      "learning_rate": 1.5393258426966292e-05,
      "loss": 2.4936,
      "step": 1527
    },
    {
      "epoch": 169.77777777777777,
      "grad_norm": 0.44437211751937866,
      "learning_rate": 1.5337078651685393e-05,
      "loss": 2.3847,
      "step": 1528
    },
    {
      "epoch": 169.88888888888889,
      "grad_norm": 2.311483144760132,
      "learning_rate": 1.5280898876404495e-05,
      "loss": 2.5179,
      "step": 1529
    },
    {
      "epoch": 170.0,
      "grad_norm": 0.5332728028297424,
      "learning_rate": 1.5224719101123596e-05,
      "loss": 2.4252,
      "step": 1530
    },
    {
      "epoch": 170.11111111111111,
      "grad_norm": 0.5388621687889099,
      "learning_rate": 1.5168539325842698e-05,
      "loss": 2.447,
      "step": 1531
    },
    {
      "epoch": 170.22222222222223,
      "grad_norm": 1.2993104457855225,
      "learning_rate": 1.51123595505618e-05,
      "loss": 2.4738,
      "step": 1532
    },
    {
      "epoch": 170.33333333333334,
      "grad_norm": 0.5639937520027161,
      "learning_rate": 1.5056179775280899e-05,
      "loss": 2.4354,
      "step": 1533
    },
    {
      "epoch": 170.44444444444446,
      "grad_norm": 3.3819527626037598,
      "learning_rate": 1.5e-05,
      "loss": 2.4701,
      "step": 1534
    },
    {
      "epoch": 170.55555555555554,
      "grad_norm": 0.8658133745193481,
      "learning_rate": 1.4943820224719102e-05,
      "loss": 2.4536,
      "step": 1535
    },
    {
      "epoch": 170.66666666666666,
      "grad_norm": 2.618077278137207,
      "learning_rate": 1.4887640449438203e-05,
      "loss": 2.5592,
      "step": 1536
    },
    {
      "epoch": 170.77777777777777,
      "grad_norm": 1.742881178855896,
      "learning_rate": 1.4831460674157305e-05,
      "loss": 0.9404,
      "step": 1537
    },
    {
      "epoch": 170.88888888888889,
      "grad_norm": 0.5313865542411804,
      "learning_rate": 1.4775280898876406e-05,
      "loss": 2.4569,
      "step": 1538
    },
    {
      "epoch": 171.0,
      "grad_norm": 0.698103129863739,
      "learning_rate": 1.4719101123595506e-05,
      "loss": 2.413,
      "step": 1539
    },
    {
      "epoch": 171.11111111111111,
      "grad_norm": 2.678807020187378,
      "learning_rate": 1.4662921348314607e-05,
      "loss": 1.0712,
      "step": 1540
    },
    {
      "epoch": 171.22222222222223,
      "grad_norm": 0.5271244049072266,
      "learning_rate": 1.4606741573033709e-05,
      "loss": 2.4643,
      "step": 1541
    },
    {
      "epoch": 171.33333333333334,
      "grad_norm": 2.345869779586792,
      "learning_rate": 1.455056179775281e-05,
      "loss": 2.4591,
      "step": 1542
    },
    {
      "epoch": 171.44444444444446,
      "grad_norm": 0.6332224607467651,
      "learning_rate": 1.4494382022471912e-05,
      "loss": 2.4248,
      "step": 1543
    },
    {
      "epoch": 171.55555555555554,
      "grad_norm": 5.3109517097473145,
      "learning_rate": 1.4438202247191013e-05,
      "loss": 2.59,
      "step": 1544
    },
    {
      "epoch": 171.66666666666666,
      "grad_norm": 2.6758744716644287,
      "learning_rate": 1.4382022471910114e-05,
      "loss": 2.4911,
      "step": 1545
    },
    {
      "epoch": 171.77777777777777,
      "grad_norm": 0.714974045753479,
      "learning_rate": 1.4325842696629212e-05,
      "loss": 2.463,
      "step": 1546
    },
    {
      "epoch": 171.88888888888889,
      "grad_norm": 0.5435816645622253,
      "learning_rate": 1.4269662921348314e-05,
      "loss": 2.4269,
      "step": 1547
    },
    {
      "epoch": 172.0,
      "grad_norm": 0.6616809368133545,
      "learning_rate": 1.4213483146067415e-05,
      "loss": 2.4887,
      "step": 1548
    },
    {
      "epoch": 172.11111111111111,
      "grad_norm": 0.537102460861206,
      "learning_rate": 1.4157303370786518e-05,
      "loss": 2.4042,
      "step": 1549
    },
    {
      "epoch": 172.22222222222223,
      "grad_norm": 0.5496083498001099,
      "learning_rate": 1.410112359550562e-05,
      "loss": 2.4604,
      "step": 1550
    },
    {
      "epoch": 172.33333333333334,
      "grad_norm": 0.4685165584087372,
      "learning_rate": 1.4044943820224721e-05,
      "loss": 2.4016,
      "step": 1551
    },
    {
      "epoch": 172.44444444444446,
      "grad_norm": 2.8790194988250732,
      "learning_rate": 1.398876404494382e-05,
      "loss": 0.8472,
      "step": 1552
    },
    {
      "epoch": 172.55555555555554,
      "grad_norm": 5.265514373779297,
      "learning_rate": 1.393258426966292e-05,
      "loss": 2.6085,
      "step": 1553
    },
    {
      "epoch": 172.66666666666666,
      "grad_norm": 0.7754900455474854,
      "learning_rate": 1.3876404494382022e-05,
      "loss": 2.432,
      "step": 1554
    },
    {
      "epoch": 172.77777777777777,
      "grad_norm": 3.819502592086792,
      "learning_rate": 1.3820224719101124e-05,
      "loss": 2.4986,
      "step": 1555
    },
    {
      "epoch": 172.88888888888889,
      "grad_norm": 0.835281252861023,
      "learning_rate": 1.3764044943820225e-05,
      "loss": 2.4943,
      "step": 1556
    },
    {
      "epoch": 173.0,
      "grad_norm": 3.627229690551758,
      "learning_rate": 1.3707865168539327e-05,
      "loss": 2.5429,
      "step": 1557
    },
    {
      "epoch": 173.11111111111111,
      "grad_norm": 1.2826333045959473,
      "learning_rate": 1.3651685393258428e-05,
      "loss": 2.4237,
      "step": 1558
    },
    {
      "epoch": 173.22222222222223,
      "grad_norm": 0.8872668147087097,
      "learning_rate": 1.3595505617977528e-05,
      "loss": 2.4901,
      "step": 1559
    },
    {
      "epoch": 173.33333333333334,
      "grad_norm": 5.952346324920654,
      "learning_rate": 1.353932584269663e-05,
      "loss": 2.5646,
      "step": 1560
    },
    {
      "epoch": 173.44444444444446,
      "grad_norm": 4.18789529800415,
      "learning_rate": 1.348314606741573e-05,
      "loss": 2.5213,
      "step": 1561
    },
    {
      "epoch": 173.55555555555554,
      "grad_norm": 3.133570909500122,
      "learning_rate": 1.3426966292134832e-05,
      "loss": 2.5097,
      "step": 1562
    },
    {
      "epoch": 173.66666666666666,
      "grad_norm": 1.8911536931991577,
      "learning_rate": 1.3370786516853933e-05,
      "loss": 0.8301,
      "step": 1563
    },
    {
      "epoch": 173.77777777777777,
      "grad_norm": 1.6458202600479126,
      "learning_rate": 1.3314606741573035e-05,
      "loss": 2.4799,
      "step": 1564
    },
    {
      "epoch": 173.88888888888889,
      "grad_norm": 0.6467185020446777,
      "learning_rate": 1.3258426966292136e-05,
      "loss": 2.4445,
      "step": 1565
    },
    {
      "epoch": 174.0,
      "grad_norm": 2.3140344619750977,
      "learning_rate": 1.3202247191011236e-05,
      "loss": 2.5256,
      "step": 1566
    },
    {
      "epoch": 174.11111111111111,
      "grad_norm": 4.217545032501221,
      "learning_rate": 1.3146067415730338e-05,
      "loss": 2.5364,
      "step": 1567
    },
    {
      "epoch": 174.22222222222223,
      "grad_norm": 1.5308936834335327,
      "learning_rate": 1.3089887640449439e-05,
      "loss": 0.7209,
      "step": 1568
    },
    {
      "epoch": 174.33333333333334,
      "grad_norm": 0.5337586998939514,
      "learning_rate": 1.303370786516854e-05,
      "loss": 2.4311,
      "step": 1569
    },
    {
      "epoch": 174.44444444444446,
      "grad_norm": 0.6084418892860413,
      "learning_rate": 1.2977528089887642e-05,
      "loss": 2.442,
      "step": 1570
    },
    {
      "epoch": 174.55555555555554,
      "grad_norm": 1.5203711986541748,
      "learning_rate": 1.2921348314606743e-05,
      "loss": 2.4366,
      "step": 1571
    },
    {
      "epoch": 174.66666666666666,
      "grad_norm": 0.8776113390922546,
      "learning_rate": 1.2865168539325841e-05,
      "loss": 2.5489,
      "step": 1572
    },
    {
      "epoch": 174.77777777777777,
      "grad_norm": 2.9822041988372803,
      "learning_rate": 1.2808988764044943e-05,
      "loss": 2.5136,
      "step": 1573
    },
    {
      "epoch": 174.88888888888889,
      "grad_norm": 5.047079086303711,
      "learning_rate": 1.2752808988764046e-05,
      "loss": 2.4437,
      "step": 1574
    },
    {
      "epoch": 175.0,
      "grad_norm": 0.7369784712791443,
      "learning_rate": 1.2696629213483147e-05,
      "loss": 2.4892,
      "step": 1575
    },
    {
      "epoch": 175.11111111111111,
      "grad_norm": 1.4108152389526367,
      "learning_rate": 1.2640449438202249e-05,
      "loss": 2.468,
      "step": 1576
    },
    {
      "epoch": 175.22222222222223,
      "grad_norm": 1.869208812713623,
      "learning_rate": 1.258426966292135e-05,
      "loss": 0.7301,
      "step": 1577
    },
    {
      "epoch": 175.33333333333334,
      "grad_norm": 0.4372696280479431,
      "learning_rate": 1.2528089887640452e-05,
      "loss": 2.4201,
      "step": 1578
    },
    {
      "epoch": 175.44444444444446,
      "grad_norm": 4.152819633483887,
      "learning_rate": 1.2471910112359551e-05,
      "loss": 2.5412,
      "step": 1579
    },
    {
      "epoch": 175.55555555555554,
      "grad_norm": 2.59456205368042,
      "learning_rate": 1.2415730337078653e-05,
      "loss": 2.5442,
      "step": 1580
    },
    {
      "epoch": 175.66666666666666,
      "grad_norm": 2.203704357147217,
      "learning_rate": 1.2359550561797752e-05,
      "loss": 2.4923,
      "step": 1581
    },
    {
      "epoch": 175.77777777777777,
      "grad_norm": 0.5228234529495239,
      "learning_rate": 1.2303370786516854e-05,
      "loss": 2.4428,
      "step": 1582
    },
    {
      "epoch": 175.88888888888889,
      "grad_norm": 0.7744448781013489,
      "learning_rate": 1.2247191011235955e-05,
      "loss": 2.4486,
      "step": 1583
    },
    {
      "epoch": 176.0,
      "grad_norm": 4.059813022613525,
      "learning_rate": 1.2191011235955057e-05,
      "loss": 2.5575,
      "step": 1584
    },
    {
      "epoch": 176.11111111111111,
      "grad_norm": 0.4330083131790161,
      "learning_rate": 1.2134831460674158e-05,
      "loss": 2.4716,
      "step": 1585
    },
    {
      "epoch": 176.22222222222223,
      "grad_norm": 0.5295332074165344,
      "learning_rate": 1.207865168539326e-05,
      "loss": 2.4106,
      "step": 1586
    },
    {
      "epoch": 176.33333333333334,
      "grad_norm": 0.6810052394866943,
      "learning_rate": 1.202247191011236e-05,
      "loss": 2.4536,
      "step": 1587
    },
    {
      "epoch": 176.44444444444446,
      "grad_norm": 0.5478250980377197,
      "learning_rate": 1.196629213483146e-05,
      "loss": 2.42,
      "step": 1588
    },
    {
      "epoch": 176.55555555555554,
      "grad_norm": 3.9413514137268066,
      "learning_rate": 1.1910112359550562e-05,
      "loss": 2.5736,
      "step": 1589
    },
    {
      "epoch": 176.66666666666666,
      "grad_norm": 0.663154661655426,
      "learning_rate": 1.1853932584269664e-05,
      "loss": 2.4411,
      "step": 1590
    },
    {
      "epoch": 176.77777777777777,
      "grad_norm": 1.7410916090011597,
      "learning_rate": 1.1797752808988765e-05,
      "loss": 0.7927,
      "step": 1591
    },
    {
      "epoch": 176.88888888888889,
      "grad_norm": 0.5637841820716858,
      "learning_rate": 1.1741573033707867e-05,
      "loss": 2.3827,
      "step": 1592
    },
    {
      "epoch": 177.0,
      "grad_norm": 0.4662097096443176,
      "learning_rate": 1.1685393258426968e-05,
      "loss": 2.3887,
      "step": 1593
    },
    {
      "epoch": 177.11111111111111,
      "grad_norm": 1.3191397190093994,
      "learning_rate": 1.1629213483146068e-05,
      "loss": 2.4476,
      "step": 1594
    },
    {
      "epoch": 177.22222222222223,
      "grad_norm": 0.42923271656036377,
      "learning_rate": 1.157303370786517e-05,
      "loss": 2.4582,
      "step": 1595
    },
    {
      "epoch": 177.33333333333334,
      "grad_norm": 7.231290817260742,
      "learning_rate": 1.151685393258427e-05,
      "loss": 2.6742,
      "step": 1596
    },
    {
      "epoch": 177.44444444444446,
      "grad_norm": 3.75473952293396,
      "learning_rate": 1.146067415730337e-05,
      "loss": 2.698,
      "step": 1597
    },
    {
      "epoch": 177.55555555555554,
      "grad_norm": 0.7400302886962891,
      "learning_rate": 1.1404494382022472e-05,
      "loss": 2.4268,
      "step": 1598
    },
    {
      "epoch": 177.66666666666666,
      "grad_norm": 2.4927029609680176,
      "learning_rate": 1.1348314606741573e-05,
      "loss": 1.1567,
      "step": 1599
    },
    {
      "epoch": 177.77777777777777,
      "grad_norm": 0.5226837992668152,
      "learning_rate": 1.1292134831460675e-05,
      "loss": 2.3729,
      "step": 1600
    },
    {
      "epoch": 177.88888888888889,
      "grad_norm": 5.279519557952881,
      "learning_rate": 1.1235955056179776e-05,
      "loss": 2.702,
      "step": 1601
    },
    {
      "epoch": 178.0,
      "grad_norm": 1.075364351272583,
      "learning_rate": 1.1179775280898877e-05,
      "loss": 2.5867,
      "step": 1602
    },
    {
      "epoch": 178.11111111111111,
      "grad_norm": 0.5227722525596619,
      "learning_rate": 1.1123595505617979e-05,
      "loss": 2.4072,
      "step": 1603
    },
    {
      "epoch": 178.22222222222223,
      "grad_norm": 1.0701898336410522,
      "learning_rate": 1.1067415730337079e-05,
      "loss": 2.4895,
      "step": 1604
    },
    {
      "epoch": 178.33333333333334,
      "grad_norm": 1.4331152439117432,
      "learning_rate": 1.101123595505618e-05,
      "loss": 2.4445,
      "step": 1605
    },
    {
      "epoch": 178.44444444444446,
      "grad_norm": 4.335538864135742,
      "learning_rate": 1.0955056179775282e-05,
      "loss": 2.6322,
      "step": 1606
    },
    {
      "epoch": 178.55555555555554,
      "grad_norm": 3.3004796504974365,
      "learning_rate": 1.0898876404494381e-05,
      "loss": 2.5797,
      "step": 1607
    },
    {
      "epoch": 178.66666666666666,
      "grad_norm": 1.4729646444320679,
      "learning_rate": 1.0842696629213483e-05,
      "loss": 2.437,
      "step": 1608
    },
    {
      "epoch": 178.77777777777777,
      "grad_norm": 0.579413890838623,
      "learning_rate": 1.0786516853932586e-05,
      "loss": 2.4569,
      "step": 1609
    },
    {
      "epoch": 178.88888888888889,
      "grad_norm": 1.9696589708328247,
      "learning_rate": 1.0730337078651687e-05,
      "loss": 1.0765,
      "step": 1610
    },
    {
      "epoch": 179.0,
      "grad_norm": 2.0405383110046387,
      "learning_rate": 1.0674157303370787e-05,
      "loss": 2.4792,
      "step": 1611
    },
    {
      "epoch": 179.11111111111111,
      "grad_norm": 5.4529876708984375,
      "learning_rate": 1.0617977528089888e-05,
      "loss": 2.5375,
      "step": 1612
    },
    {
      "epoch": 179.22222222222223,
      "grad_norm": 0.518967866897583,
      "learning_rate": 1.056179775280899e-05,
      "loss": 2.4525,
      "step": 1613
    },
    {
      "epoch": 179.33333333333334,
      "grad_norm": 2.0893990993499756,
      "learning_rate": 1.050561797752809e-05,
      "loss": 2.4616,
      "step": 1614
    },
    {
      "epoch": 179.44444444444446,
      "grad_norm": 1.689863681793213,
      "learning_rate": 1.0449438202247191e-05,
      "loss": 0.9691,
      "step": 1615
    },
    {
      "epoch": 179.55555555555554,
      "grad_norm": 0.4418635368347168,
      "learning_rate": 1.0393258426966292e-05,
      "loss": 2.4513,
      "step": 1616
    },
    {
      "epoch": 179.66666666666666,
      "grad_norm": 0.5750389099121094,
      "learning_rate": 1.0337078651685394e-05,
      "loss": 2.4284,
      "step": 1617
    },
    {
      "epoch": 179.77777777777777,
      "grad_norm": 0.5385394096374512,
      "learning_rate": 1.0280898876404495e-05,
      "loss": 2.439,
      "step": 1618
    },
    {
      "epoch": 179.88888888888889,
      "grad_norm": 0.5940237641334534,
      "learning_rate": 1.0224719101123597e-05,
      "loss": 2.411,
      "step": 1619
    },
    {
      "epoch": 180.0,
      "grad_norm": 0.5414539575576782,
      "learning_rate": 1.0168539325842698e-05,
      "loss": 2.409,
      "step": 1620
    },
    {
      "epoch": 180.11111111111111,
      "grad_norm": 2.883242607116699,
      "learning_rate": 1.0112359550561798e-05,
      "loss": 2.5849,
      "step": 1621
    },
    {
      "epoch": 180.22222222222223,
      "grad_norm": 0.5981583595275879,
      "learning_rate": 1.00561797752809e-05,
      "loss": 2.4152,
      "step": 1622
    },
    {
      "epoch": 180.33333333333334,
      "grad_norm": 10.966286659240723,
      "learning_rate": 1e-05,
      "loss": 2.9268,
      "step": 1623
    },
    {
      "epoch": 180.44444444444446,
      "grad_norm": 0.552156388759613,
      "learning_rate": 9.9438202247191e-06,
      "loss": 2.4248,
      "step": 1624
    },
    {
      "epoch": 180.55555555555554,
      "grad_norm": 1.8155752420425415,
      "learning_rate": 9.887640449438202e-06,
      "loss": 2.4604,
      "step": 1625
    },
    {
      "epoch": 180.66666666666666,
      "grad_norm": 0.5461994409561157,
      "learning_rate": 9.831460674157303e-06,
      "loss": 2.42,
      "step": 1626
    },
    {
      "epoch": 180.77777777777777,
      "grad_norm": 3.3436996936798096,
      "learning_rate": 9.775280898876405e-06,
      "loss": 1.1581,
      "step": 1627
    },
    {
      "epoch": 180.88888888888889,
      "grad_norm": 4.291326999664307,
      "learning_rate": 9.719101123595506e-06,
      "loss": 2.7683,
      "step": 1628
    },
    {
      "epoch": 181.0,
      "grad_norm": 0.482940673828125,
      "learning_rate": 9.662921348314608e-06,
      "loss": 2.4007,
      "step": 1629
    },
    {
      "epoch": 181.11111111111111,
      "grad_norm": 11.638506889343262,
      "learning_rate": 9.606741573033707e-06,
      "loss": 2.8202,
      "step": 1630
    },
    {
      "epoch": 181.22222222222223,
      "grad_norm": 2.9566283226013184,
      "learning_rate": 9.550561797752809e-06,
      "loss": 2.7237,
      "step": 1631
    },
    {
      "epoch": 181.33333333333334,
      "grad_norm": 3.27191424369812,
      "learning_rate": 9.49438202247191e-06,
      "loss": 2.4919,
      "step": 1632
    },
    {
      "epoch": 181.44444444444446,
      "grad_norm": 0.7906396389007568,
      "learning_rate": 9.438202247191012e-06,
      "loss": 2.4353,
      "step": 1633
    },
    {
      "epoch": 181.55555555555554,
      "grad_norm": 4.236819267272949,
      "learning_rate": 9.382022471910113e-06,
      "loss": 2.4959,
      "step": 1634
    },
    {
      "epoch": 181.66666666666666,
      "grad_norm": 4.587798595428467,
      "learning_rate": 9.325842696629215e-06,
      "loss": 2.6017,
      "step": 1635
    },
    {
      "epoch": 181.77777777777777,
      "grad_norm": 2.490206480026245,
      "learning_rate": 9.269662921348316e-06,
      "loss": 1.0733,
      "step": 1636
    },
    {
      "epoch": 181.88888888888889,
      "grad_norm": 2.9222912788391113,
      "learning_rate": 9.213483146067416e-06,
      "loss": 2.5476,
      "step": 1637
    },
    {
      "epoch": 182.0,
      "grad_norm": 2.4333584308624268,
      "learning_rate": 9.157303370786517e-06,
      "loss": 2.5613,
      "step": 1638
    },
    {
      "epoch": 182.11111111111111,
      "grad_norm": 0.8890323638916016,
      "learning_rate": 9.101123595505619e-06,
      "loss": 2.5048,
      "step": 1639
    },
    {
      "epoch": 182.22222222222223,
      "grad_norm": 0.45992663502693176,
      "learning_rate": 9.044943820224718e-06,
      "loss": 2.4177,
      "step": 1640
    },
    {
      "epoch": 182.33333333333334,
      "grad_norm": 0.68330979347229,
      "learning_rate": 8.98876404494382e-06,
      "loss": 2.4297,
      "step": 1641
    },
    {
      "epoch": 182.44444444444446,
      "grad_norm": 0.535749614238739,
      "learning_rate": 8.932584269662921e-06,
      "loss": 2.4002,
      "step": 1642
    },
    {
      "epoch": 182.55555555555554,
      "grad_norm": 0.646872341632843,
      "learning_rate": 8.876404494382023e-06,
      "loss": 2.4573,
      "step": 1643
    },
    {
      "epoch": 182.66666666666666,
      "grad_norm": 0.5823289752006531,
      "learning_rate": 8.820224719101124e-06,
      "loss": 2.4197,
      "step": 1644
    },
    {
      "epoch": 182.77777777777777,
      "grad_norm": 9.949851036071777,
      "learning_rate": 8.764044943820226e-06,
      "loss": 1.2031,
      "step": 1645
    },
    {
      "epoch": 182.88888888888889,
      "grad_norm": 0.7419911026954651,
      "learning_rate": 8.707865168539327e-06,
      "loss": 2.4493,
      "step": 1646
    },
    {
      "epoch": 183.0,
      "grad_norm": 1.4384739398956299,
      "learning_rate": 8.651685393258427e-06,
      "loss": 2.5349,
      "step": 1647
    },
    {
      "epoch": 183.11111111111111,
      "grad_norm": 3.5908753871917725,
      "learning_rate": 8.595505617977528e-06,
      "loss": 0.8889,
      "step": 1648
    },
    {
      "epoch": 183.22222222222223,
      "grad_norm": 0.5816267132759094,
      "learning_rate": 8.53932584269663e-06,
      "loss": 2.4637,
      "step": 1649
    },
    {
      "epoch": 183.33333333333334,
      "grad_norm": 5.932009696960449,
      "learning_rate": 8.48314606741573e-06,
      "loss": 2.5222,
      "step": 1650
    },
    {
      "epoch": 183.44444444444446,
      "grad_norm": 0.9634274244308472,
      "learning_rate": 8.426966292134832e-06,
      "loss": 2.4978,
      "step": 1651
    },
    {
      "epoch": 183.55555555555554,
      "grad_norm": 4.670982837677002,
      "learning_rate": 8.370786516853934e-06,
      "loss": 2.499,
      "step": 1652
    },
    {
      "epoch": 183.66666666666666,
      "grad_norm": 2.1037955284118652,
      "learning_rate": 8.314606741573035e-06,
      "loss": 2.5434,
      "step": 1653
    },
    {
      "epoch": 183.77777777777777,
      "grad_norm": 0.5591908693313599,
      "learning_rate": 8.258426966292135e-06,
      "loss": 2.4546,
      "step": 1654
    },
    {
      "epoch": 183.88888888888889,
      "grad_norm": 0.7711734771728516,
      "learning_rate": 8.202247191011237e-06,
      "loss": 2.4155,
      "step": 1655
    },
    {
      "epoch": 184.0,
      "grad_norm": 0.811294436454773,
      "learning_rate": 8.146067415730338e-06,
      "loss": 2.4414,
      "step": 1656
    },
    {
      "epoch": 184.11111111111111,
      "grad_norm": 1.86774742603302,
      "learning_rate": 8.089887640449438e-06,
      "loss": 0.8539,
      "step": 1657
    },
    {
      "epoch": 184.22222222222223,
      "grad_norm": 0.4221068322658539,
      "learning_rate": 8.033707865168539e-06,
      "loss": 2.4528,
      "step": 1658
    },
    {
      "epoch": 184.33333333333334,
      "grad_norm": 0.5279190540313721,
      "learning_rate": 7.97752808988764e-06,
      "loss": 2.425,
      "step": 1659
    },
    {
      "epoch": 184.44444444444446,
      "grad_norm": 3.172244071960449,
      "learning_rate": 7.921348314606742e-06,
      "loss": 2.4651,
      "step": 1660
    },
    {
      "epoch": 184.55555555555554,
      "grad_norm": 2.0333733558654785,
      "learning_rate": 7.865168539325843e-06,
      "loss": 2.5557,
      "step": 1661
    },
    {
      "epoch": 184.66666666666666,
      "grad_norm": 0.4581284821033478,
      "learning_rate": 7.808988764044945e-06,
      "loss": 2.4192,
      "step": 1662
    },
    {
      "epoch": 184.77777777777777,
      "grad_norm": 0.6354535222053528,
      "learning_rate": 7.752808988764046e-06,
      "loss": 2.4344,
      "step": 1663
    },
    {
      "epoch": 184.88888888888889,
      "grad_norm": 1.339450716972351,
      "learning_rate": 7.696629213483146e-06,
      "loss": 2.5068,
      "step": 1664
    },
    {
      "epoch": 185.0,
      "grad_norm": 0.5970942974090576,
      "learning_rate": 7.640449438202247e-06,
      "loss": 2.4352,
      "step": 1665
    },
    {
      "epoch": 185.11111111111111,
      "grad_norm": 0.589293897151947,
      "learning_rate": 7.584269662921349e-06,
      "loss": 2.3555,
      "step": 1666
    },
    {
      "epoch": 185.22222222222223,
      "grad_norm": 0.7648154497146606,
      "learning_rate": 7.5280898876404495e-06,
      "loss": 2.4213,
      "step": 1667
    },
    {
      "epoch": 185.33333333333334,
      "grad_norm": 0.5340718626976013,
      "learning_rate": 7.471910112359551e-06,
      "loss": 2.3892,
      "step": 1668
    },
    {
      "epoch": 185.44444444444446,
      "grad_norm": 2.220081329345703,
      "learning_rate": 7.415730337078652e-06,
      "loss": 0.9004,
      "step": 1669
    },
    {
      "epoch": 185.55555555555554,
      "grad_norm": 8.995986938476562,
      "learning_rate": 7.359550561797753e-06,
      "loss": 2.5617,
      "step": 1670
    },
    {
      "epoch": 185.66666666666666,
      "grad_norm": 0.5739542841911316,
      "learning_rate": 7.303370786516854e-06,
      "loss": 2.3621,
      "step": 1671
    },
    {
      "epoch": 185.77777777777777,
      "grad_norm": 0.49686312675476074,
      "learning_rate": 7.247191011235956e-06,
      "loss": 2.4143,
      "step": 1672
    },
    {
      "epoch": 185.88888888888889,
      "grad_norm": 0.6709563136100769,
      "learning_rate": 7.191011235955057e-06,
      "loss": 2.4297,
      "step": 1673
    },
    {
      "epoch": 186.0,
      "grad_norm": 1.4692763090133667,
      "learning_rate": 7.134831460674157e-06,
      "loss": 2.4726,
      "step": 1674
    },
    {
      "epoch": 186.11111111111111,
      "grad_norm": 1.9950743913650513,
      "learning_rate": 7.078651685393259e-06,
      "loss": 1.0153,
      "step": 1675
    },
    {
      "epoch": 186.22222222222223,
      "grad_norm": 0.5522083640098572,
      "learning_rate": 7.022471910112361e-06,
      "loss": 2.4286,
      "step": 1676
    },
    {
      "epoch": 186.33333333333334,
      "grad_norm": 0.5988723635673523,
      "learning_rate": 6.96629213483146e-06,
      "loss": 2.468,
      "step": 1677
    },
    {
      "epoch": 186.44444444444446,
      "grad_norm": 2.743689775466919,
      "learning_rate": 6.910112359550562e-06,
      "loss": 2.5175,
      "step": 1678
    },
    {
      "epoch": 186.55555555555554,
      "grad_norm": 0.5840834975242615,
      "learning_rate": 6.853932584269663e-06,
      "loss": 2.4468,
      "step": 1679
    },
    {
      "epoch": 186.66666666666666,
      "grad_norm": 1.2678438425064087,
      "learning_rate": 6.797752808988764e-06,
      "loss": 2.4502,
      "step": 1680
    },
    {
      "epoch": 186.77777777777777,
      "grad_norm": 0.5238818526268005,
      "learning_rate": 6.741573033707865e-06,
      "loss": 2.4263,
      "step": 1681
    },
    {
      "epoch": 186.88888888888889,
      "grad_norm": 1.3194539546966553,
      "learning_rate": 6.685393258426967e-06,
      "loss": 2.4929,
      "step": 1682
    },
    {
      "epoch": 187.0,
      "grad_norm": 0.665289044380188,
      "learning_rate": 6.629213483146068e-06,
      "loss": 2.4392,
      "step": 1683
    },
    {
      "epoch": 187.11111111111111,
      "grad_norm": 1.2863823175430298,
      "learning_rate": 6.573033707865169e-06,
      "loss": 2.4096,
      "step": 1684
    },
    {
      "epoch": 187.22222222222223,
      "grad_norm": 0.8378009796142578,
      "learning_rate": 6.51685393258427e-06,
      "loss": 2.4947,
      "step": 1685
    },
    {
      "epoch": 187.33333333333334,
      "grad_norm": 1.1415812969207764,
      "learning_rate": 6.460674157303372e-06,
      "loss": 2.4227,
      "step": 1686
    },
    {
      "epoch": 187.44444444444446,
      "grad_norm": 2.6487250328063965,
      "learning_rate": 6.404494382022471e-06,
      "loss": 2.4358,
      "step": 1687
    },
    {
      "epoch": 187.55555555555554,
      "grad_norm": 0.72603839635849,
      "learning_rate": 6.348314606741574e-06,
      "loss": 2.4046,
      "step": 1688
    },
    {
      "epoch": 187.66666666666666,
      "grad_norm": 2.4182615280151367,
      "learning_rate": 6.292134831460675e-06,
      "loss": 1.1255,
      "step": 1689
    },
    {
      "epoch": 187.77777777777777,
      "grad_norm": 0.43634864687919617,
      "learning_rate": 6.235955056179776e-06,
      "loss": 2.4152,
      "step": 1690
    },
    {
      "epoch": 187.88888888888889,
      "grad_norm": 3.103653907775879,
      "learning_rate": 6.179775280898876e-06,
      "loss": 2.503,
      "step": 1691
    },
    {
      "epoch": 188.0,
      "grad_norm": 0.48709359765052795,
      "learning_rate": 6.123595505617978e-06,
      "loss": 2.3942,
      "step": 1692
    },
    {
      "epoch": 188.11111111111111,
      "grad_norm": 0.5710951089859009,
      "learning_rate": 6.067415730337079e-06,
      "loss": 2.4031,
      "step": 1693
    },
    {
      "epoch": 188.22222222222223,
      "grad_norm": 6.113094806671143,
      "learning_rate": 6.01123595505618e-06,
      "loss": 2.6784,
      "step": 1694
    },
    {
      "epoch": 188.33333333333334,
      "grad_norm": 3.616668939590454,
      "learning_rate": 5.955056179775281e-06,
      "loss": 2.4831,
      "step": 1695
    },
    {
      "epoch": 188.44444444444446,
      "grad_norm": 0.49759641289711,
      "learning_rate": 5.8988764044943826e-06,
      "loss": 2.4116,
      "step": 1696
    },
    {
      "epoch": 188.55555555555554,
      "grad_norm": 2.4211299419403076,
      "learning_rate": 5.842696629213484e-06,
      "loss": 2.5071,
      "step": 1697
    },
    {
      "epoch": 188.66666666666666,
      "grad_norm": 0.5718874931335449,
      "learning_rate": 5.786516853932585e-06,
      "loss": 2.4296,
      "step": 1698
    },
    {
      "epoch": 188.77777777777777,
      "grad_norm": 1.9231505393981934,
      "learning_rate": 5.730337078651685e-06,
      "loss": 0.966,
      "step": 1699
    },
    {
      "epoch": 188.88888888888889,
      "grad_norm": 6.555384635925293,
      "learning_rate": 5.674157303370787e-06,
      "loss": 2.7412,
      "step": 1700
    },
    {
      "epoch": 189.0,
      "grad_norm": 0.5069090127944946,
      "learning_rate": 5.617977528089888e-06,
      "loss": 2.4237,
      "step": 1701
    },
    {
      "epoch": 189.11111111111111,
      "grad_norm": 1.5255881547927856,
      "learning_rate": 5.5617977528089895e-06,
      "loss": 2.4298,
      "step": 1702
    },
    {
      "epoch": 189.22222222222223,
      "grad_norm": 0.9129584431648254,
      "learning_rate": 5.50561797752809e-06,
      "loss": 2.4301,
      "step": 1703
    },
    {
      "epoch": 189.33333333333334,
      "grad_norm": 1.5899522304534912,
      "learning_rate": 5.449438202247191e-06,
      "loss": 0.9611,
      "step": 1704
    },
    {
      "epoch": 189.44444444444446,
      "grad_norm": 3.163713216781616,
      "learning_rate": 5.393258426966293e-06,
      "loss": 2.5539,
      "step": 1705
    },
    {
      "epoch": 189.55555555555554,
      "grad_norm": 3.8177614212036133,
      "learning_rate": 5.3370786516853935e-06,
      "loss": 2.5078,
      "step": 1706
    },
    {
      "epoch": 189.66666666666666,
      "grad_norm": 0.916290819644928,
      "learning_rate": 5.280898876404495e-06,
      "loss": 2.4079,
      "step": 1707
    },
    {
      "epoch": 189.77777777777777,
      "grad_norm": 0.6520069241523743,
      "learning_rate": 5.2247191011235955e-06,
      "loss": 2.4557,
      "step": 1708
    },
    {
      "epoch": 189.88888888888889,
      "grad_norm": 1.0961337089538574,
      "learning_rate": 5.168539325842697e-06,
      "loss": 2.4133,
      "step": 1709
    },
    {
      "epoch": 190.0,
      "grad_norm": 0.525456428527832,
      "learning_rate": 5.112359550561798e-06,
      "loss": 2.4415,
      "step": 1710
    },
    {
      "epoch": 190.11111111111111,
      "grad_norm": 1.7132993936538696,
      "learning_rate": 5.056179775280899e-06,
      "loss": 0.9012,
      "step": 1711
    },
    {
      "epoch": 190.22222222222223,
      "grad_norm": 0.581223726272583,
      "learning_rate": 5e-06,
      "loss": 2.3801,
      "step": 1712
    },
    {
      "epoch": 190.33333333333334,
      "grad_norm": 2.133267402648926,
      "learning_rate": 4.943820224719101e-06,
      "loss": 2.4052,
      "step": 1713
    },
    {
      "epoch": 190.44444444444446,
      "grad_norm": 1.3163303136825562,
      "learning_rate": 4.8876404494382024e-06,
      "loss": 2.3944,
      "step": 1714
    },
    {
      "epoch": 190.55555555555554,
      "grad_norm": 0.5785359144210815,
      "learning_rate": 4.831460674157304e-06,
      "loss": 2.4362,
      "step": 1715
    },
    {
      "epoch": 190.66666666666666,
      "grad_norm": 0.9421152472496033,
      "learning_rate": 4.7752808988764044e-06,
      "loss": 2.4127,
      "step": 1716
    },
    {
      "epoch": 190.77777777777777,
      "grad_norm": 0.47862303256988525,
      "learning_rate": 4.719101123595506e-06,
      "loss": 2.3804,
      "step": 1717
    },
    {
      "epoch": 190.88888888888889,
      "grad_norm": 3.8893685340881348,
      "learning_rate": 4.662921348314607e-06,
      "loss": 2.6312,
      "step": 1718
    },
    {
      "epoch": 191.0,
      "grad_norm": 0.5468922853469849,
      "learning_rate": 4.606741573033708e-06,
      "loss": 2.4059,
      "step": 1719
    },
    {
      "epoch": 191.11111111111111,
      "grad_norm": 0.5990374088287354,
      "learning_rate": 4.550561797752809e-06,
      "loss": 2.4227,
      "step": 1720
    },
    {
      "epoch": 191.22222222222223,
      "grad_norm": 2.1136560440063477,
      "learning_rate": 4.49438202247191e-06,
      "loss": 1.1804,
      "step": 1721
    },
    {
      "epoch": 191.33333333333334,
      "grad_norm": 4.433318138122559,
      "learning_rate": 4.438202247191011e-06,
      "loss": 2.4862,
      "step": 1722
    },
    {
      "epoch": 191.44444444444446,
      "grad_norm": 0.6409903764724731,
      "learning_rate": 4.382022471910113e-06,
      "loss": 2.424,
      "step": 1723
    },
    {
      "epoch": 191.55555555555554,
      "grad_norm": 5.035891056060791,
      "learning_rate": 4.325842696629213e-06,
      "loss": 2.5881,
      "step": 1724
    },
    {
      "epoch": 191.66666666666666,
      "grad_norm": 3.7158777713775635,
      "learning_rate": 4.269662921348315e-06,
      "loss": 2.6134,
      "step": 1725
    },
    {
      "epoch": 191.77777777777777,
      "grad_norm": 5.022674083709717,
      "learning_rate": 4.213483146067416e-06,
      "loss": 2.5055,
      "step": 1726
    },
    {
      "epoch": 191.88888888888889,
      "grad_norm": 2.1693456172943115,
      "learning_rate": 4.157303370786518e-06,
      "loss": 2.4968,
      "step": 1727
    },
    {
      "epoch": 192.0,
      "grad_norm": 0.5791630148887634,
      "learning_rate": 4.101123595505618e-06,
      "loss": 2.4311,
      "step": 1728
    },
    {
      "epoch": 192.11111111111111,
      "grad_norm": 3.61102294921875,
      "learning_rate": 4.044943820224719e-06,
      "loss": 2.5539,
      "step": 1729
    },
    {
      "epoch": 192.22222222222223,
      "grad_norm": 0.4114529490470886,
      "learning_rate": 3.98876404494382e-06,
      "loss": 2.4005,
      "step": 1730
    },
    {
      "epoch": 192.33333333333334,
      "grad_norm": 2.300016403198242,
      "learning_rate": 3.932584269662922e-06,
      "loss": 1.0192,
      "step": 1731
    },
    {
      "epoch": 192.44444444444446,
      "grad_norm": 0.7285314202308655,
      "learning_rate": 3.876404494382023e-06,
      "loss": 2.4217,
      "step": 1732
    },
    {
      "epoch": 192.55555555555554,
      "grad_norm": 0.8068503141403198,
      "learning_rate": 3.820224719101124e-06,
      "loss": 2.4453,
      "step": 1733
    },
    {
      "epoch": 192.66666666666666,
      "grad_norm": 0.8462062478065491,
      "learning_rate": 3.7640449438202247e-06,
      "loss": 2.4526,
      "step": 1734
    },
    {
      "epoch": 192.77777777777777,
      "grad_norm": 0.42235344648361206,
      "learning_rate": 3.707865168539326e-06,
      "loss": 2.4434,
      "step": 1735
    },
    {
      "epoch": 192.88888888888889,
      "grad_norm": 3.592755079269409,
      "learning_rate": 3.651685393258427e-06,
      "loss": 2.4187,
      "step": 1736
    },
    {
      "epoch": 193.0,
      "grad_norm": 0.5050343871116638,
      "learning_rate": 3.5955056179775286e-06,
      "loss": 2.4064,
      "step": 1737
    },
    {
      "epoch": 193.11111111111111,
      "grad_norm": 0.8710671663284302,
      "learning_rate": 3.5393258426966296e-06,
      "loss": 2.4756,
      "step": 1738
    },
    {
      "epoch": 193.22222222222223,
      "grad_norm": 0.530694305896759,
      "learning_rate": 3.48314606741573e-06,
      "loss": 2.4109,
      "step": 1739
    },
    {
      "epoch": 193.33333333333334,
      "grad_norm": 0.9181772470474243,
      "learning_rate": 3.4269662921348316e-06,
      "loss": 2.4691,
      "step": 1740
    },
    {
      "epoch": 193.44444444444446,
      "grad_norm": 0.517108142375946,
      "learning_rate": 3.3707865168539327e-06,
      "loss": 2.4311,
      "step": 1741
    },
    {
      "epoch": 193.55555555555554,
      "grad_norm": 0.8006929755210876,
      "learning_rate": 3.314606741573034e-06,
      "loss": 2.4154,
      "step": 1742
    },
    {
      "epoch": 193.66666666666666,
      "grad_norm": 0.6084094047546387,
      "learning_rate": 3.258426966292135e-06,
      "loss": 2.3914,
      "step": 1743
    },
    {
      "epoch": 193.77777777777777,
      "grad_norm": 0.5105134844779968,
      "learning_rate": 3.2022471910112357e-06,
      "loss": 2.3578,
      "step": 1744
    },
    {
      "epoch": 193.88888888888889,
      "grad_norm": 2.16005277633667,
      "learning_rate": 3.1460674157303375e-06,
      "loss": 0.9083,
      "step": 1745
    },
    {
      "epoch": 194.0,
      "grad_norm": 0.4605664014816284,
      "learning_rate": 3.089887640449438e-06,
      "loss": 2.4537,
      "step": 1746
    },
    {
      "epoch": 194.11111111111111,
      "grad_norm": 0.6938716173171997,
      "learning_rate": 3.0337078651685396e-06,
      "loss": 2.4084,
      "step": 1747
    },
    {
      "epoch": 194.22222222222223,
      "grad_norm": 1.0581300258636475,
      "learning_rate": 2.9775280898876406e-06,
      "loss": 2.4351,
      "step": 1748
    },
    {
      "epoch": 194.33333333333334,
      "grad_norm": 0.6001561284065247,
      "learning_rate": 2.921348314606742e-06,
      "loss": 2.4001,
      "step": 1749
    },
    {
      "epoch": 194.44444444444446,
      "grad_norm": 6.7851667404174805,
      "learning_rate": 2.8651685393258426e-06,
      "loss": 2.6539,
      "step": 1750
    },
    {
      "epoch": 194.55555555555554,
      "grad_norm": 0.8280834555625916,
      "learning_rate": 2.808988764044944e-06,
      "loss": 2.4734,
      "step": 1751
    },
    {
      "epoch": 194.66666666666666,
      "grad_norm": 0.5361620783805847,
      "learning_rate": 2.752808988764045e-06,
      "loss": 2.4343,
      "step": 1752
    },
    {
      "epoch": 194.77777777777777,
      "grad_norm": 0.7910929322242737,
      "learning_rate": 2.6966292134831465e-06,
      "loss": 2.4183,
      "step": 1753
    },
    {
      "epoch": 194.88888888888889,
      "grad_norm": 1.8323378562927246,
      "learning_rate": 2.6404494382022475e-06,
      "loss": 0.9066,
      "step": 1754
    },
    {
      "epoch": 195.0,
      "grad_norm": 1.1256440877914429,
      "learning_rate": 2.5842696629213485e-06,
      "loss": 2.4295,
      "step": 1755
    },
    {
      "epoch": 195.11111111111111,
      "grad_norm": 2.2481112480163574,
      "learning_rate": 2.5280898876404495e-06,
      "loss": 2.4465,
      "step": 1756
    },
    {
      "epoch": 195.22222222222223,
      "grad_norm": 0.9128661155700684,
      "learning_rate": 2.4719101123595505e-06,
      "loss": 2.4781,
      "step": 1757
    },
    {
      "epoch": 195.33333333333334,
      "grad_norm": 1.5229946374893188,
      "learning_rate": 2.415730337078652e-06,
      "loss": 0.9142,
      "step": 1758
    },
    {
      "epoch": 195.44444444444446,
      "grad_norm": 0.986349880695343,
      "learning_rate": 2.359550561797753e-06,
      "loss": 2.5053,
      "step": 1759
    },
    {
      "epoch": 195.55555555555554,
      "grad_norm": 3.6606011390686035,
      "learning_rate": 2.303370786516854e-06,
      "loss": 2.3985,
      "step": 1760
    },
    {
      "epoch": 195.66666666666666,
      "grad_norm": 1.8073713779449463,
      "learning_rate": 2.247191011235955e-06,
      "loss": 2.4625,
      "step": 1761
    },
    {
      "epoch": 195.77777777777777,
      "grad_norm": 5.014154434204102,
      "learning_rate": 2.1910112359550564e-06,
      "loss": 2.5673,
      "step": 1762
    },
    {
      "epoch": 195.88888888888889,
      "grad_norm": 0.8700577020645142,
      "learning_rate": 2.1348314606741574e-06,
      "loss": 2.3512,
      "step": 1763
    },
    {
      "epoch": 196.0,
      "grad_norm": 0.7244845628738403,
      "learning_rate": 2.078651685393259e-06,
      "loss": 2.4496,
      "step": 1764
    },
    {
      "epoch": 196.11111111111111,
      "grad_norm": 3.1892263889312744,
      "learning_rate": 2.0224719101123594e-06,
      "loss": 1.0128,
      "step": 1765
    },
    {
      "epoch": 196.22222222222223,
      "grad_norm": 0.6339479088783264,
      "learning_rate": 1.966292134831461e-06,
      "loss": 2.3485,
      "step": 1766
    },
    {
      "epoch": 196.33333333333334,
      "grad_norm": 4.213331699371338,
      "learning_rate": 1.910112359550562e-06,
      "loss": 2.4603,
      "step": 1767
    },
    {
      "epoch": 196.44444444444446,
      "grad_norm": 1.3929232358932495,
      "learning_rate": 1.853932584269663e-06,
      "loss": 2.4061,
      "step": 1768
    },
    {
      "epoch": 196.55555555555554,
      "grad_norm": 0.5285572409629822,
      "learning_rate": 1.7977528089887643e-06,
      "loss": 2.4228,
      "step": 1769
    },
    {
      "epoch": 196.66666666666666,
      "grad_norm": 0.4409061670303345,
      "learning_rate": 1.741573033707865e-06,
      "loss": 2.44,
      "step": 1770
    },
    {
      "epoch": 196.77777777777777,
      "grad_norm": 0.8197799921035767,
      "learning_rate": 1.6853932584269663e-06,
      "loss": 2.3852,
      "step": 1771
    },
    {
      "epoch": 196.88888888888889,
      "grad_norm": 4.419240951538086,
      "learning_rate": 1.6292134831460675e-06,
      "loss": 2.572,
      "step": 1772
    },
    {
      "epoch": 197.0,
      "grad_norm": 0.9509421586990356,
      "learning_rate": 1.5730337078651688e-06,
      "loss": 2.4212,
      "step": 1773
    },
    {
      "epoch": 197.11111111111111,
      "grad_norm": 0.8742499947547913,
      "learning_rate": 1.5168539325842698e-06,
      "loss": 2.3898,
      "step": 1774
    },
    {
      "epoch": 197.22222222222223,
      "grad_norm": 0.4919675588607788,
      "learning_rate": 1.460674157303371e-06,
      "loss": 2.3771,
      "step": 1775
    },
    {
      "epoch": 197.33333333333334,
      "grad_norm": 0.5589162707328796,
      "learning_rate": 1.404494382022472e-06,
      "loss": 2.4052,
      "step": 1776
    },
    {
      "epoch": 197.44444444444446,
      "grad_norm": 1.5577746629714966,
      "learning_rate": 1.3483146067415732e-06,
      "loss": 2.3908,
      "step": 1777
    },
    {
      "epoch": 197.55555555555554,
      "grad_norm": 0.8497756123542786,
      "learning_rate": 1.2921348314606742e-06,
      "loss": 2.4439,
      "step": 1778
    },
    {
      "epoch": 197.66666666666666,
      "grad_norm": 5.006871223449707,
      "learning_rate": 1.2359550561797752e-06,
      "loss": 2.5916,
      "step": 1779
    },
    {
      "epoch": 197.77777777777777,
      "grad_norm": 0.5966243743896484,
      "learning_rate": 1.1797752808988765e-06,
      "loss": 2.444,
      "step": 1780
    },
    {
      "epoch": 197.88888888888889,
      "grad_norm": 3.2055842876434326,
      "learning_rate": 1.1235955056179775e-06,
      "loss": 0.9628,
      "step": 1781
    },
    {
      "epoch": 198.0,
      "grad_norm": 0.4349004924297333,
      "learning_rate": 1.0674157303370787e-06,
      "loss": 2.3659,
      "step": 1782
    },
    {
      "epoch": 198.11111111111111,
      "grad_norm": 1.4648994207382202,
      "learning_rate": 1.0112359550561797e-06,
      "loss": 0.7815,
      "step": 1783
    },
    {
      "epoch": 198.22222222222223,
      "grad_norm": 0.6662408113479614,
      "learning_rate": 9.55056179775281e-07,
      "loss": 2.4395,
      "step": 1784
    },
    {
      "epoch": 198.33333333333334,
      "grad_norm": 0.5329864621162415,
      "learning_rate": 8.988764044943822e-07,
      "loss": 2.4239,
      "step": 1785
    },
    {
      "epoch": 198.44444444444446,
      "grad_norm": 0.5502538681030273,
      "learning_rate": 8.426966292134832e-07,
      "loss": 2.4202,
      "step": 1786
    },
    {
      "epoch": 198.55555555555554,
      "grad_norm": 0.7447676062583923,
      "learning_rate": 7.865168539325844e-07,
      "loss": 2.4329,
      "step": 1787
    },
    {
      "epoch": 198.66666666666666,
      "grad_norm": 1.3792649507522583,
      "learning_rate": 7.303370786516855e-07,
      "loss": 2.4576,
      "step": 1788
    },
    {
      "epoch": 198.77777777777777,
      "grad_norm": 0.6131513118743896,
      "learning_rate": 6.741573033707866e-07,
      "loss": 2.3686,
      "step": 1789
    },
    {
      "epoch": 198.88888888888889,
      "grad_norm": 5.016663074493408,
      "learning_rate": 6.179775280898876e-07,
      "loss": 2.5523,
      "step": 1790
    },
    {
      "epoch": 199.0,
      "grad_norm": 0.5612925887107849,
      "learning_rate": 5.617977528089887e-07,
      "loss": 2.4503,
      "step": 1791
    },
    {
      "epoch": 199.11111111111111,
      "grad_norm": 1.1237989664077759,
      "learning_rate": 5.056179775280899e-07,
      "loss": 2.4991,
      "step": 1792
    },
    {
      "epoch": 199.22222222222223,
      "grad_norm": 2.77512788772583,
      "learning_rate": 4.494382022471911e-07,
      "loss": 2.4313,
      "step": 1793
    },
    {
      "epoch": 199.33333333333334,
      "grad_norm": 3.5460727214813232,
      "learning_rate": 3.932584269662922e-07,
      "loss": 2.5103,
      "step": 1794
    },
    {
      "epoch": 199.44444444444446,
      "grad_norm": 1.0628467798233032,
      "learning_rate": 3.370786516853933e-07,
      "loss": 2.4058,
      "step": 1795
    },
    {
      "epoch": 199.55555555555554,
      "grad_norm": 0.7862481474876404,
      "learning_rate": 2.8089887640449437e-07,
      "loss": 2.3952,
      "step": 1796
    },
    {
      "epoch": 199.66666666666666,
      "grad_norm": 0.47547489404678345,
      "learning_rate": 2.2471910112359554e-07,
      "loss": 2.3685,
      "step": 1797
    },
    {
      "epoch": 199.77777777777777,
      "grad_norm": 2.76629376411438,
      "learning_rate": 1.6853932584269665e-07,
      "loss": 2.5548,
      "step": 1798
    },
    {
      "epoch": 199.88888888888889,
      "grad_norm": 2.0170657634735107,
      "learning_rate": 1.1235955056179777e-07,
      "loss": 1.1426,
      "step": 1799
    },
    {
      "epoch": 200.0,
      "grad_norm": 3.8565144538879395,
      "learning_rate": 5.6179775280898885e-08,
      "loss": 2.5609,
      "step": 1800
    }
  ],
  "logging_steps": 1,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 200,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 42776631705600.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
