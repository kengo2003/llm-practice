{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 90,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 19.922765731811523,
      "learning_rate": 0.0,
      "loss": 43.6969,
      "step": 1
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 21.658571243286133,
      "learning_rate": 5e-06,
      "loss": 43.1427,
      "step": 2
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8348547220230103,
      "learning_rate": 1e-05,
      "loss": 3.0577,
      "step": 3
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 12.973689079284668,
      "learning_rate": 1.5e-05,
      "loss": 41.4697,
      "step": 4
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 12.158730506896973,
      "learning_rate": 2e-05,
      "loss": 36.8384,
      "step": 5
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 14.199030876159668,
      "learning_rate": 2.5e-05,
      "loss": 43.7232,
      "step": 6
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 9.135817527770996,
      "learning_rate": 3e-05,
      "loss": 43.6975,
      "step": 7
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 12.76889419555664,
      "learning_rate": 3.5e-05,
      "loss": 44.2743,
      "step": 8
    },
    {
      "epoch": 1.0,
      "grad_norm": 13.657625198364258,
      "learning_rate": 4e-05,
      "loss": 41.6737,
      "step": 9
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 11.260503768920898,
      "learning_rate": 4.5e-05,
      "loss": 43.0065,
      "step": 10
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 12.383933067321777,
      "learning_rate": 5e-05,
      "loss": 40.1693,
      "step": 11
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 30.136829376220703,
      "learning_rate": 5.500000000000001e-05,
      "loss": 43.4463,
      "step": 12
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.7131065726280212,
      "learning_rate": 6e-05,
      "loss": 2.8931,
      "step": 13
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 11.121794700622559,
      "learning_rate": 6.500000000000001e-05,
      "loss": 40.0641,
      "step": 14
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 16.37792205810547,
      "learning_rate": 7e-05,
      "loss": 45.0232,
      "step": 15
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 17.1468563079834,
      "learning_rate": 7.500000000000001e-05,
      "loss": 37.3325,
      "step": 16
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 14.237114906311035,
      "learning_rate": 8e-05,
      "loss": 41.3055,
      "step": 17
    },
    {
      "epoch": 2.0,
      "grad_norm": 21.829418182373047,
      "learning_rate": 8.5e-05,
      "loss": 42.3403,
      "step": 18
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 12.097325325012207,
      "learning_rate": 9e-05,
      "loss": 40.5385,
      "step": 19
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.9690164923667908,
      "learning_rate": 9.5e-05,
      "loss": 3.2224,
      "step": 20
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 10.568102836608887,
      "learning_rate": 0.0001,
      "loss": 39.4249,
      "step": 21
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 13.920561790466309,
      "learning_rate": 9.857142857142858e-05,
      "loss": 42.3749,
      "step": 22
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 17.444644927978516,
      "learning_rate": 9.714285714285715e-05,
      "loss": 47.4548,
      "step": 23
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 17.502511978149414,
      "learning_rate": 9.571428571428573e-05,
      "loss": 43.7799,
      "step": 24
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 36.2794075012207,
      "learning_rate": 9.428571428571429e-05,
      "loss": 42.5259,
      "step": 25
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 14.505847930908203,
      "learning_rate": 9.285714285714286e-05,
      "loss": 42.389,
      "step": 26
    },
    {
      "epoch": 3.0,
      "grad_norm": 19.711383819580078,
      "learning_rate": 9.142857142857143e-05,
      "loss": 39.5092,
      "step": 27
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 16.022607803344727,
      "learning_rate": 9e-05,
      "loss": 38.6767,
      "step": 28
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 11.880099296569824,
      "learning_rate": 8.857142857142857e-05,
      "loss": 38.3231,
      "step": 29
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 43.01071548461914,
      "learning_rate": 8.714285714285715e-05,
      "loss": 45.164,
      "step": 30
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.8013361692428589,
      "learning_rate": 8.571428571428571e-05,
      "loss": 2.9178,
      "step": 31
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 19.353652954101562,
      "learning_rate": 8.428571428571429e-05,
      "loss": 45.5196,
      "step": 32
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 28.058631896972656,
      "learning_rate": 8.285714285714287e-05,
      "loss": 38.1252,
      "step": 33
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 12.047804832458496,
      "learning_rate": 8.142857142857143e-05,
      "loss": 38.2686,
      "step": 34
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 13.766705513000488,
      "learning_rate": 8e-05,
      "loss": 41.0978,
      "step": 35
    },
    {
      "epoch": 4.0,
      "grad_norm": 15.60866928100586,
      "learning_rate": 7.857142857142858e-05,
      "loss": 39.7287,
      "step": 36
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 16.1986083984375,
      "learning_rate": 7.714285714285715e-05,
      "loss": 42.1648,
      "step": 37
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 73.60953521728516,
      "learning_rate": 7.571428571428571e-05,
      "loss": 39.2716,
      "step": 38
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 13.674614906311035,
      "learning_rate": 7.428571428571429e-05,
      "loss": 41.1967,
      "step": 39
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.8912383913993835,
      "learning_rate": 7.285714285714286e-05,
      "loss": 2.8858,
      "step": 40
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 15.326603889465332,
      "learning_rate": 7.142857142857143e-05,
      "loss": 40.2031,
      "step": 41
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 17.69964027404785,
      "learning_rate": 7e-05,
      "loss": 39.9554,
      "step": 42
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 11.35615062713623,
      "learning_rate": 6.857142857142858e-05,
      "loss": 39.1553,
      "step": 43
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 16.720348358154297,
      "learning_rate": 6.714285714285714e-05,
      "loss": 37.0201,
      "step": 44
    },
    {
      "epoch": 5.0,
      "grad_norm": 20.76923370361328,
      "learning_rate": 6.571428571428571e-05,
      "loss": 38.6622,
      "step": 45
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 30.719058990478516,
      "learning_rate": 6.428571428571429e-05,
      "loss": 42.6095,
      "step": 46
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 18.54888153076172,
      "learning_rate": 6.285714285714286e-05,
      "loss": 42.1363,
      "step": 47
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 41.758052825927734,
      "learning_rate": 6.142857142857143e-05,
      "loss": 42.4966,
      "step": 48
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 28.915756225585938,
      "learning_rate": 6e-05,
      "loss": 40.4013,
      "step": 49
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 70.2442398071289,
      "learning_rate": 5.8571428571428575e-05,
      "loss": 44.3491,
      "step": 50
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 25.194808959960938,
      "learning_rate": 5.714285714285714e-05,
      "loss": 38.0223,
      "step": 51
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 13.940171241760254,
      "learning_rate": 5.571428571428572e-05,
      "loss": 37.2969,
      "step": 52
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 30.315074920654297,
      "learning_rate": 5.428571428571428e-05,
      "loss": 39.1142,
      "step": 53
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.3179404735565186,
      "learning_rate": 5.285714285714286e-05,
      "loss": 2.912,
      "step": 54
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 24.509958267211914,
      "learning_rate": 5.142857142857143e-05,
      "loss": 42.17,
      "step": 55
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 25.361101150512695,
      "learning_rate": 5e-05,
      "loss": 42.3585,
      "step": 56
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 12.703400611877441,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 38.3417,
      "step": 57
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 19.89890480041504,
      "learning_rate": 4.714285714285714e-05,
      "loss": 36.1129,
      "step": 58
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 13.930988311767578,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 36.7835,
      "step": 59
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 16.589635848999023,
      "learning_rate": 4.428571428571428e-05,
      "loss": 37.5254,
      "step": 60
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 16.50747299194336,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 34.287,
      "step": 61
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 20.90694236755371,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 37.7578,
      "step": 62
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.8911851644515991,
      "learning_rate": 4e-05,
      "loss": 2.9766,
      "step": 63
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 16.91651153564453,
      "learning_rate": 3.857142857142858e-05,
      "loss": 36.9493,
      "step": 64
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 16.84487533569336,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 33.6854,
      "step": 65
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 18.448772430419922,
      "learning_rate": 3.571428571428572e-05,
      "loss": 38.6713,
      "step": 66
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 20.6877384185791,
      "learning_rate": 3.428571428571429e-05,
      "loss": 38.5213,
      "step": 67
    },
    {
      "epoch": 7.555555555555555,
      "grad_norm": 17.920276641845703,
      "learning_rate": 3.285714285714286e-05,
      "loss": 32.8976,
      "step": 68
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 29.206493377685547,
      "learning_rate": 3.142857142857143e-05,
      "loss": 36.9823,
      "step": 69
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 21.003734588623047,
      "learning_rate": 3e-05,
      "loss": 35.7596,
      "step": 70
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 21.310007095336914,
      "learning_rate": 2.857142857142857e-05,
      "loss": 39.981,
      "step": 71
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.0059245824813843,
      "learning_rate": 2.714285714285714e-05,
      "loss": 2.9552,
      "step": 72
    },
    {
      "epoch": 8.11111111111111,
      "grad_norm": 32.10515213012695,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 41.669,
      "step": 73
    },
    {
      "epoch": 8.222222222222221,
      "grad_norm": 14.20136833190918,
      "learning_rate": 2.4285714285714288e-05,
      "loss": 33.3673,
      "step": 74
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 13.438337326049805,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 34.1711,
      "step": 75
    },
    {
      "epoch": 8.444444444444445,
      "grad_norm": 13.908841133117676,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 2.8964,
      "step": 76
    },
    {
      "epoch": 8.555555555555555,
      "grad_norm": 14.626741409301758,
      "learning_rate": 2e-05,
      "loss": 38.3734,
      "step": 77
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 32.92375183105469,
      "learning_rate": 1.8571428571428572e-05,
      "loss": 41.9413,
      "step": 78
    },
    {
      "epoch": 8.777777777777779,
      "grad_norm": 27.47132110595703,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 32.4049,
      "step": 79
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 18.36073875427246,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 34.5692,
      "step": 80
    },
    {
      "epoch": 9.0,
      "grad_norm": 27.454761505126953,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 34.1632,
      "step": 81
    },
    {
      "epoch": 9.11111111111111,
      "grad_norm": 16.69927406311035,
      "learning_rate": 1.2857142857142857e-05,
      "loss": 32.2657,
      "step": 82
    },
    {
      "epoch": 9.222222222222221,
      "grad_norm": 22.850751876831055,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 33.377,
      "step": 83
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 14.567927360534668,
      "learning_rate": 1e-05,
      "loss": 36.2177,
      "step": 84
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 20.421720504760742,
      "learning_rate": 8.571428571428573e-06,
      "loss": 36.0585,
      "step": 85
    },
    {
      "epoch": 9.555555555555555,
      "grad_norm": 0.8839637637138367,
      "learning_rate": 7.142857142857143e-06,
      "loss": 2.8355,
      "step": 86
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 17.314077377319336,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 35.4104,
      "step": 87
    },
    {
      "epoch": 9.777777777777779,
      "grad_norm": 40.31674575805664,
      "learning_rate": 4.285714285714286e-06,
      "loss": 34.0063,
      "step": 88
    },
    {
      "epoch": 9.88888888888889,
      "grad_norm": 19.124177932739258,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 31.3988,
      "step": 89
    },
    {
      "epoch": 10.0,
      "grad_norm": 26.828113555908203,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 31.7915,
      "step": 90
    }
  ],
  "logging_steps": 1,
  "max_steps": 90,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2138831585280.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
